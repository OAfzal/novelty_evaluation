[
  {
    "id": 44,
    "paper_id": "JytL2MrlLT",
    "reference_text": "This paper explores the potential of modernizing the classical Nearest Neighbor approach for tabular data by leveraging a differentiable K-nearest neighbors variant, Neighborhood Components Analysis (NCA), and introduces MODERNNCA, an improved version of NCA that integrates deep learning techniques such as stochastic gradient descent (SGD), nonlinear embeddings, and a Stochastic Neighborhood Sampling (SNS) strategy to boost computational efficiency and model performance. Revisiting a classic nearest-neighbor approach with contemporary deep learning techniques is a novel approach, particularly since NCA had been previously limited by computational efficiency and scalability, and this approach aims to unify insights from both traditional and modern tabular prediction methods. However, the paper’s contribution feels incremental rather than pioneering, as the improvements in MODERNNCA rely on established techniques (SGD, SNS, and nonlinear embeddings) without introducing a fundamentally new concept or method, making the novelty limited and essentially optimizing an existing algorithm rather than providing a unique advancement. The paper also lacks direct comparisons with other KNN-inspired deep learning methods that have similarly benefited from modern optimization strategies, which weakens the argument for MODERNNCA’s distinctiveness and impact.",
    "candidate_a_text": "This paper revisits the Neighborhood Components Analysis (NCA) and adapts it for tabular data learning, proposing ModernNCA as an enhanced approach. While the approach effectively leverages modern deep learning techniques to enhance classical NCA and demonstrates strong empirical results, the core modifications—using a representation space for distance calculations, employing SGD, and mini-batch training—have already been explored in prior research. This raises concerns regarding the originality of the contribution, as the changes appear more like tunings of established techniques rather than introducing a fundamentally new method.",
    "candidate_b_text": "This paper revisits the classic KNN method and explores ways to improve it using modern deep learning techniques. The proposed MODERNNCA is a deep learning version of KNN, which achieves good performance in both classification and regression tasks, essentially serving as a strong deep baseline for tabular tasks. However, the idea of using a deep learning version of KNN for tabular data is not novel, as it has been explored in previous works like TabR.",
    "candidate_a_label": "Human Review 2",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": 2,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          2,
          4
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "2": true,
            "4": true
          },
          "2": {
            "1": true,
            "2": true,
            "4": true
          },
          "4": {
            "1": true,
            "2": true,
            "4": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "2": 1.0,
          "4": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 39
  },
  {
    "id": 60,
    "paper_id": "WlKGZuolEk",
    "reference_text": "This paper proposes a method called HMKM to match image region features with categories, representing categories at both the object level and attribute level, and serving as a plug-and-play module to improve detection performance of novel categories in Open-Vocabulary Object Detection models. However, the innovative contribution relative to existing studies appears limited. While this work has improved the performance of the existing OVOD architecture, the novelty of this study is somewhat difficult to identify. Numerous related works already utilize visual prototype knowledge to enhance model classification ability, and a more explicit discussion on how it differs significantly from related methods should be conducted.",
    "candidate_a_text": "This paper introduces a new approach called Hierarchical Multimodal Knowledge Matching (HMKM) for training-free open-vocabulary object detection. The idea of combining object-level and attribute-level knowledge for open-vocabulary object detection is interesting, and the proposed method is training-free and can be easily integrated into existing object detection models. However, the paper only compares HMKM with a few baseline methods, and it would be better if more recent and relevant methods, particularly those that also leverage multimodal knowledge for open-vocabulary object detection, could be included for comparison to more accurately assess the proposed method’s contributions and relative performance. A detailed comparison of the proposed method with these approaches would help to highlight its unique contributions and limitations.",
    "candidate_b_text": "This paper presents Hierarchical Multimodal Knowledge Matching (HMKM), an innovative, training-free, hierarchical knowledge matching approach for open-vocabulary object detection. By combining object-level and attribute-level prototypes, it addresses a critical gap in novel category detection effectively. HMKM’s model-independent, plug-and-play framework offers a substantial improvement in detecting previously unseen categories and can be integrated into existing models without requiring additional training. However, HMKM lacks comparisons with a wider range of state-of-the-art OVOD methods, and while it claims multimodal knowledge independence, this is not rigorously analyzed.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "Human Review 4",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 4,
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 0.5,
        "num_reviews": 5,
        "consensus_type": "majority",
        "other_reviews": [
          0,
          1,
          3,
          4
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": false,
            "2": true,
            "3": false,
            "4": false
          },
          "1": {
            "0": false,
            "1": true,
            "2": true,
            "3": false,
            "4": false
          },
          "2": {
            "0": true,
            "1": true,
            "2": true,
            "3": false,
            "4": false
          },
          "3": {
            "0": false,
            "1": false,
            "2": false,
            "3": true,
            "4": false
          },
          "4": {
            "0": false,
            "1": false,
            "2": false,
            "3": false,
            "4": true
          }
        },
        "agreement_scores": {
          "0": 0.25,
          "1": 0.25,
          "2": 0.5,
          "3": 0.0,
          "4": 0.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 1
  },
  {
    "id": 21,
    "paper_id": "wE8wJXgI9T",
    "reference_text": "This paper investigates the phenomenon known as the \"modality gap\" in multi-modal contrastive learning models like CLIP and proposes renaming it to \"contrastive gap,\" arguing that this gap emerges as a consequence of contrastive training rather than modality differences. The paper provides a comprehensive empirical analysis of how dimensionality and batch size affect the contrastive gap, offering insights into why this phenomenon occurs in multi-modal models. The proposed solution of adding uniformity and alignment terms to the CLIP loss is relatively simple to implement and shows some improvements in certain tasks. However, the paper incorrectly attributes CLIP's loss function to SimCLR's NT-Xent loss, when CLIP actually builds upon multi-class N-pair loss, which undermines the paper's theoretical foundation and technical credibility. There is also insufficient justification for renaming \"modality gap\" to \"contrastive gap,\" and the paper lacks a comparison with previous modality gap solutions (e.g., Liang et al. 2022).",
    "candidate_a_text": "An empirical study on the impact of several loss functions on the CLIP embedding space has been presented in this work. The key contribution of this work is relatively marginal. The proposed contrastive gap lacks insightful theoretical evidence and guarantees, and the proposed mitigation strategies all build on top of existing works. While the new proposed fine-tuning loss shows some improvements, the overall technical novelty is lacking.",
    "candidate_b_text": "This submission provides a theoretically grounded reframing of the \"modality gap\" in multi-modal contrastive models (e.g., CLIP), arguing that it is fundamentally a \"contrastive gap\" induced by the geometry of the contrastive loss, rather than by modality, data, or architecture. The main technical contribution is the explicit application of alignment and uniformity regularization to the loss function in large-scale multi-modal models, building on prior theoretical work (Wang & Isola, 2020) but extending it to the multi-modal setting. While prior works (e.g., Liang et al., Oh et al.) have empirically analyzed and reduced the gap using other methods (projection, hard negatives), this submission's novelty lies in its formal theoretical framing and large-scale empirical validation of alignment/uniformity regularization. The empirical finding that reducing the gap improves downstream performance is consistent with previous literature, and the main advance is in the method and theoretical interpretation rather than in practical outcomes. Reviewers should note that while the submission's theoretical perspective and explicit loss design are substantive, the empirical improvements and the identification of the gap as loss-induced are incremental relative to prior work.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "3": true
          },
          "3": {
            "1": true,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 2
  },
  {
    "id": 59,
    "paper_id": "XTxdDEFR6D",
    "reference_text": "This paper introduces LLM4Solver, a framework that leverages large language models (LLMs) to assist in algorithm design for combinatorial optimization, specifically targeting Mixed Integer Linear Programs. The approach involves using an LLM to generate algorithmic components in a programming language, which are subsequently refined through evolutionary algorithms. The authors claim that this method surpasses state-of-the-art human-designed and learning-based heuristics in terms of solution quality and efficiency across various benchmarks. However, the novelty of the approach appears limited, as similar uses of LLMs for algorithm design and optimization have already been explored in recent literature. Notably, works such as \"Evolution of heuristics: Towards efficient automatic algorithm design using large language model,\" \"LLaMoCo: Instruction tuning of large language models for optimization code generation,\" \"LLaMEA: A large language model evolutionary algorithm for automatically generating metaheuristics,\" \"ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution,\" \"Autonomous multi-objective optimization using large language model,\" and \"OptiMUS: Optimization Modeling Using MIP Solvers and large language models\" have addressed related ideas. While LLM4Solver demonstrates promising results, the core concept of prompting LLMs to generate code for optimization tasks is not particularly novel, and the paper primarily compares itself to the work of Kuang et al. without sufficiently situating its contributions relative to the broader body of recent work. The main contribution seems to be the specific integration of LLM-generated code with evolutionary refinement, but this incremental advance does not substantially differentiate the approach from existing methods. To more convincingly establish its novelty, the paper would benefit from expanded experimental comparisons and a clearer articulation of how its framework advances beyond prior LLM-based optimization techniques.",
    "candidate_a_text": "LLM4Solver applies the established paradigm of combining large language models (LLMs) with evolutionary search to the new domain of *exact* mixed-integer programming (MIP) solver algorithm design, evaluating on the challenging MIPLIB 2017 benchmark. While the methodology—LLM-driven code/algorithm generation with derivative-free optimization—is very similar to recent works like AEL, ReEvo, and AutoSAT, LLM4Solver’s main contribution is its focus on full solver algorithms for exact MIP, rather than heuristics or metaheuristics for routing or packing. The authors’ claims of being “first” are accurate for this specific application domain, but not for the general approach, and the distinction between “heuristic” and “algorithm” is sometimes overstated. Some relevant related works (e.g., DeepACO, Sym-NCO) are omitted, and the empirical gains may be partly attributable to benchmark choice. Overall, the work represents an incremental but well-executed advance in a rapidly evolving field, with its primary novelty in application scope rather than fundamental methodology.",
    "candidate_b_text": "This paper introduces LLM4Solver, a novel framework that leverages large language models (LLMs) to automatically design high-quality algorithms for combinatorial optimization (CO) solvers, specifically focusing on diving heuristics within branch-and-bound (B&B) algorithms. I find the core idea of using LLMs to generate algorithmic components for CO solvers to be both novel and promising, representing a significant achievement in demonstrating that LLMs can effectively tackle the complex task of algorithm design, which has traditionally been challenging for machine learning methods. Unlike many neural network-based approaches, LLM4Solver produces interpretable heuristics with code comments, making it easier for domain experts to understand and improve the underlying logic and search patterns, which I consider another major strength. The multi-objective extension of the framework, allowing for heuristics that generalize across different problem types, addresses a key limitation of many existing approaches and stands out as a valuable contribution. The empirical results compellingly demonstrate that LLM4Solver outperforms both human-designed heuristics and a state-of-the-art learning-based method in terms of solution quality, solving efficiency, and cross-benchmark generalization, with ablation studies showing the necessity of combining LLMs and evolutionary algorithms for high performance. Overall, I believe that this paper makes a significant contribution to the field of automated algorithm design and has the potential for significant impact, as the framework’s ability to generate high-quality, interpretable heuristics with improved generalization capabilities represents a substantial step forward.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {}
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 8
  },
  {
    "id": 93,
    "paper_id": "CKYsXi0dOV",
    "reference_text": "This paper presents BLIP-3-Video, a novel multimodal language model for video understanding that introduces an innovative temporal encoder to significantly reduce the number of visual tokens needed to represent videos, offering a new approach to efficiency in video understanding models. While the integration of the temporal encoder is highlighted as a key innovation, I question the core novelty of this paper compared to prior works such as LLaMA-VID, Video-LLaVA, LLaVA-VID, and LLaVA-NEXT, as similar architectures have already been explored in these works. Although the experimental results show that 32 tokens achieved better performance on four short video benchmarks, this standard may change with different video lengths, scenarios, and question complexity, raising concerns about the scalability and generalizability of the method. Perhaps a more effective mechanism for accommodating more frames and selecting key information for video question answering from a large number of visual tokens is worth exploring, rather than focusing on a specific numerical value of visually overfitted tokens on a few benchmarks.",
    "candidate_a_text": "BLIP-3-Video presents an incremental advance in token-efficient video representation for vision-language models, primarily through the architectural design and explicit integration of a learnable spatio-temporal pooling module. While the submission claims fundamental novelty and to be the first to demonstrate high-quality video QA with only 16–32 tokens, similar results and approaches have already been reported in recent works such as TESTA, SMAUG, and LLaVA-PruMerge. The use of sequential models like Token Turing Machines as temporal encoders is a direct application of existing methods rather than a conceptual innovation. The main technical contribution is the specific pooling architecture, but the overall approach aligns closely with the field’s ongoing trend toward aggressive token reduction and learnable aggregation. Reviewers should recognize BLIP-3-Video as a well-executed, incremental improvement within an established research paradigm, rather than a fundamentally novel or paradigm-shifting contribution.",
    "candidate_b_text": "This paper introduces BLIP-3-Video, a novel multimodal language model designed for efficient video understanding, with the core innovation lying in its temporal encoder, which significantly reduces the number of visual tokens required to represent a video and directly addresses the computational challenges of processing long videos in large vision-language models. I find that the core idea of using a temporal encoder to reduce the number of visual tokens is a significant contribution, and the exploration of two distinct temporal encoders—spatio-temporal attentional pooling and Token Turing Machines (TTM)—is another strength that allows for a comparison of different approaches to temporal modeling within the same framework. However, the paper's novelty is somewhat limited by its reliance on existing techniques, as the core idea of using a temporal encoder to reduce the number of visual tokens is not entirely new and similar approaches have been explored in prior works. The authors acknowledge this to some extent, but a more detailed comparison with existing methods, such as token pruning or token merging techniques, would help to better contextualize the contributions of the proposed model and highlight its unique aspects. Despite this, the fact that BLIP-3-Video can abstract a video into as few as 32 tokens while maintaining high accuracy, and achieves competitive performance compared to much larger state-of-the-art models, is a remarkable achievement and constitutes a significant step towards more efficient video understanding. Overall, while the approach offers practical and efficiency improvements, the lack of a thorough analysis of how the proposed temporal encoder differs from existing token reduction methods makes it difficult to fully assess the model's unique contributions.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "3": true
          },
          "3": {
            "0": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 14
  },
  {
    "id": 34,
    "paper_id": "otXB6odSG8",
    "reference_text": "This paper employs the Neuralode method to perform the Atmospheric Radiation Parameterization task. While it may perform relatively well in weather forecasting, it lacks novel insights for the ML community, as it primarily appears to apply standard ML modules to AI4Science tasks. The methods used are previously published, and although adapting an existing method from area A to area B could reach the level of a Nature/Science paper, it doesn't meet my standards for ICLR. Typically, the main contribution should lie in the methodological design, incorporating modifications based on the specific problem at hand, and the comparison should focus on SOTA methods to highlight its novelty. Comparing only with outdated work makes it difficult to assess the novelty of the proposed method, and simply applying an existing method from area A to area B should truly surprise people, as such transfers are generally not considered easy. I believe this work falls short in both aspects.",
    "candidate_a_text": "This study uses a broad range of ML models, including CatBoost, Neural ODEs, CNNs, GRUs, and their polynomial activation counterparts, to assess the performance of different architectures in emulating radiation and to plug the best scheme into a regional climate model for speedup assessment. Apart from speedup in radiation emulation, the novelty of the work is not clear to me, since the central idea behind the work has been present by past papers and the paper pretty much uses established neural networks in their out-of-box configuration to emulate radiation. The idea that ML can solve this computational deadlock has been known for a while and some works mentioned in the study have also shown some preliminary progress in this direction. Because I find the ML novelty lacking and because the ideas have already been proposed in past studies, in my opinion, the paper might better fit in a physical science journal which would allow the authors to focus on the scientific merits of their particular analysis.",
    "candidate_b_text": "This paper explores the application of neural networks for radiative transfer parameterization within weather and climate models, focusing on the Arctic region. I found the authors' exploration of various neural network architectures, including point-wise, profile-wise, and sequential models, to be a significant strength, and their systematic approach demonstrates a clear understanding of the problem's intricacies. The use of a profile-wise recurrent neural network (RNN) to emulate radiative flux propagation is a key aspect of their approach, and the finding that profile-wise RNNs perform best highlights the importance of considering vertical propagation and sequential dependencies in atmospheric data. The core contribution of the paper lies in demonstrating the feasibility and effectiveness of using neural networks, particularly profile-wise RNNs, for radiative transfer parameterization, leading to substantial computational gains within a regional weather model, as evidenced by the 26.5-fold speedup in radiation step computation. While the paper presents a valuable contribution by showcasing the potential of neural networks for accelerating radiative transfer calculations in weather and climate models, it lacks a detailed comparison to other existing parameterization schemes and does not provide a clear justification for the choice of the profile-wise RNN architecture over alternatives. Additionally, limitations in generalizability, potential overfitting due to a high number of parameters, and a lack of rigorous evaluation of overall forecast accuracy restrict the significance and originality of the contribution. Despite these limitations, the systematic evaluation of different neural network architectures and the demonstrated speedup provide a good starting point for future research in this area.",
    "candidate_a_label": "Human Review 1",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": 1,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          2
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "2": true
          },
          "1": {
            "0": true,
            "1": true,
            "2": true
          },
          "2": {
            "0": true,
            "1": true,
            "2": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "1": 1.0,
          "2": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 5
  },
  {
    "id": 63,
    "paper_id": "7P7FsPL05D",
    "reference_text": "The paper presents a framework (DuRND) that integrates two groups of lightweight random network pairs to jointly generate novelty and contribution rewards, with coefficients for these rewards scaled throughout training to balance exploitation and exploration, and integrates DuRND into PPO. While the problem the paper tries to solve is important and the idea is simple and clearly presented, some clarifications are needed to help me gauge the contribution of this work. It is unclear how challenging the evaluated tasks are, making it hard to gauge the contribution, and the results raise questions about the paper's assumption regarding the limitations of novelty-based reward.",
    "candidate_a_text": "This paper proposes a novel framework, Dual Random Networks Distillation (DuRND), to address the exploration-exploitation dilemma in reinforcement learning by combining novelty-based exploration and value-based exploitation in a unified framework. The novelty and contribution rewards in DuRND are based on prediction errors from random network modules, which may not capture complex state relationships in certain environments, and the reliance on prediction error as a proxy for novelty might be insufficient where the state space has a complex structure or the prediction error does not align with actual information gain. The approach introduces an interesting combination of novelty and contribution rewards but relies on computationally efficient heuristics that might not capture the true novelty in complex environments. While the framework demonstrates effectiveness and low computational overhead, the linear adjustment of weights for novelty and contribution rewards is a heuristic that may not be optimal or universally applicable. Overall, the originality lies in the integration of novelty and value-based signals, but there remain significant limitations in the robustness and generalizability of the contribution due to dependency on specific proxy measures and heuristic schedules.",
    "candidate_b_text": "This paper presents an approach to better balance novelty-based exploration and exploitation by introducing Dual Random Network Distillation (DuRND), an extension of the novelty-based bonus from RND that combines two bonuses based on novelty and contribution to success. Unlike RND, DuRND aims to focus later exploration behavior on novel states that led to successful trajectories or milestones. While experiments show that DuRND outperforms some novelty-seeking and reward-shaping approaches, the approach relies on strong assumptions about what constitutes success or failure and requires knowledge about the environment, such as milestones and $T_{max}$. Overall, DuRND's success seems to depend heavily on how milestones and other hyperparameters are set, and thus it lacks the level of applicability that may be required in general reinforcement learning. The contributions would be more significant if the authors could design ways to reduce dependence on the environment and training-specific information.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "Human Review 3",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 3,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.6666666666666666,
        "num_reviews": 4,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          2,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "2": false,
            "3": true
          },
          "1": {
            "0": true,
            "1": true,
            "2": false,
            "3": true
          },
          "2": {
            "0": false,
            "1": false,
            "2": true,
            "3": false
          },
          "3": {
            "0": true,
            "1": true,
            "2": false,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 0.6666666666666666,
          "1": 0.6666666666666666,
          "2": 0.0,
          "3": 0.6666666666666666
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 6
  },
  {
    "id": 54,
    "paper_id": "ueeqGvQozB",
    "reference_text": "This paper introduces ML4MILP, an original benchmark collection targeted to the MILP domain, aiming to fill the gap of having a large scale, standardized benchmark for evaluating mixed integer linear program solvers. The benchmark is relevant for the ML and DL community, especially as ML-based approaches are becoming popular in this field. The introduction and related work provide a well-written, self-contained overview of existing solvers, techniques, and related benchmarks, highlighting the general lack of a larger standardized benchmark for MILPs. Based on structural and embedding distance between instances, the authors show that existing benchmarks are often more heterogeneous than the MILP benchmark they propose, which allows for some insight into the structure of the proposed and existing benchmarks. However, the overall contribution—introduction of the benchmark and showing the lack of homogeneity of existing classes of instances based on similarity evaluation metrics with respect to structure and neural embedding distances—is somewhat limited in scientific insight and currently results in me having a hard time voting for acceptance at a main ML and DL conference such as ICLR; maybe submitting the paper to a Benchmark and Dataset track of a suitable conference might be more targeted.",
    "candidate_a_text": "This paper proposes a benchmark for evaluating machine learning-based mixed-integer linear programming solvers, consisting of a dataset of 100,000 MILP instances classified into 70 classes, and provides a baseline of different algorithms on the dataset. While the dataset is significantly larger than previous ones, I find that the proposed dataset is not significantly different from the existing ones, and the authors do not provide a clear motivation for why the existing benchmarks are not sufficient. The authors claim that existing benchmarks are not homogeneous, but they do not provide a clear definition of homogeneity in the context of MILP instances, nor sufficient evidence that the proposed dataset is indeed more homogeneous than the existing ones. The argument that the similarity between instances in the same class is higher than in existing benchmarks is not convincing without a clear explanation of how similarity is measured and why it matters. Although the proposed dataset’s larger size is emphasized, the authors do not provide a clear justification for why a larger dataset is necessarily better. Overall, the novelty and significance of the contribution are unclear due to a lack of rigorous comparisons to prior benchmarks, definitions of claimed improvements, and justifications for the benefits claimed.",
    "candidate_b_text": "This submission introduces ML4MILP, a large-scale, open-source MILP benchmark featuring over 100,000 instances and a novel, embedding-based instance classification scheme that combines graph statistics and GNN-derived embeddings. Its main methodological advance is the use of GNN-based embeddings for fine-grained instance categorization, which, while new for MILP datasets, is an incremental extension of existing clustering and representation learning techniques. The dataset’s scale and the embedding-based classification provide more nuanced groupings than prior benchmarks (e.g., Distributional MIPLIB, ML4CO), though the underlying problem diversity remains similar. Some claims of “first” or uniquely fine-grained classification are somewhat overstated, as prior datasets do offer domain and hardness labels, and the provision of high-quality solutions and benchmarking protocols largely follow established practice. Overall, the submission’s primary contribution is infrastructural—advancing benchmark scale and classification methodology—rather than introducing fundamentally new algorithmic concepts.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "2": {
            "2": true,
            "3": true
          },
          "3": {
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 47
  },
  {
    "id": 39,
    "paper_id": "skJLOae8ew",
    "reference_text": "This paper proposes a new pipeline for generating architectural floor plans by applying diffusion models, fine-tuning them to learn implicit design concepts in architectural design, and generating detailed and functional floor plans. However, the technical innovation in this paper is quite insufficient, and the introduction to the U-Net architecture is entirely superfluous. For ICLR, this paper clearly lacks innovation and systematic methodology, and it does not quite meet the threshold for current AI conferences.",
    "candidate_a_text": "This assessment finds that the submission adapts diffusion models—specifically a customized U-Net architecture with integrated upscaling—for efficient, high-resolution architectural floor plan generation, situating it within a well-established trend of domain adaptation and engineering optimization in generative modeling. While the authors claim to be the first to apply diffusion models to this domain and to uniquely capture architectural semantics, these claims are not fully substantiated, as prior works such as HouseDiffusion and Tell2Design have already explored similar territory. The main technical contributions appear to be in the specific engineering choices and integration of upscaling for floor plans, representing an incremental rather than a fundamental advance. The submission omits direct comparison with the most relevant prior work and tends to overstate its novelty, particularly regarding \"firstness\" and semantic capability. Reviewers are advised to focus on the empirical results and robustness of the engineering, but to calibrate expectations regarding conceptual innovation and to request more thorough citation and comparison with existing literature.",
    "candidate_b_text": "This paper introduces a new diffusion model designed for generating architectural floor plans, adapting the U-Net architecture and employing upscaling techniques to enhance efficiency and detail. The application of diffusion models to architectural design is a relatively new and promising field. However, the proposed method lacks novelty, as it primarily adapts an existing diffusion model (U-Net) for floor plan generation without introducing significant innovations. The paper does not cite several important works in the field, such as DreamPlan, HouseDiffuser, and HouseGan++, which further highlights the absence of originality and differentiation from prior work.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "3": true
          },
          "3": {
            "0": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 41
  },
  {
    "id": 94,
    "paper_id": "ursX3k1rTO",
    "reference_text": "This paper presents a transformer-based architecture to generate symmetric crystals conditioned on space groups in a two-stage process, and proposes a novel representation of crystal symmetry that could facilitate learning of crystal symmetry with deep learning approaches. The paper also proposes metrics to assess the symmetry of the generated crystals and highlights further gains over baseline approaches. While several works are cited in the related works section, neither described nor highlighted the difference from their approach, making it difficult to assess the uniqueness of the contribution. It is also important to add how many new tokens the method generates or if it just predicts the fixed set of tokens in different combinations (and these combinations result in more template novelty than just sampling existing templates from training data). Finally, in Table 1a, I would like to see the number of novel templates as absolute numbers instead of percentages, and the percentage of novel but structurally invalid generations from your method.",
    "candidate_a_text": "The submission presents a permutation-invariant, symmetry-conditioned autoregressive transformer for crystal structure generation, with its main technical novelty being the omission of positional encoding to enforce permutation invariance. This approach is an incremental refinement over closely related prior work, particularly CrystalFormer, which already uses Wyckoff positions and space group conditioning in a similar transformer architecture. The authors’ claims of being the “first” to combine these elements are overstated, as similar representations and conditioning have been explored in recent literature, and some relevant works (e.g., PARD) are not cited. While the empirical results are competitive, the performance gains may stem from implementation details rather than fundamental conceptual advances. Overall, the submission fits within a rapidly evolving field characterized by incremental improvements, and reviewers should interpret claims of novelty and superiority with caution, seeking more direct comparisons and a fuller discussion of related work.",
    "candidate_b_text": "This paper proposes the Wyckoff Transformer, a generative model designed for creating highly symmetric crystal structures by explicitly leveraging space group symmetry through the use of Wyckoff positions as a discrete, permutation-invariant representation of atomic locations. The Wyckoff Transformer introduces a novel approach to crystal generation by utilizing Wyckoff positions to encode symmetries explicitly, making it unique among generative models. Unlike traditional methods, it avoids positional encoding and uses permutation-invariant tokenization tailored to space group symmetries, a creative and effective innovation for materials science. This approach addresses limitations in prior methods, which struggled to produce symmetry-compliant structures, and demonstrates superior performance in symmetry-conditioned generation, creating a diverse set of stable crystal structures that respect the underlying physical symmetries. The model’s potential for symmetry-conditioned generation highlights a promising direction for future research in material informatics and generative modeling.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "Human Review 0",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 0,
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 0.5,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          0,
          4
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": false,
            "4": false
          },
          "1": {
            "0": false,
            "1": true,
            "4": true
          },
          "4": {
            "0": false,
            "1": true,
            "4": true
          }
        },
        "agreement_scores": {
          "0": 0.0,
          "1": 0.5,
          "4": 0.5
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 33
  },
  {
    "id": 31,
    "paper_id": "qrTOtUdz4Z",
    "reference_text": "This paper proposes a novel framework, ConceptGCD, that uses a covariance loss to learn distinctive concepts to facilitate category discovery in the Generalized Category Discovery (GCD) task. The novelty of the proposed framework is limited: the major contribution is the application of covariance loss in GCD. While the framework is well-designed to allow the model to use both \"derivable concepts\" to aid novel class discovery and \"underivable concepts\" customized to novel classes, the main originality lies in leveraging covariance loss to promote diversity in concept learning. The method achieves strong performance across six benchmarks, outperforming previous approaches, but the contribution is primarily incremental in applying covariance loss within the GCD context.",
    "candidate_a_text": "This paper proposes a novel framework for Generalized Category Discovery (GCD), named ConceptGCD, which introduces a new way of learning with known classes by derivable and underivable concepts. The proposed method is novel. It is interesting and effective to introduce the concept of concepts into the GCD field and learn the different concepts separately. The experimental results show that the proposed method outperforms the previous state-of-the-art methods on several GCD datasets.",
    "candidate_b_text": "This paper introduces ConceptGCD, a novel framework for Generalized Category Discovery (GCD) that leverages a concept-based approach to improve the discovery of novel classes. The introduction of the ConceptGCD framework, which classifies concepts into derivable and underivable classes and addresses them in a staged manner, is innovative and well-justified.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.5,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          2
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "2": false
          },
          "1": {
            "0": true,
            "1": true,
            "2": false
          },
          "2": {
            "0": false,
            "1": false,
            "2": true
          }
        },
        "agreement_scores": {
          "0": 0.5,
          "1": 0.5,
          "2": 0.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 32
  },
  {
    "id": 11,
    "paper_id": "INzc851YaM",
    "reference_text": "The paper proposes the Preference-Attended Multi-Objective Decision Transformer (PA-MODT) to facilitate the learning across large preference spaces and handling unknown preferences during evaluation, specifically by integrating a preference-attention block into the MODT architecture to enhance preference encoding. The novelty and contributions are very limited. I think the proposed method has very limited novelty and contributions. Besides, the method’s applicability is also limited since it only suits the transformer-based architectures in MORL.",
    "candidate_a_text": "The paper presents an empirical analysis on how advances in transformer architecture affect model performance on MORL tasks, specifically proposing Preference-Attended Multi-Objective Decision Transformer (PA-MODT), a new architecture based on MODT to facilitate the preference encoding problem. The novelty of the proposed method and contributions are very limited. In my opinion, this paper exhibits very limited novelty and applicability, since the methods only fit the transformer-based architectures in MORL, and their properties are not clarified clearly.",
    "candidate_b_text": "The paper introduces the Preference-Attended Multi-Objective Decision Transformer (PA-MODT), a novel architecture for multi-objective reinforcement learning (MORL) that integrates a preference-attention block within a modular transformer structure to address challenges in handling large preference spaces and unknown preferences. This architecture enhances the model's ability to generalize across different preferences and trajectories, leading to improved performance in generating optimal Pareto fronts. However, although the paper presents this novel architecture, experimental evaluation could be strengthened by comparing PA-MODT with a broader range of state-of-the-art baselines beyond just MODT and MORvS; including more established MORL algorithms would provide a more robust validation of PA-MODT's claimed improvements in handling complex multi-objective tasks.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "3": true
          },
          "3": {
            "0": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 11
  },
  {
    "id": 42,
    "paper_id": "pbDqZBn2X2",
    "reference_text": "This paper proposes a cost-aware diffusion solver (CADO) for combinatorial optimization and a fine-tuning framework based on reinforcement learning for diffusion models in this domain. To the best of my knowledge, current techniques using diffusion models as well as RL fine-tuning are not common in combinatorial optimization, and this paper provides a detailed open-source implementation that serves as a baseline for subsequent similar studies. However, the innovation of the method is relatively low; the method of training and fine-tuning is very simple, which is slightly lacking as the innovation point of this paper. I do not see the authors' approach as a significant change from existing algorithms based on reinforcement learning fine-tuning. The method of supervised learning and fine-tuning is more like a trick than sufficient to support the innovation of the article, and taking feasible punishment and optimization goal as reinforcement learning reward is a very simple idea in the field of combinatorial optimization. There is no obvious groundbreaking in addressing the difficult problems in combinatorial optimization, and I am still not sure why this framework is necessary or what necessary adjustments have been made to ensure its applicability to combinatorial optimization problems.",
    "candidate_a_text": "The paper introduces CADO, a framework that combines supervised learning (SL) pre-training and reinforcement learning (RL) fine-tuning for large-scale combinatorial optimization (CO) problems. However, the novelty of the proposed approach is limited, as it combines existing techniques in a straightforward way. The use of RL for fine-tuning is not new, and the paper does not provide a clear motivation for why this particular combination of SL and RL is superior to other possible approaches.",
    "candidate_b_text": "This paper proposes a new Cost-Aware Diffusion solver for combinatorial Optimization (CADO) method via RL finetuning. Overall, the method is really simple, and lacks a strong sense of novelty. In two-phase training, the first phase is the same as DIFUSCO, and the second phase is just adoption of the existing decoder and RL algorithm. The proposed method incorporates the cost information and the decoder, which enhance the overall performance with significant efficacy, but the approach does not present a substantial departure from existing methods.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "Human Review 3",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 3,
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "2": {
            "2": true,
            "3": true
          },
          "3": {
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 2
  },
  {
    "id": 4,
    "paper_id": "uV3Gdoq2ez",
    "reference_text": "This paper introduces the ReviewMT dataset, a large-scale collection of academic peer review records from ICLR and NeurIPS conferences, intended to facilitate research on language model-assisted peer review. While the dataset is positioned as a novel resource for studying the iterative and dialogic nature of real-world peer review, the technical novelty of the work is limited. The dataset itself is primarily assembled by downloading and processing publicly available review documents from the OpenReview platform, with its structure directly mirroring the existing formats used by ICLR and NeurIPS, including the standard multi-reviewer and area chair (AC) framework. The subsequent experiments involve fine-tuning several sub-10B parameter language models using the LLAMA-FACTOR library, but do not introduce new modeling techniques or analytical approaches. The claim of offering a \"novel perspective on the complete peer-review process\" is overstated, as the dataset’s structure and content are largely inherited from the original conference review processes rather than representing a fundamentally new approach. Overall, the contribution is primarily in dataset aggregation and empirical benchmarking, without significant technical innovation or advancement beyond existing resources and methodologies.",
    "candidate_a_text": "This submission presents an integrated framework for LLM-driven peer review, combining multi-turn, role-based dialogue modeling, a large-scale annotated dataset (ReviewMT), and a comprehensive benchmark with role-specific evaluation metrics. While the integration and scale are notable, the core concepts—multi-turn, role-based simulation, and fine-grained benchmarking—have been explored in prior works such as AgentReview, PeerSum, SEA, and MT-Bench-101. The authors tend to overstate their novelty, particularly in claiming to be the first to model peer review as multi-turn, role-based dialogue and to construct such a dataset, while omitting relevant related work (e.g., PeerSum, SciAssess, TELeR) that addresses similar problems. The main contribution is the unification and extension of existing approaches rather than a fundamental conceptual breakthrough. Reviewers should be aware that empirical improvements may stem from dataset scale or annotation detail, and that the submission’s novelty is primarily incremental and integrative.",
    "candidate_b_text": "This paper presents a dataset called ReviewMT, which contains papers and reviews from ICLR and NeurIPS, and proposes a set of evaluation metrics to evaluate the performance of LLMs on the ReviewMT dataset. The main contributions are the large-scale dataset—including information such as paper titles, abstracts, and review texts—and the proposed evaluation metrics. However, the dataset is limited to papers from ICLR and NeurIPS, which may not be representative of other fields or domains, and it does not include information about the authors of the papers or the reviewers who wrote the reviews. The evaluation metrics proposed by the authors may not be comprehensive or accurate and may not capture all aspects of the performance of LLMs on the ReviewMT dataset.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {}
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 13
  },
  {
    "id": 46,
    "paper_id": "7P7FsPL05D",
    "reference_text": "The paper presents a framework (DuRND) that integrates two groups of lightweight random network pairs to jointly generate novelty and contribution rewards, with coefficients for these rewards scaled throughout training to balance exploitation and exploration, and integrates DuRND into PPO. While the problem the paper tries to solve is important and the idea is simple and clearly presented, some clarifications are needed to help me gauge the contribution of this work. It is unclear how challenging the evaluated tasks are, making it hard to gauge the contribution, and the results raise questions about the paper's assumption regarding the limitations of novelty-based reward.",
    "candidate_a_text": "This paper presents an approach to better balance novelty-based exploration and exploitation by introducing Dual Random Network Distillation (DuRND), an extension of the novelty-based bonus from RND that combines two bonuses based on novelty and contribution to success. Unlike RND, DuRND aims to focus later exploration behavior on novel states that led to successful trajectories or milestones. While experiments show that DuRND outperforms some novelty-seeking and reward-shaping approaches, the approach relies on strong assumptions about what constitutes success or failure and requires knowledge about the environment, such as milestones and $T_{max}$. Overall, DuRND's success seems to depend heavily on how milestones and other hyperparameters are set, and thus it lacks the level of applicability that may be required in general reinforcement learning. The contributions would be more significant if the authors could design ways to reduce dependence on the environment and training-specific information.",
    "candidate_b_text": "DuRND is a direct extension of Random Network Distillation (RND), introducing a dual random network architecture to compute both novelty (exploration) and \"contribution\" (exploitation) rewards, with a dynamic scheduling mechanism that shifts emphasis over time. While the use of random networks for both signals is a novel implementation detail, the underlying concepts—combining exploration and exploitation signals and dynamically adjusting their balance—are well-established in the literature (e.g., SORS, ROSA, Sibling Rivalry, SERENE). The authors accurately position DuRND as a low-overhead, scalable method, but somewhat overstate its conceptual novelty and the dichotomy between prior approaches. Empirical results show improved performance on standard sparse-reward benchmarks, though such gains may be attributable to implementation choices rather than fundamental innovation. Overall, DuRND offers a practical and well-engineered architectural tweak to RND, with its main contribution being a simple, dual-network design for joint intrinsic reward computation, rather than a conceptual breakthrough.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.6666666666666666,
        "num_reviews": 4,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          2,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "2": false,
            "3": true
          },
          "1": {
            "0": true,
            "1": true,
            "2": false,
            "3": true
          },
          "2": {
            "0": false,
            "1": false,
            "2": true,
            "3": false
          },
          "3": {
            "0": true,
            "1": true,
            "2": false,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 0.6666666666666666,
          "1": 0.6666666666666666,
          "2": 0.0,
          "3": 0.6666666666666666
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 14
  },
  {
    "id": 55,
    "paper_id": "uy9oR0nYCW",
    "reference_text": "This paper proposes a conceptual explainability framework for audio deepfake detection, emphasizing sample-specific, time-specific, and feature-specific explanations interpretable by humans, and provides empirical evaluations of explainability methods for transformer-based detection models using occlusion and attention visualization techniques. However, both the proposed methods for audio explainability, occlusion and attention visualization, are already existing methods that are very common in literature, especially for vision and language tasks, and the paper did not make any modality-specific changes or edit the methods in any way for the audio domain. The roll-out attention method was introduced for natural language tokens, and while the paper claims to adapt the method for audio tokens, it is unclear what kind of modifications were made except just replacing the tokens, making the novelty introduced by the paper in these methods questionable. The paper also claims to introduce a novel benchmark to evaluate the generalization capabilities of deepfake audio classifiers by training on one dataset and evaluating on another, but this mechanism is already well-known and well-practiced, with no new contribution to the dataset, evaluation metric, or other changes, so it is questionable to consider this benchmark as a contribution. Thus, the novelty and originality of the paper’s contributions are limited.",
    "candidate_a_text": "This paper proposes to use explainable methods for state-of-the-art transformer-based audio deepfake detectors and open-sources a novel benchmark for real-world generalizability. The authors also compare and contrast various explainability methods, offering a conceptual contribution that defines key requirements for explainability in deepfake audio detection.",
    "candidate_b_text": "This paper proposes a novel explainability framework for audio transformers in audio deepfake classification by utilizing image occlusion to detect feature importance and attention roll-out to better understand features, and also open-sources a novel benchmark for detecting audio deepfakes in real-world cases by training on the ASVspoof5 dataset and testing on the FakeAVCeleb dataset. As the authors themselves have pointed out, attention roll-out and image occlusion-based analysis have been in existence for quite some time, but the novelty of the proposed work lies in applying them in spectrograms to aid in the explainability of audio deepfake analysis. However, how these attention roll-out and image occlusion-based analyses are aiding explainability specific to the audio deepfake analysis is not adequately explained, and how their contribution differs from already existing contributions of attention roll-out and image occlusion-based analysis methods in image feature explainability remains unclear. Using attention visualization to attribute where a transformer model is putting importance is not novel, and their analysis does not show enough contribution specific to explaining the decision process in audio deepfake classification in transformers. The idea of testing on a new dataset itself is not particularly novel, and they have not provided adequate explanations as to how their novel benchmark would be more helpful in audio deepfake classification.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "Human Review 1",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 1,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "3": true
          },
          "1": {
            "0": true,
            "1": true,
            "3": true
          },
          "3": {
            "0": true,
            "1": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "1": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 15
  },
  {
    "id": 64,
    "paper_id": "RdG7LVGnQi",
    "reference_text": "This paper argues that, in the context of training multi-width LLM OFA supernets, decoupling shared weights to eliminate interference from weight sharing between subnets, using low-rank adapters for training efficiency, and dynamically adjusting the sampling strategy across training steps improves deployment time. Although not extremely novel, it is a fun and interesting paper to read. I think this is incremental work—not very novel but useful and has some novelty. It applies concepts developed in other contexts in a new related context. The observations on the implications of uniform subnet sampling during training are interesting, but not well explained. There has been some recent work claiming that interference mitigation in OFA training isn't very helpful. The authors of the paper under review claim that this problem is important and describe a potential solution. Contrasting with prior work that makes a contrasting argument would improve the paper. Prior work chose \"subnets\" for uniform sampling, while this paper chooses \"subnet mean widths,\" and your evaluation shows that it works a little bit better, though it is unclear if this is sampling noise or a genuine improvement.",
    "candidate_a_text": "This paper introduces LLM-QFA, a supernet-based approach that fine-tunes multiple quantized models with identical architectures but varied bit widths to cater to different deployment scenarios. The main contribution lies in integrating quantization-aware training (QAT) with LoRA, allowing different subnets to share the same LoRA component. The approach primarily combines existing techniques—quantization-aware training and LoRA—within an OFA context, but it does not substantively test the full implications of this setting. Without new insights or substantive contributions, the approach appears to merely integrate established methods without yielding notable theoretical or practical advancements.",
    "candidate_b_text": "This paper introduces the LLM-QFA framework, a novel approach to reduce the training cost of deploying large language models across diverse scenarios. The idea of using a once-for-all training paradigm for large language models to reduce training costs across diverse scenarios is interesting. The paper presents LLM-QFA's interference-less fine-tuning and resource-balanced sampling strategy, and shows that LLM-QFA produces optimal quantized models and outperforms existing methods like QA-LoRA and GPTQ in terms of performance and efficiency.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 0.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "2": {
            "2": true,
            "3": false
          },
          "3": {
            "2": false,
            "3": true
          }
        },
        "agreement_scores": {
          "2": 0.0,
          "3": 0.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 16
  },
  {
    "id": 30,
    "paper_id": "YCdag94iZs",
    "reference_text": "This paper introduces a technique called MILCA, designed to perform counting and summing of features, where feature weights are predicted using a fully connected network (FCN) with a projection replacing the softmax layer to produce coefficients within a specified range. In my view, this paper lacks sufficient novelty. The counting-based approach appears to be a straightforward extension within the MIL space, and it does not introduce any new theoretical contributions either.",
    "candidate_a_text": "This paper introduces a novel approach to Multiple Instance Learning (MIL) by incorporating counting and attention mechanisms, where instead of relying on a single aggregated bag embedding for classification, the method identifies representative features across the dataset and encodes each bag as a count of these features, with informative features identified using statistical methods like the Mann Whitney test. While the concepts and issues presented in the paper have been previously discussed, the methods introduced offer a fresh perspective, and the method proposes a seemingly novel approach for encoding bags as a count of dataset-wide features. However, there is limited technical novelty, as the idea of counting in MIL is not novel and was already explored in other works, and the model reuses a lot of existing modules such as the Mann-Whitney test, thus novelty is also limited. The proposed approach seems to have better performance than the baseline on the disease classification using T cells repertoires benchmark, and the paper introduces a new dataset, named Wiki dataset, as an additional contribution. The Table 1 should compare with recent or state-of-the-art methods, otherwise the second novelty claim is not valid.",
    "candidate_b_text": "MILCA is a simple, efficient Multiple Instance Learning (MIL) method that extends counting/summing approaches by introducing learned feature weights and a projection step, aiming to improve interpretability and efficiency over attention-based models. The submission positions itself as an alternative to attention-based and graph-based MIL, but the technical novelty is incremental, mainly involving a different normalization (projection vs. softmax) and learned weighting, both of which have been explored in related motif-based and aggregation methods. Empirical results show modest accuracy improvements (about 3%) and efficiency gains, particularly in high-dimensional, low-sample regimes, though these may be context-dependent and not unique to MILCA. The authors somewhat overstate the conceptual distinction between \"counting\" and \"attention,\" as both are forms of weighted aggregation, and the practical impact of their technical variations may be limited. Overall, MILCA offers a practical, interpretable, and efficient extension of existing counting-based MIL methods, with its main contribution being empirical performance rather than a fundamentally new paradigm.",
    "candidate_a_label": "Human Review 2",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": 2,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 4,
        "consensus_type": "majority",
        "other_reviews": [
          0,
          2,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "2": false,
            "3": true
          },
          "1": {
            "0": true,
            "1": true,
            "2": true,
            "3": true
          },
          "2": {
            "0": false,
            "1": true,
            "2": true,
            "3": false
          },
          "3": {
            "0": true,
            "1": true,
            "2": false,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 0.6666666666666666,
          "1": 1.0,
          "2": 0.3333333333333333,
          "3": 0.6666666666666666
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 38
  },
  {
    "id": 53,
    "paper_id": "Ax3uliEBVR",
    "reference_text": "The authors propose an equivariant Topological Deep Learning framework that deals with geometric node features and can be generalized to many topological domains including simplicial, cell, combinatorial, and path complexes. The authors add an important piece of work for the Topological Deep Learning (TDL) community as there is not much literature on Equivariant TDL, and the novel benchmark based on geospatial information is novel. However, novelty is the key disadvantage of the paper, as it seems that the work just extends prior works on graphs to TDL; even though the theoretical insights are important, they are mostly an extension from graphs. Furthermore, the property of “heterogeneous interactions” was actually mentioned in prior literature, so it is not fair to claim that ETNNs are set up for this characteristic, but more like TDL in general already possesses this property.",
    "candidate_a_text": "This paper introduces an equivariant model within the framework of topological deep learning, generalizing the equivariant graph neural network architecture from Santorras et al. from the setting of graphs to message passing over combinatorial complexes, and allowing for message passing with cells that have heterogeneous node features over differing ranks. I really like that the authors introduced a novel geometric prediction task into the literature, as many of the benchmarks for TDL were somewhat old and outdated, and the new task appropriately features integration of data over different dimensional regions in a way that showcases the central feature of the paper—reconciling data with features on subspaces of differing dimension, i.e., the ‘heterogeneous interactions’ promised in the abstract. For me personally, I find it hard to understand the framing of this as a part of an entirely new conceptual field of topological deep learning beyond GNNs, and question the genuine novelty of papers like this; unless I’m mistaken, the basic content of proposition (1) is that an ETNN can be reformulated as an EGNN, meaning that the main novelty is the clever choice of ‘lifting' the data into a certain graph and some delineation of the learning based on ‘rank’, and even the proof of theorem (1) is basically a straightforward adaption of the corresponding result for EGNNs. I personally get the sense that the ‘topology’ part—and hence the novelty of these kinds of architectures—is overplayed a little, as it seems that the inclusion of domain-specific data along with the design of the graph is doing most of the work, and I don’t personally see a strong connection with the ‘lifted’ combinatorial complexes used in this paper and topology in the classical sense. The claim that this is the first work to explore combinatorial topological modeling of multi-resolution irregular geospatial data seems overstated given existing literature in Topological Data Analysis applied to geospatial data, and I would appreciate clarification on how this approach differs from or advances beyond these existing works.",
    "candidate_b_text": "The submission introduces ETNNs, a general framework for E(n)-equivariant message passing that extends prior models (notably EGNN and EMPSN) from graphs and simplicial complexes to arbitrary combinatorial complexes, thereby unifying and generalizing existing approaches in equivariant topological deep learning. The main technical novelty lies in this abstraction, which enables modeling of more general higher-order structures and allows geometric features to be incorporated at all cell orders, though the practical benefits of these extensions are only modestly demonstrated in standard domains. Claims of increased expressivity and efficiency over prior models are supported by theoretical and empirical analysis, but the improvements are incremental and may partly result from implementation choices rather than fundamental algorithmic advances. The application to a new geospatial dataset is a useful demonstration of generality, but does not constitute a methodological advance. Overall, the work is a technically sound and well-positioned generalization, but reviewers should be aware that its practical impact and novelty may be somewhat overstated unless further empirical evidence is provided for domains where combinatorial complexes offer clear advantages.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          0,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "2": true,
            "3": false
          },
          "2": {
            "0": true,
            "2": true,
            "3": true
          },
          "3": {
            "0": false,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 0.5,
          "2": 1.0,
          "3": 0.5
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 21
  },
  {
    "id": 98,
    "paper_id": "A72sZWB66Q",
    "reference_text": "This paper presents HyperDet, a novel and generalizable detection framework designed to effectively identify synthetic images by integrating shared knowledge from lightweight expert detectors and leveraging a large pretrained vision model. I find the approach novel, particularly in its incorporation of hypernetworks that generate optimized weights for specialized LoRA experts, which enhances the extraction of generalized discernible artifacts. The authors also propose an SRM filter grouping strategy to capture varying levels of pixel artifacts and introduce a novel objective function to balance pixel and semantic artifacts. However, I note that the claimed novel objective function is simply a weighted sum of the binary cross-entropy loss of the original image and the filtered image.",
    "candidate_a_text": "This paper presents HyperDet, a novel and effective framework for detecting synthesized images with high generalization capabilities. While the proposed method combines low-level and semantic features and uses a mixture of experts (MoE) for selection, the paper lacks sufficient motivation and insight into why this combination is novel and significant. Simply combining existing techniques without providing a compelling rationale or analysis of the advantages over prior work may not meet the bar for novelty and significance required by ICLR.",
    "candidate_b_text": "HyperDet is an incremental advance in the field of AI-generated image detection, building most directly on the MoLE paradigm by introducing a hypernetwork to generate and merge LoRA weights for expert detectors—a technical extension rather than a conceptual leap. While the use of a hypernetwork for LoRA weight generation is new in this context, other contributions such as grouping SRM filters and balancing pixel/semantic objectives are routine adaptations of established ideas. The empirical results show strong performance gains, but these may be attributable to ensembling and large-scale training rather than the specific hypernetwork mechanism. The authors’ characterizations of prior work are generally accurate, though they somewhat overstate the novelty and omit several recent, relevant feature fusion and hierarchical detection methods. Reviewers should recognize HyperDet as a solid technical extension with notable empirical results, but not as a fundamentally new direction in the field.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "3": true
          },
          "3": {
            "0": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 0
  },
  {
    "id": 49,
    "paper_id": "QFaj7InstQ",
    "reference_text": "This paper introduces the Item Language Model (ILM), a framework designed to bridge behavioral embeddings from recommendation systems with language understanding in Large Language Models (LLMs). The core contribution lies in adapting a Querying Transformer (QFormer) architecture with a novel item-item contrastive loss, enabling unified and interleaved processing of both behavioral and textual information for language generation tasks. The approach demonstrates innovation in its use of QFormer with item-item contrastive learning to address the modality gap between recommendation signals and language understanding. While this represents a creative step toward integrating behavioral and semantic information, the novelty is somewhat limited by the existence of related work, such as LC-Rec and BinLLM, which also aim to bridge collaborative and semantic signals within large language models. The main distinction of this paper is the specific adaptation of QFormer and the introduction of the item-item contrastive loss, which is well-justified and empirically validated. However, a more thorough comparison with these recent LLM-based recommendation systems would help to better establish the unique contributions and boundaries of novelty. Overall, the paper offers an interesting and incremental advance in the integration of recommendation and language modeling, but its novelty would be strengthened by a clearer articulation of differences from closely related approaches.",
    "candidate_a_text": "This paper proposes a novel approach by adapting the Querying Transformer model to align behavioral and textual data in recommendation systems, integrating collaborative filtering embeddings with large language models. Although these methods are mentioned in the related work, a more detailed comparison with other methods that integrate collaborative filtering with LLMs, such as CoLLM and OpenP5, would strengthen the paper.",
    "candidate_b_text": "This paper proposes a novel framework called Item-Language Model (ILM) that combines the strengths of traditional recommendation systems with large language models, addressing the limitations of both approaches. The introduction of an item-item contrastive loss and a user-item contrastive loss is presented as a contribution to help preserve behavioral information while adapting to the text modality, and the two-phase training workflow is introduced to reduce the computational cost of fully fine-tuning large language models while achieving comparable performance. However, the motivation for this research is unclear, particularly regarding the benefits of joint training on language and recommendation tasks, and the authors do not provide a compelling rationale for why an item-language model is necessary or what specific advantages it offers over existing approaches that separate these tasks. The paper lacks a thorough comparison with existing state-of-the-art methods, making it difficult to assess the true originality and significance of the proposed method, and does not clearly explain the limitations of existing approaches or how the ILM overcomes them.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {}
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 20
  },
  {
    "id": 28,
    "paper_id": "uGJxl2odR0",
    "reference_text": "The paper proposes to tackle the challenge of Neural Processes with variable input dimensions by extending the Transformer Neural Process architecture with a Dimension Aggregator Block, leveraging Positional Embeddings to account for different input dimensions before transforming features into a fixed dimensional space. The idea of leveraging positional embeddings on the dimensions axis is, I believe, novel and interesting on itself. However, Positional Embeddings are a well-studied part of the transformer architecture, especially in Language Models, and there has been a substantial amount of work on newer and better positional embeddings; the community has largely moved on from Sinusoidal embeddings to other approaches such as RoPE. While the setting here is very different, I see weaknesses in the paper given that positional embeddings are a crucial part for the DAB module to be dimension-agnostic, and I believe this warrants at least a discussion and acknowledgement of recent work, as well as a stronger argument for using Sinusoidal Embeddings. Despite these concerns, I do not think highlighting the failure modes of using Positional Embeddings diminishes the contributions of the paper; quite the contrary, it is important to highlight the limitations of the proposed approach, and if there is evidence that these failure modes do not exist in the setting exposed here, it makes for an even stronger paper. Overall, while there are some methodological and novelty concerns, I still think the empirical value of this work is enough to be accepted.",
    "candidate_a_text": "This paper introduces a novel approach called Dimension Agnostic Neural Processes (DANP), an extension of Neural Processes (NPs), that addresses significant limitations in current NP models by allowing the handling of tasks with varying input and output dimensions—a notable advancement in the field. DANP incorporates a Dimension Agnostic Block (DAB) and a Transformer-based latent path, enabling it to effectively generalize across different tasks with limited data, which is a significant strength and highlights its potential for real-world applications. While DANP is compared with several NP variants, the comparison with other types of meta-learning models or general regression techniques is missing, which could provide a broader context for its performance.",
    "candidate_b_text": "This paper presents the Dimension Agnostic Neural Process (DANP), which incorporates a Dimension Aggregator Block (DAB) to transform input features into a fixed-dimensional space, aiming to enhance the model's ability to handle diverse datasets. I find that the paper has an evident level of novelty, tackling the diverse input and output dimensions challenge in uncertainty aware meta-learning methods such as neural processes. Two novelties seem to be the case here: the dimension aggregation block and the latent path, in a transformer-like architecture. The authors outperform past existing Neural Process methods, demonstrating advantages and improvements on GP regression, Image and Video Completion, and Bayesian Optimization tasks, although the results in some cases, such as GP Regression (from-scratch case) and image completion, appear to be only marginal improvements over existing methods.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "Human Review 4",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 4,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          4
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "4": false
          },
          "4": {
            "0": false,
            "4": true
          }
        },
        "agreement_scores": {
          "0": 0.0,
          "4": 0.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 21
  },
  {
    "id": 90,
    "paper_id": "HyN9POiYhN",
    "reference_text": "This paper investigates gender bias in autoregressive large language models (LLMs) by examining bias transfer—the correlation between models’ intrinsic bias (as measured by next-token prediction) and their bias in zero- or few-shot prompting—across a range of experimental settings and open-source models, with a particular focus on the effectiveness of prompt engineering techniques for mitigating such bias. However, the novelty of the work is limited, as there is already a substantial body of literature addressing similar questions regarding zero- and few-shot evaluation of causal LLM bias (see, for example, Ganguli et al., 2023; Bai et al., 2024; Lin et al., 2024; Huang et al., 2023; and the comprehensive survey by Ranjan et al., 2024). The paper does not cite this closely related prior work and is written as if previous studies of LLM bias were restricted to masked language models or fine-tuned autoregressive models, which artificially inflates its contribution. While the specific focus on bias transfer as a metric and the detailed experimental findings may offer some incremental novelty—particularly since prior work such as Ladhak et al. (2023) has only considered bias transfer in the context of summarization tasks and fine-tuning—the overall research area is already crowded. Thus, the main contribution appears to be the application of the bias transfer metric to a broader set of tasks and models, but this advance is modest in light of the extensive existing literature. The paper would benefit from a more thorough engagement with prior work and a clearer articulation of how its findings differ from or build upon established research in the field.",
    "candidate_a_text": "This submission systematically investigates whether gender bias present in pre-trained causal LLMs (such as Mistral, Falcon, and Llama) transfers to downstream tasks (pronoun co-reference resolution) when models are adapted via prompting, rather than fine-tuning. The key empirical finding is a strong, robust correlation (≥0.90) between intrinsic and downstream gender bias in prompt-adapted causal LLMs, in contrast to prior work on fine-tuned masked language models, which found weak or no such transfer. The study’s methodological novelty lies in its focus on prompt-based adaptation in causal LLMs and its systematic variation of prompt and few-shot factors, providing robustness evidence for the main result. While the authors’ claims of being the first to systematically analyze bias transfer in this adaptation regime are largely accurate, the distinction from some recent related work (e.g., Ladhak et al., 2023) could be more clearly articulated. Overall, the submission offers a substantive empirical advance in understanding bias transfer dynamics in modern LLMs, though its conceptual novelty is primarily in the adaptation regime and model type rather than in the general framework of bias transfer analysis.",
    "candidate_b_text": "This paper investigates the transfer of gender bias from pre-trained causal language models to prompted models using the WinoBias dataset, focusing on prompt-adapted causal models, which is timely given the increasing use of prompting in deploying language models. However, the paper's scope is limited to gender bias and the WinoBias dataset, which may not fully capture the complexity of bias transfer in language models, and it does not extensively compare its findings with previous work on bias transfer in masked language models (MLMs). Given the conflicting findings in prior research, a more detailed discussion of how the results relate to and differ from previous work is needed to establish the paper's originality. The study's contribution lies in highlighting the strong correlation between intrinsic biases in pre-trained models and those in prompted models, but the lack of a thorough comparison with existing literature and exploration of a wider range of biases and datasets somewhat limits the significance and uniqueness of the contribution.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {}
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 13
  },
  {
    "id": 40,
    "paper_id": "PYmrUQmMEw",
    "reference_text": "This paper introduces LLaMA-Omni, an end-to-end model architecture designed for low-latency, high-quality speech interaction with large language models (LLMs), and presents InstructS2S-200K, a large dataset of conversational, oral-style speech instructions and responses to support more natural speech-based dialogue. While the InstructS2S-200K dataset addresses a notable gap in conversational speech instruction data, the novelty of the model architecture itself is limited. The proposed system primarily combines existing components—namely, a pre-trained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder—without a clear breakthrough in architectural design. Specifically, connecting a speech encoder to an LLM via a speech adaptor has been widely explored in prior research, and the streaming TTS module closely follows previous designs, particularly the approach outlined in Fang et al. (2024) [1]. As such, although LLaMA-Omni successfully assembles a functional pipeline for speech-based interactions with LLMs, it largely builds upon established methods and does not deliver significant innovation in model design. This reliance on prior work somewhat limits the paper’s impact from a research innovation standpoint, and further clarification of the unique contributions would be necessary to better establish its novelty.",
    "candidate_a_text": "LLaMA-Omni is positioned within the rapidly evolving field of end-to-end, low-latency speech interaction models that integrate LLMs with streaming speech processing, closely related to recent works like Mini-Omni and Moshi. While the authors claim novelty in eliminating intermediate transcription and enabling simultaneous text and speech generation, these features are already present in the most relevant prior art, making the conceptual advances largely incremental. The main technical distinction is the use of a CTC-based non-autoregressive streaming decoder with a LLaMA backbone, along with notable resource efficiency, though these improvements may stem from implementation choices rather than fundamental innovation. The introduction of the InstructS2S-200K dataset is a useful contribution, but similar datasets exist and direct comparisons are lacking. Overall, the submission is a strong instantiation of current trends, but its core contributions should be more rigorously contextualized against closely related recent works, as the claimed novelty is often overstated and the technical delta is modest.",
    "candidate_b_text": "This paper introduces LLama-Omni, an innovative end-to-end model architecture for real-time speech interaction with large language models (LLMs). The method is similar to previous work such as SALMONN, except for the TTS module, which is replaced by a vocoder in this study. However, the authors do not sufficiently discuss the differences between the proposed method and SALMONN.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {}
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 1
  },
  {
    "id": 86,
    "paper_id": "QFaj7InstQ",
    "reference_text": "This paper introduces the Item Language Model (ILM), a framework designed to bridge behavioral embeddings from recommendation systems with language understanding in Large Language Models (LLMs). The core contribution lies in adapting a Querying Transformer (QFormer) architecture with a novel item-item contrastive loss, enabling unified and interleaved processing of both behavioral and textual information for language generation tasks. The approach demonstrates innovation in its use of QFormer with item-item contrastive learning to address the modality gap between recommendation signals and language understanding. While this represents a creative step toward integrating behavioral and semantic information, the novelty is somewhat limited by the existence of related work, such as LC-Rec and BinLLM, which also aim to bridge collaborative and semantic signals within large language models. The main distinction of this paper is the specific adaptation of QFormer and the introduction of the item-item contrastive loss, which is well-justified and empirically validated. However, a more thorough comparison with these recent LLM-based recommendation systems would help to better establish the unique contributions and boundaries of novelty. Overall, the paper offers an interesting and incremental advance in the integration of recommendation and language modeling, but its novelty would be strengthened by a clearer articulation of differences from closely related approaches.",
    "candidate_a_text": "This assessment finds that the \"Item Language Model\" (ILM) submission presents incremental advances in the integration of behavioral embeddings into large language models for recommendation, primarily through the use of a Querying Transformer and an item-item contrastive loss. While these architectural and loss function choices are technically distinct, they represent variations on well-established approaches in a rapidly evolving and competitive field. The conceptual framework and problem space are shared with several recent works (e.g., CoLLM, User-LLM, CALRec), some of which are not cited or directly compared against, limiting the clarity of the submission's true novelty. The evaluation methods and claimed improvements are standard for the area, and the submission does not convincingly demonstrate unique capabilities or tasks beyond prior art. Overall, the work is a routine extension of existing research, and reviewers should recognize its contributions as incremental rather than groundbreaking, with a need for more thorough comparison to closely related methods.",
    "candidate_b_text": "This paper proposes a novel approach by adapting the Querying Transformer model to align behavioral and textual data in recommendation systems, integrating collaborative filtering embeddings with large language models. Although these methods are mentioned in the related work, a more detailed comparison with other methods that integrate collaborative filtering with LLMs, such as CoLLM and OpenP5, would strengthen the paper.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {}
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 22
  },
  {
    "id": 41,
    "paper_id": "aueXfY0Clv",
    "reference_text": "The proposed Depth Pro model employs a ViT architecture for zero-shot metric monocular depth estimation, targeting applications such as novel view synthesis. While Depth Pro benefits from pretrained ViT backbones, its architecture primarily builds on existing elements rather than introducing fundamentally new mechanisms for depth estimation, which limits its architectural novelty. The paper introduces a two-stage training approach that integrates synthetic and real-world datasets, enhancing depth boundary accuracy, and includes new metrics for evaluating depth boundaries that address a gap in existing benchmarks by focusing on boundary precision, which is critical for applications like view synthesis that demand fine details.",
    "candidate_a_text": "This paper presents a foundation model for zero-shot metric monocular depth estimation that is capable of producing metric depth maps with absolute scale on arbitrary images without requiring metadata such as camera intrinsics. The model's ability to produce metric depth maps with absolute scale on arbitrary images without requiring metadata such as camera intrinsics is a significant advancement in the field of monocular depth estimation. The high-resolution output and fast processing time make it suitable for real-time applications. The model's superior performance in sharp delineation of object boundaries and focal length estimation across multiple datasets demonstrates its robustness and accuracy.",
    "candidate_b_text": "This paper introduces a foundational model for single-image depth estimation, trained on a large collection of datasets, resulting in high-resolution depth maps with sharp object boundaries and enabling the estimation of camera intrinsic parameters. Although effective, the proposed method's novelty is somewhat constrained as it leverages existing approaches; the loss functions and training strategy draw heavily from DPT (Ranftl et al., 2022), and the metric depth estimation follows ideas from Metric3D (Yin et al., 2023). The contributions of this work are significant, with the pre-trained model holding substantial potential for a range of downstream tasks, but more discussion on the unique aspects would strengthen the paper.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "Human Review 3",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 3,
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "2": {
            "2": true,
            "3": true
          },
          "3": {
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 11
  },
  {
    "id": 68,
    "paper_id": "QoDDNkx4fP",
    "reference_text": "This paper proposes a novel framework, ETA (Evaluating Then Aligning), which addresses a critical gap in the safety of Vision Language Models (VLMs) by focusing on inference-time alignment. Unlike existing approaches that primarily rely on extensive fine-tuning or are limited to specific types of inputs, ETA offers a fresh perspective by combining multimodal evaluation and bi-level alignment without requiring additional training. This plug-and-play nature makes it a highly original contribution, providing a more flexible and scalable solution for enhancing VLM safety. The significance of this work is substantial, as it introduces a method that does not require fine-tuning and can be easily integrated into existing systems, making it practical for widespread use. By improving safety while maintaining model efficiency and general capabilities, ETA could encourage broader adoption of VLMs.",
    "candidate_a_text": "ETA proposes a modular, plug-and-play, inference-time safety alignment framework for vision-language models (VLMs), combining explicit multimodal evaluation (visual and textual) with bi-level alignment (interference prefix and best-of-N search). Its main contribution is the systematic integration and empirical validation of these established techniques, rather than introducing fundamentally new algorithms. While ETA demonstrates improved safety and utility over prior methods, its novelty is incremental, as similar modular inference-time defenses (e.g., ECSO, AdaShield, MLLM-Protector) exist, and several relevant recent works are omitted from the comparison. The authors’ claims of unique novelty and prior work limitations are somewhat overstated, though their engineering advance is meaningful. Reviewers should weigh ETA’s practical integration and robust evaluation against the incremental nature of its contribution and the need for more comprehensive contextualization within the rapidly evolving field.",
    "candidate_b_text": "This paper proposes a new safeguard mechanism for the Vision Language Model (VLM) during the inference phase. However, although the simplicity of the proposed methods, the novelty seems significantly limited, as the proposed methods are based on the simple application of existing models such as CLIP and Reward Models.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "Human Review 1",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 1,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          1
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": false
          },
          "1": {
            "0": false,
            "1": true
          }
        },
        "agreement_scores": {
          "0": 0.0,
          "1": 0.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 24
  },
  {
    "id": 1,
    "paper_id": "3b9SKkRAKw",
    "reference_text": "This paper introduces a novel 3D lesion inpainting method, LeFusion, which uses diffusion models to address data scarcity in medical imaging by generating synthetic lesions in lung CT and cardiac MRI scans for data augmentation in lesion segmentation tasks. The authors identify that existing lesion inpainting methods struggle to preserve anatomically accurate backgrounds alongside the inpainted lesion, and LeFusion is introduced to address this challenge with a lesion-focused diffusion loss and background preservation at inference time using RePaint. While the Introduction and Background sections seem to imply that the proposed lesion-focused loss is a novel contribution proposed for the first time by the authors, this might not be necessarily true considering that there have been other works that employ similar approaches [2, 3]; mentioning them could further strengthen the contextualisation of the approach. The paper provides multiple key contributions which not only address the research gap but also deal with modality-specific challenges related to lesion texture and shape heterogeneity, and these claims are well supported by the results.",
    "candidate_a_text": "LeFusion is presented as a lesion-focused diffusion model for medical image synthesis, claiming explicit separation of lesion and background, fine-grained control via histogram-based texture conditioning, and multi-channel decomposition for multi-class lesion generation. While the authors assert several \"firsts,\" independent analysis shows that most contributions—such as mask conditioning, multi-channel decomposition, and mask-based control—are already established in recent diffusion and GAN-based works, making LeFusion primarily an incremental advance. The main technical novelty appears to be the specific implementation of histogram-based texture control and the integration of forward-diffused background into the reverse process, though the conceptual groundwork exists in prior literature. The authors' rhetoric sometimes overstates the novelty and underplays similarities to closely related works, with some relevant prior methods not fully cited or compared. Overall, LeFusion is a solid, well-executed refinement of existing techniques, but its contributions are best viewed as technical improvements and combinations rather than paradigm-shifting innovations; reviewers should calibrate their assessment accordingly given the field's maturity.",
    "candidate_b_text": "This manuscript presents a diffusion model that utilizes forward-diffused backgrounds and reverse-diffused foregrounds as inputs, allowing the model to concentrate on reconstructing lesions specifically, and applies a post-processing method to enhance generation quality. The novelty of the proposed approach is limited, as the method does not significantly modify the underlying conditional diffusion process but instead introduces variations solely in the input. I would appreciate revisions to Figures 1 and 2 to more clearly illustrate the novelty of your proposed approach and to focus on presenting a straightforward and cohesive pipeline that highlights the mechanisms unique to your method.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "Human Review 3",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 3,
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          2,
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "2": true,
            "3": true
          },
          "2": {
            "1": true,
            "2": true,
            "3": false
          },
          "3": {
            "1": true,
            "2": false,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "2": 0.5,
          "3": 0.5
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 27
  },
  {
    "id": 75,
    "paper_id": "miIE56qM10",
    "reference_text": "This paper introduces an approach that trains an external lightweight model to mitigate inherent biases in LLMs, such as overconfidence and underconfidence. While the proposed model demonstrates enhanced performance and generalization capabilities across multiple models and datasets, the novelty is limited by the fact that existing work has explored training an external calibrator specifically for LLMs. The paper proposes a post-processing calibration method that requires task-specific training data, but the baselines used for comparison are confidence elicitation techniques (pre-processing methods) that do not require training data, making the comparison potentially unfair. Established post-processing techniques like isotonic regression, Platt scaling, and recalibration with Bayesian neural networks should be considered, and the authors should cite relevant work, include applicable baselines, and justify any inapplicable baselines.",
    "candidate_a_text": "This paper proposes a novel approach to uncertainty estimation in LLMs by training a corrector model to fix the miscalibration of logit-based uncertainty scores. The method is novel and addresses a significant challenge in uncertainty estimation for LLMs. The method can be integrated with various logit-based uncertainty methods, and the empirical results show consistent improvements on TriviaQA and SciQA.",
    "candidate_b_text": "This paper focuses on uncertainty estimation of LLMs and proposes to introduce an external corrector for refining uncertainty estimation, which is a Roberta-based regression model predicting a value added to the uncertainty score estimated by baselines. However, Liu et al. [3] generated their training set in a manner similar to this work and introduced a calibration mechanism to adjust logits, reducing the novelty of the proposed data construction process. Additionally, Ulmer et al. [4] propose an external model to estimate the confidence of LLM outputs, closely resembling the proposed method, which also trains an external model for confidence prediction. This potentially diminishes the novelty of the external corrector.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "Human Review 2",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 2,
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          2,
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "2": true,
            "3": true
          },
          "2": {
            "1": true,
            "2": true,
            "3": true
          },
          "3": {
            "1": true,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 9
  },
  {
    "id": 57,
    "paper_id": "AAjCYWXC5I",
    "reference_text": "This paper introduces a zero-shot in-context adversarial learning framework for Large Language Models (LLMs) aimed at enhancing research ideation. The approach leverages a multi-agent system inspired by the academic peer review process, with distinct proposer, reviewer, and area chair roles, to iteratively refine research ideas along the axes of novelty and feasibility. This framework fills a notable gap in the field by conceptually adapting adversarial learning to the context of LLM-driven idea generation, an area that has seen limited exploration. The use of a peer review-inspired multi-agent setup to promote iterative improvement in idea generation represents a conceptually novel contribution, distinguishing the work from prior approaches that typically do not model such structured, adversarial interactions among LLM agents. Overall, the paper’s novelty lies in its creative adaptation of adversarial learning principles and academic peer review dynamics to the automated ideation process, offering a fresh perspective and a promising direction for advancing LLM-based research support systems.",
    "candidate_a_text": "This paper introduces a zero-shot in-context adversarial learning framework to enhance research ideation using multi-LLM-agent interactions, where a proposer agent, reviewer agent, and area chair agent iteratively refine research ideas along novelty and feasibility dimensions, mirroring the academic review process. The multi-agent framework is well-designed, highly engaging, and interactive. The authors also propose a relative quality ranking metric to evaluate idea generation quality, showing that this approach significantly improves the novelty and feasibility of generated research ideas.",
    "candidate_b_text": "This paper presents a novel approach to enhancing research ideation using large language models (LLMs) by introducing a \"zero-shot in-context adversarial learning\" framework specifically tailored for research idea generation. The method draws inspiration from generative adversarial networks (GANs) but adapts the adversarial paradigm to the unique challenges posed by LLMs and open-ended ideation tasks. This represents a creative combination of existing concepts applied to a new domain, as the adversarial learning framework is operationalized through a unique multi-agent system in which each agent (Proposer, Reviewer, Area Chair) plays a distinct role in refining research ideas. Additionally, the paper proposes a new evaluation metric, the \"relative quality ranking score,\" designed to assess the novelty and feasibility of LLM-generated research ideas relative to a benchmark of human-generated ideas. This metric addresses the challenge of evaluating open-ended text generation by moving beyond traditional metrics and offering a more nuanced assessment. While the paper positions both the adversarial learning framework and the relative quality ranking metric as novel contributions, the novelty primarily lies in the creative integration and adaptation of existing ideas rather than the introduction of entirely new theoretical constructs. The GAN-inspired framing, while innovative, is more metaphorical than rigorously mathematical, and the multi-agent system mimics established peer review dynamics in a new context. Furthermore, although the relative quality ranking metric is presented as a novel evaluation tool, a more thorough discussion of its relationship to existing metrics for open-ended text generation would strengthen the novelty claim, as it remains unclear whether similar approaches have been explored in prior literature. Overall, the paper offers a fresh and thoughtfully integrated approach to LLM-driven research ideation, but its novelty is best characterized as a creative synthesis and domain-specific adaptation of established ideas, rather than a fundamentally new paradigm.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "Human Review 1",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 1,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          1
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": false
          },
          "1": {
            "0": false,
            "1": true
          }
        },
        "agreement_scores": {
          "0": 0.0,
          "1": 0.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 29
  },
  {
    "id": 16,
    "paper_id": "uy9oR0nYCW",
    "reference_text": "This paper proposes a conceptual explainability framework for audio deepfake detection, emphasizing sample-specific, time-specific, and feature-specific explanations interpretable by humans, and provides empirical evaluations of explainability methods for transformer-based detection models using occlusion and attention visualization techniques. However, both the proposed methods for audio explainability, occlusion and attention visualization, are already existing methods that are very common in literature, especially for vision and language tasks, and the paper did not make any modality-specific changes or edit the methods in any way for the audio domain. The roll-out attention method was introduced for natural language tokens, and while the paper claims to adapt the method for audio tokens, it is unclear what kind of modifications were made except just replacing the tokens, making the novelty introduced by the paper in these methods questionable. The paper also claims to introduce a novel benchmark to evaluate the generalization capabilities of deepfake audio classifiers by training on one dataset and evaluating on another, but this mechanism is already well-known and well-practiced, with no new contribution to the dataset, evaluation metric, or other changes, so it is questionable to consider this benchmark as a contribution. Thus, the novelty and originality of the paper’s contributions are limited.",
    "candidate_a_text": "This paper analyses existing explainable methods, such as Occlusion and Attention visualization, for deepfake audio detection tasks using three baseline models and two existing datasets. The paper is merely an analysis paper of existing explainable methods, and there are no significant novel contributions to this paper. Although the authors mention in line 052 that the contributions are \"Empirical evaluations of novel explainability methods for audio transformers,\" I cannot judge what novelty is in attention visualization for transformers, as it has happened a lot in literature. More than 50% of the paper is just explaining trivial and non-paper contributions only.",
    "candidate_b_text": "This assessment finds that the submission addresses two active themes in audio deepfake detection—explainability for transformer-based models and generalization to real-world, cross-dataset scenarios—by adapting known attention roll-out mechanisms and proposing a tailored conceptual framework. While the authors claim novelty in these areas, the assessment concludes that the main contributions are incremental adaptations of established XAI techniques, with the primary innovation being their application to audio deepfake detection rather than the development of fundamentally new methods. The introduction of a new cross-dataset benchmark (ASVspoof 5 → FakeAVCeleb) is a practical contribution, but cross-dataset evaluation is already standard in the field. The empirical evaluation is thorough, yet similar comparative studies exist, and the submission omits discussion of several highly relevant recent works, sometimes overstating the limitations of prior art. Overall, the submission’s value lies in its comprehensive integration of existing methods and resources, but reviewers should be aware that its conceptual advances are modest and its claims of novelty may be overstated.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "3": true
          },
          "1": {
            "0": true,
            "1": true,
            "3": true
          },
          "3": {
            "0": true,
            "1": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "1": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 30
  },
  {
    "id": 32,
    "paper_id": "DSsSPr0RZJ",
    "reference_text": "This paper introduces DSBench, a comprehensive benchmark designed to evaluate data science agents on realistic tasks derived from ModelOff and Kaggle competitions. By integrating a wide range of data analysis and modeling challenges, DSBench aims to provide a rigorous and practical environment for assessing agent performance under real-world conditions. The benchmark is further distinguished by the introduction of the Relative Performance Gap (RPG) metric, which seeks to normalize evaluation across diverse modeling tasks. The contribution is notable in that it pushes the boundaries of traditional benchmarking by moving beyond synthetic or narrowly scoped tasks, instead offering a fresh and more holistic approach to evaluating data science agents. DSBench sets a new standard in the field, driving advancements in realistic, end-to-end task performance and supporting the development of intelligent, autonomous data science agents. Overall, the paper’s novelty lies in both the scale and realism of the benchmark, as well as the effort to address evaluation inconsistencies through the RPG metric, marking a significant and original step forward in data science benchmarking.",
    "candidate_a_text": "This paper introduces DSBench, a comprehensive benchmark designed to evaluate the performance of data science agents on realistic tasks sourced from ModelOff and Kaggle competitions. The benchmark consists of 466 data analysis tasks and 74 data modeling tasks, and provides a thorough evaluation of multiple state-of-the-art LLMs, LVLMs, and agents, offering a comprehensive comparison of their performance on these tasks. The benchmark tasks are sourced from real-world competitions, making the evaluation more realistic and relevant to practical applications in the field of data science.",
    "candidate_b_text": "DSBench is a new benchmark for data science agents that distinguishes itself primarily by sourcing tasks from real-world competitions (ModelOff and Kaggle), offering a large and diverse set of data analysis and modeling challenges. While the authors claim comprehensive realism and multimodality, these aspects are already addressed by recent benchmarks such as Spider2-V, OSWorld, and InfiAgent-DABench, making DSBench’s main novelty its specific task sourcing and explicit coverage of both analysis and modeling. The introduction of the Relative Performance Gap (RPG) metric for normalized evaluation across heterogeneous tasks is a useful, though incremental, methodological contribution. Some claims of uniqueness—particularly regarding multimodality, end-to-end workflows, and revealing agent-expert performance gaps—are overstated, as similar features and findings exist in prior work. Overall, DSBench is a timely and relevant addition to the field, but reviewers should recognize that its advances are incremental rather than fundamentally transformative within the current landscape of data science agent benchmarks.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {}
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 49
  },
  {
    "id": 9,
    "paper_id": "F0GNv13ojF",
    "reference_text": "This paper investigates how to effectively leverage reward models during reinforcement learning (RL) training to enhance large language models' (LLMs) mathematical reasoning abilities. The authors identify key limitations in both Outcome-supervised and Process-supervised Reward Models, notably revealing that the latter can lead to reward hacking through unnecessary repetition of steps. To address these issues, the paper introduces two techniques—Clipping and Delta mechanisms—designed to prevent reward exploitation while preserving the benefits of process rewards, and demonstrates their effectiveness across multiple LLMs and benchmarks. While the identification of reward hacking in process-supervised settings and the systematic analysis of reward model limitations represent valuable contributions, the novelty of the proposed solutions is limited. Clipping is a well-established technique in the RL community, commonly used to stabilize training, and the concept of bounded rewards is a fundamental principle in RL. Similarly, the Delta mechanism, though presented as novel, essentially implements reward shaping, which is another well-known concept in the field, as discussed in prior work such as Ng et al. (Policy Invariance Under Reward Transformations), Harutyunyan et al. (Hindsight credit assignment), and Arjona et al. (Rudder). Overall, the paper's contribution is more incremental than innovative, primarily applying established RL techniques to the specific context of LLM reasoning. The main finding—that reward models can be exploited during training—is somewhat expected given the general challenges of reward design in RL. The paper would benefit from more clearly articulating how the proposed mechanisms differ from existing approaches to better establish their novelty.",
    "candidate_a_text": "This paper delves into the effectiveness of reward models in enhancing the reasoning capabilities of large language models during reinforcement learning training, and proposes two novel reward refinement techniques: Clipping and Delta, aimed at preventing reward hacking and ensuring a bounded RL objective. The authors use synthetic solutions to analyze reward hacking, which is a novel and insightful approach. While the proposed Clipping and Delta mechanisms are simple yet effective solutions to the reward hacking problem, and the paper provides a thorough analysis of this issue, the effectiveness of these techniques is not thoroughly validated and the paper's claims about the generalizability and robustness of the proposed methods are not fully supported by the evidence provided. The evaluation is primarily focused on the Qwen series of models and mathematical reasoning benchmarks (MATH and GSM8K), which raises concerns about the broader applicability and significance of the contributions beyond these settings.",
    "candidate_b_text": "This submission systematically analyzes the use of process-supervised reward models (PRM/ORM) as RL rewards for LLM mathematical reasoning, focusing on training-time dynamics and identifying issues such as \"training collapse\" due to reward misspecification. The authors introduce two reward refinement techniques—Clip (a standard reward clipping adaptation) and Delta (a more novel, stepwise improvement-based reward)—to mitigate these issues, and empirically validate their effectiveness on standard math benchmarks. While the empirical focus on training-time failure modes and the Delta mechanism represent incremental advances, many of the techniques (especially Clip) are routine adaptations of established RL methods, and the claim of \"first in-depth evaluation\" is somewhat overstated given related prior work. The submission's main contribution is a detailed empirical study and practical demonstration of reward refinement, rather than fundamentally new concepts or algorithms. Overall, the work is a valuable but incremental addition to a rapidly evolving field, and its novelty should be interpreted in the context of extensive existing research on reward model robustness, reward hacking, and normalization.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {}
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 32
  },
  {
    "id": 43,
    "paper_id": "e8c7XDRJcg",
    "reference_text": "The paper presents three methods to perform Unsupervised Modality Adaptation (UMA): Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer Through Time (C3T). Unsupervised Modality Adaptation is frequently used for cross-modal learning in many applications and cannot be regarded as a new contribution of this paper. Compared to ST and CA, the Cross-modal Transfer Through Time (C3T) performs pseudo label generation and alignment in temporal sequences, while other operations, including feature learning, RGB-IMU alignment, and classification, follow the same procedures, so the novelty of the proposed method needs to be reconsidered. The experiment comparison is not reliable since the proposed approach is not compared with related frameworks, and there are many existing works as listed in Section 2. I suggest considering refining the key innovations and redefining the key representations for innovation.",
    "candidate_a_text": "This paper proposes a cross-modal transfer method for human action recognition, based on the assumption that there is a shared latent representation space for different modalities, and includes three variants: student-teacher, contrastive alignment, and cross-modal transfer through time. The novelty of the paper is limited, as the assumption of a shared latent representation space for different modalities has been proposed by many previous works. While the idea of aligning modalities in a common latent space is well-established, the paper needs to more clearly articulate the specific novel contributions beyond the existing literature on shared latent spaces. For example, the 'cross-modal transfer through time' (C3T) method should be explained in more detail, highlighting how it differs from existing temporal alignment techniques, and the paper should emphasize the unique aspects of its approach.",
    "candidate_b_text": "The paper investigates an unsupervised modality adaptation method for the action recognition model, proposing Cross-modal Transfer Through Time (C3T), which performs alignment on each temporal step rather than on video-level feature representation as in Contrastive Alignment (CA). The novelty is minor, which is only on doing the alignment per temporal step, without proposing a new alignment method or an architectural change to current state-of-the-arts. The lack of state-of-the-art comparison also makes it hard to justify the significance of the novelty besides its improvement upon Student-Teacher (ST) and CA methods.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "Human Review 3",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 3,
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "3": true
          },
          "3": {
            "1": true,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 12
  },
  {
    "id": 78,
    "paper_id": "uuvujfQXZy",
    "reference_text": "This paper proposed a novel type of CBM called UCBM, which utilizes concept discovery methods to discover concepts learnt by pretrained, black-box models and convert them into CBM. The paper proposed an unsupervised concept discovery mechanism for CBMs, which is novel in the field of CBM, as the method does not need pre-defined concept sets. Additionally, a novel input-dependent concept selection mechanism is proposed to dynamically retain a sparse set for prediction; however, the idea of input-dependent concept selection is not novel, as, for example, Panousis et al. (2023) introduced a binary latent indicator to dynamically select activating concepts according to input embedding and mask out other concepts. I agree that the introduction of unsupervised concept discovery into CBM is interesting, but the missing of text representation limits the novelty.",
    "candidate_a_text": "This paper proposes learning post-hoc Concept Bottleneck Models (CBMs) in an unsupervised manner, using an unsupervised non-negative matrix factorization to discover concept vectors, an input-dependent alignment score selection, and a sparse linear layer for class prediction. Unlike previous methods, UCBMs do not assume a set of concepts learned by black-box models, but instead discover the concept vectors in an unsupervised manner. The idea of input-dependent sparsity constraint is interesting and non-trivial. However, the proposed method has limited novelty, as it uses an existing matrix decomposition method called CRAFT for feature generation, and the novelty is limited to proposing input-dependent selection applied before learning the sparse linear layer. The paper lacks comparisons with newer methods, particularly with [2], which trains a learnable dictionary to approximate the embedding space of VLMs in a supervised manner without using a pre-defined concept set.",
    "candidate_b_text": "This paper presents a novel approach to converting a black-box model into a concept bottleneck model (CBM) using unsupervised concept discovery, and proposes an input-dependent concept selection mechanism where only a sparse set of concepts relevant to the input are dynamically retained. The idea of using concept discovery for CBM is interesting and novel. However, the paper does not provide comparisons with other concept discovery methods such as [1], other methods that convert a black-box model into a CBM such as [2,3], or with CBMs trained from scratch, which limits the ability to fully assess the originality and significance of the contribution.",
    "candidate_a_label": "Human Review 1",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": 1,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          2
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "2": true
          },
          "1": {
            "0": true,
            "1": true,
            "2": true
          },
          "2": {
            "0": true,
            "1": true,
            "2": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "1": 1.0,
          "2": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 6
  },
  {
    "id": 81,
    "paper_id": "2prShxdLkX",
    "reference_text": "The paper proposes MoDGS, a novel pipeline to render high quality novel views of dynamic scenes from casually captured monocular videos, specifically addressing the limitations of existing methods for such videos by reducing the dependency on rapid camera motion. MoDGS represents an original approach within novel view synthesis and dynamic scene modeling, introducing a 3D-aware initialization mechanism and an innovative ordinal depth loss that maintains depth order among frames rather than relying solely on absolute values. This novel use of ordinal depth loss offers a new perspective on addressing depth consistency issues, with practical implications for improving depth coherence in dynamic scenes captured casually. The method’s ability to handle both static and dynamic elements in monocular videos opens new avenues for monocular depth estimation and dynamic scene modeling, where single-camera approaches have been historically constrained by depth inconsistency issues. By enabling high-quality novel view synthesis from single-camera footage without multiview camera motions, MoDGS broadens the scope of dynamic scene reconstruction and makes it accessible to a wider range of use cases.",
    "candidate_a_text": "This paper presents MoDGS, a method for dynamic scene reconstruction from a monocular video that proposes 3D-aware initialization and an ordinal depth loss to improve performance. The proposed method is a straightforward extension of the deformation-based dynamic Gaussian splatting methods, such as Deformable 3DGS and CogS, and the technical novelty of this paper is limited. While the proposed 3D-aware initialization and ordinal depth loss are intuitive and effective, the overall contribution appears incremental.",
    "candidate_b_text": "MoDGS is an incremental contribution within the dynamic monocular Gaussian Splatting (GS) field, targeting novel view synthesis from casually captured monocular videos with static or slow-moving cameras—a problem also addressed by recent works like MoSca, Dynamic Gaussian Marbles, and Shape of Motion. The main technical advances claimed are a \"3D-aware\" initialization scheme and the integration of an ordinal depth loss, which may improve stability and depth consistency but are refinements of existing ideas rather than conceptual breakthroughs. The authors' claims of being the \"first\" to solve this problem are overstated, as several concurrent works address similar settings with related techniques, and the distinctions are primarily in implementation details. Prior work is sometimes characterized as less capable than it may actually be, and the novelty of MoDGS lies in the specific adaptation and integration of known methods rather than in the overall approach. Reviewers should focus on the empirical significance of these refinements and the thoroughness of comparative evaluation with the most relevant recent methods.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 0.5,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          0,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "2": false,
            "3": false
          },
          "2": {
            "0": false,
            "2": true,
            "3": true
          },
          "3": {
            "0": false,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 0.0,
          "2": 0.5,
          "3": 0.5
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 35
  },
  {
    "id": 18,
    "paper_id": "jCNRcHrfLo",
    "reference_text": "This paper introduces a novel approach, Hierarchical Prompts with Context-Aware Calibration (HiCA), to enhance Open Vocabulary Object Detection (OVD). HiCA improves generalization by utilizing hierarchical prompts that map object regions through both coarse- and fine-grained knowledge, capturing shared information across base and novel classes. Additionally, context-aware calibration refines detection by linking contextual information with object categories, reducing background interference. Despite the emphasis on enhancing detection for novel classes in OVD, the improvement shown in Tables 1 and 4 is relatively minor, and most modifications do not outperform simpler, template-based methods for novel classes, which raises concerns about the practicality and overall impact of the proposed approach for new class detection.",
    "candidate_a_text": "This paper proposes hierarchical prompts with context-aware calibration (HiCA) for open-vocabulary object detection, leveraging both coarse-grained superclass knowledge and fine-grained category knowledge to improve generalization to novel classes while utilizing the visual context of the image to enhance these effects. However, I find that the proposed hierarchical prompts method is quite similar to the learnable multi-modal prompts method in [1], where multiple learnable text tokens are also used to prompt the text encoder for generating text embeddings. While the authors do not use learnable visual prompts as in [1], I still think the proposed hierarchical prompts method is an incremental work based on [1]. The originality of the method appears limited by this similarity, and a comparison experiment with [1] is necessary to better establish the specific contribution.",
    "candidate_b_text": "The submission introduces HiCA, a method for open-vocabulary object detection that combines explicit hierarchical prompts (superclass/class) with context-aware calibration via unsupervised visual context clustering and a distribution generation layer, all within a prompt learning framework. While the technical implementation—particularly the use of hierarchical prompts and unsupervised context clustering for calibration—is novel, the underlying conceptual ideas of leveraging semantic hierarchies and contextual information are well-established in recent literature (e.g., HierKD, DetCLIPv3, HTRPN). The authors tend to overstate their novelty, underrepresenting the sophistication and relevance of prior work that already integrates hierarchy and context, albeit through different mechanisms such as distillation or generative modeling. The main delta lies in the specific combination and technical realization of these ideas within prompt learning, rather than in a fundamentally new conceptual advance. Reviewers should note that, in this mature and rapidly evolving field, most recent contributions—including this one—are incremental and technical, and direct comparison to the most relevant recent works is needed to fairly assess the true contribution.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.5,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": false,
            "3": true
          },
          "1": {
            "0": false,
            "1": true,
            "3": false
          },
          "3": {
            "0": true,
            "1": false,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 0.5,
          "1": 0.0,
          "3": 0.5
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 5
  },
  {
    "id": 56,
    "paper_id": "p6eQRlaxGo",
    "reference_text": "This paper presents AtomFlow, a flow matching model for designing a protein structure to bind a small molecule ligand, which jointly denoises the structure of the protein and ligand and does not require knowledge of the ligand pose, unlike RFDiffusionAA. The methodology can be described as a flow-matching version of RFDiff-AA and does not score high on originality or novelty from a ML perspective. Further, the flow model architecture and noising process are based on AlphaFlow, with different justification but no difference in practice as far as I can tell. The overall significance of the contribution is unclear as it represents an incremental methodological advance over RFDiff-AA with more or less the same model capabilities. The authors argue that not needing to specify the ligand pose is a big plus, but no meaningful evidence or use case is provided for this distinction.",
    "candidate_a_text": "This paper introduces ATOMFLOW, a generative model designed to create ligand-binding proteins from 2D molecular graphs without requiring the pre-known 3D structure of the target molecule. The authors propose a flow matching framework for designing protein-ligand complexes that can generate both the protein and ligand conformations simultaneously, leveraging a unified biotoken representation for both proteins and ligands to enable seamless interaction modeling between different biomolecules. While the model demonstrates comparable performance to RFDiffusionAA, a state-of-the-art model, the innovation compared to RFDiffusionAA is limited and the model does not seem to outperform it. The paper also lacks a comparison with other pocket design methods, such as PocketGen and FlowSite, and it would be beneficial to discuss the advantages of ATOMFLOW over these methods.",
    "candidate_b_text": "This paper presents a deep generative model called ATOMFLOW, which is designed to generate ligand-binding proteins from scratch and operates on the quotient space of all possible alignments of N atoms in R^3, with each structure represented as a series of 3D tokens. The proposed method is technically sound and innovative, and the experimental results demonstrate that ATOMFLOW outperforms the state-of-the-art model, RFDiffusionAA, in terms of self-consistency, binding affinity, diversity, and novelty.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          2,
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "2": true,
            "3": true
          },
          "2": {
            "1": true,
            "2": true,
            "3": true
          },
          "3": {
            "1": true,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 37
  },
  {
    "id": 17,
    "paper_id": "NsFZZU9gvk",
    "reference_text": "This paper introduces a benchmark consisting of 100 harmful behaviors for red-teaming web agents, aiming to evaluate the alignment and safety of such agents in practical scenarios. While the authors appear to be among the first to propose a dedicated benchmark for red-teaming web agents by the submission deadline, the novelty of the contribution is limited. There is already a substantial body of related work in this domain, such as Tian et al. (2023), and the paper does not sufficiently discuss how its approach differs from or advances beyond these existing methods. Furthermore, the red-teaming methodology itself lacks novelty, as it primarily involves manually crafting or transforming examples from prior work, without introducing automated or agent-specific techniques. Simply designing a set of 100 examples for red-teaming does not constitute a significant advance, and the paper would benefit from a more comprehensive comparison with prior toolkits and a clearer articulation of its unique contributions. Overall, while the benchmark is an early effort in this direction, its incremental nature and lack of methodological innovation limit its impact.",
    "candidate_a_text": "The paper introduces a benchmark for evaluating the safety of LLM-powered browser agents, consisting of 100 harmful tasks that browser agents should not be able to accomplish. The paper highlights an important, timely, and under-explored problem, and I believe that the benchmark introduced in the paper could be very valuable to the research community.",
    "candidate_b_text": "This assessment finds that the submission offers a substantive and timely contribution by introducing BrowserART, a comprehensive, publicly released toolkit for red teaming browser-based LLM agents, and by empirically demonstrating a significant alignment gap between chat LLMs and their browser agent counterparts. The work is well-positioned within the rapidly evolving field of LLM safety, extending systematic evaluation to the relatively underexplored domain of browser agents. However, the authors’ claims of being the \"first\" comprehensive study in this area are overstated, as they omit citation and comparison to recent, directly relevant agentic jailbreak literature (e.g., \"Refusal-Trained LLMs Are Easily Jailbroken As Browser Agents\"). The technical advances—while incremental—are valuable, particularly in the breadth and agent-specific focus of the toolkit and the robust empirical results. Reviewers should weigh the contribution’s novelty in the context of recent agentic studies and recognize that the main differentiation lies in the scale and public release of the toolkit, rather than in conceptual breakthroughs.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {}
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 38
  },
  {
    "id": 19,
    "paper_id": "miIE56qM10",
    "reference_text": "This paper introduces an approach that trains an external lightweight model to mitigate inherent biases in LLMs, such as overconfidence and underconfidence. While the proposed model demonstrates enhanced performance and generalization capabilities across multiple models and datasets, the novelty is limited by the fact that existing work has explored training an external calibrator specifically for LLMs. The paper proposes a post-processing calibration method that requires task-specific training data, but the baselines used for comparison are confidence elicitation techniques (pre-processing methods) that do not require training data, making the comparison potentially unfair. Established post-processing techniques like isotonic regression, Platt scaling, and recalibration with Bayesian neural networks should be considered, and the authors should cite relevant work, include applicable baselines, and justify any inapplicable baselines.",
    "candidate_a_text": "This paper focuses on uncertainty estimation of LLMs and proposes to introduce an external corrector for refining uncertainty estimation, which is a Roberta-based regression model predicting a value added to the uncertainty score estimated by baselines. However, Liu et al. [3] generated their training set in a manner similar to this work and introduced a calibration mechanism to adjust logits, reducing the novelty of the proposed data construction process. Additionally, Ulmer et al. [4] propose an external model to estimate the confidence of LLM outputs, closely resembling the proposed method, which also trains an external model for confidence prediction. This potentially diminishes the novelty of the external corrector.",
    "candidate_b_text": "This submission introduces a modular, external \"Corrector\" model that refines uncertainty estimates from large language models (LLMs) by post-hoc correction of outputs from existing methods, aiming to address issues like over/under-confidence and ranking inversions. The approach is positioned as orthogonal and complementary to current uncertainty estimation techniques, and is designed to be lightweight, model-agnostic, and easily integrated atop various base estimators. While the implementation is practical and flexible, the conceptual novelty is incremental, as similar learnable or ensemble correction models (e.g., MARS, LARS, LM-Polygraph) already exist in the literature. The authors somewhat overstate the uniqueness of their contributions and the limitations of prior work, particularly regarding inversion correction and robustness. Reviewers should focus on the empirical robustness and integration benefits of the Corrector, but recognize that the main advance is a practical extension rather than a paradigm shift.",
    "candidate_a_label": "Human Review 2",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": 2,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          2,
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "2": true,
            "3": true
          },
          "2": {
            "1": true,
            "2": true,
            "3": true
          },
          "3": {
            "1": true,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 39
  },
  {
    "id": 80,
    "paper_id": "p6eQRlaxGo",
    "reference_text": "This paper presents AtomFlow, a flow matching model for designing a protein structure to bind a small molecule ligand, which jointly denoises the structure of the protein and ligand and does not require knowledge of the ligand pose, unlike RFDiffusionAA. The methodology can be described as a flow-matching version of RFDiff-AA and does not score high on originality or novelty from a ML perspective. Further, the flow model architecture and noising process are based on AlphaFlow, with different justification but no difference in practice as far as I can tell. The overall significance of the contribution is unclear as it represents an incremental methodological advance over RFDiff-AA with more or less the same model capabilities. The authors argue that not needing to specify the ligand pose is a big plus, but no meaningful evidence or use case is provided for this distinction.",
    "candidate_a_text": "ATOMFLOW introduces a generative model for protein-ligand complex design that jointly generates both ligand and protein structures from 2D molecular graphs, addressing ligand flexibility and protein-ligand interactions in a unified, end-to-end framework. The main technical advance is the application of SE(3)-equivariant flow-matching at the atomic level, enabling iterative, joint updates of both ligand and protein, which distinguishes it from prior models that either require fixed ligand conformers (e.g., RFDiffusionAA) or are limited to pocket refinement (e.g., PocketGen, FlowSite). The authors’ claim of being the first to enable de novo, all-atom complex generation from 2D ligand graphs is accurate within the generative modeling context, though unified atomic representations have been explored in predictive models like Umol. While the unified \"biotoken\" framework and atomic flow-matching are meaningful extensions, the underlying concepts are increasingly standard in the field; the novelty lies in their integration for this specific generative task. Overall, the submission represents a substantive and well-substantiated advance, though some claims slightly overstate conceptual novelty, and additional comparisons to related predictive and sequence-structure co-design models could further clarify its scope.",
    "candidate_b_text": "This paper presents the ATOMFLOW architecture for the design of protein folds conditioned by the ligand SMILES description. It's a well written technical paper about a novel architecture that can be useful for the bioinformatics community, but there is little technical novelty in the architecture blocks, loss function, or flow matching process (but the architecture is novel and useful). When generating folds for a purine ligand, the results were extremely conservative, with zero novelty and extremely low diversity, preferring the fold that dominantly occurs in the PDB, which suggests a significant bias towards folds that are abundant in the PDB. The designs have scores > 0.5, meaning that the fold structure is the same as in the PDB and no new folds have been discovered. It seems AtomFlow has memorized the space of all protein folds in the PDB and simply selects the one it has seen during training, so I suggest the authors run more cross-validation tests hiding some fold classes from the training and trying to reproduce them in the test.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "Human Review 2",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 2,
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          2,
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "2": true,
            "3": true
          },
          "2": {
            "1": true,
            "2": true,
            "3": true
          },
          "3": {
            "1": true,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 17
  },
  {
    "id": 92,
    "paper_id": "FR2WQcwjG4",
    "reference_text": "This paper focuses on the problem of novelty detection under style shifts and proposes a novelty detection method that crafts an auxiliary OOD set with style features similar to the ID set but with different core features, utilizing a task-based knowledge distillation strategy to distinguish core features from style features. In essence, the performance of the proposed method mainly relies on the quality of the generated data, and this paper only utilizes some commonly-used operations and does not propose any inspired ideas, which lacks novelty. There exist some methods that aim to leverage large-scale models, e.g., CLIP, to solve this challenge, and the authors should introduce these methods and make an analysis. In essence, these operations are commonly used methods and this paper does not propose any inspired ideas, which lacks novelty.",
    "candidate_a_text": "This paper proposes a method for novelty detection in the presence of style shifts using a teacher-student framework that leverages saliency maps from a pretrained teacher network to identify core features of in-distribution samples. The proposed method is well-motivated and theoretically justified. I find the proposed method to be novel and it achieves state-of-the-art results on a wide range of datasets when compared to previous methods.",
    "candidate_b_text": "This assessment finds that the submission sits at the intersection of contrastive learning, synthetic OOD crafting, and teacher-student frameworks, with its main novelty being the use of Grad-CAM to distort core features for OOD sample synthesis and a new distillation objective for student-teacher output alignment/divergence. While the combination of these elements is new, each component has precedents in recent literature, and the field already contains meta-data free, style-robust approaches (e.g., Normalization Perturbation), making some novelty claims less pronounced. The authors’ characterizations of prior work are generally accurate but sometimes overstate limitations or underplay the capabilities of related methods, particularly regarding style robustness and meta-data requirements. Empirical improvements are notable but may be influenced by implementation details, and the practical impact of the proposed OOD crafting mechanism versus existing augmentations should be further isolated. Overall, the submission offers a substantive but incremental advance, with its strongest differentiation in the specific mechanism for saliency-guided OOD crafting and integration with a novel distillation objective, but less so in its broader framework or meta-data free claims.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          1
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true
          },
          "1": {
            "0": true,
            "1": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "1": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 41
  },
  {
    "id": 36,
    "paper_id": "e8c7XDRJcg",
    "reference_text": "The paper presents three methods to perform Unsupervised Modality Adaptation (UMA): Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer Through Time (C3T). Unsupervised Modality Adaptation is frequently used for cross-modal learning in many applications and cannot be regarded as a new contribution of this paper. Compared to ST and CA, the Cross-modal Transfer Through Time (C3T) performs pseudo label generation and alignment in temporal sequences, while other operations, including feature learning, RGB-IMU alignment, and classification, follow the same procedures, so the novelty of the proposed method needs to be reconsidered. The experiment comparison is not reliable since the proposed approach is not compared with related frameworks, and there are many existing works as listed in Section 2. I suggest considering refining the key innovations and redefining the key representations for innovation.",
    "candidate_a_text": "This assessment finds that the submission addresses Unsupervised Modality Adaptation (UMA) for Human Action Recognition (HAR), formalizing a strict setting where no target modality labels are available during training—a meaningful but incremental specialization over prior cross-modal transfer work. The proposed C3T method combines contrastive alignment and teacher-student paradigms, introducing temporal convolutions and shared self-attention for cross-modal temporal alignment; however, these are technical variations rather than fundamentally new concepts, as similar mechanisms exist in prior transformer-based models. The authors’ claims of novelty are somewhat overstated, particularly regarding the unexplored nature of UMA and the assertion that all prior methods require labeled data in both modalities, since some recent works (e.g., IMU2CLIP, ImageBind) do support unsupervised or zero-shot transfer. The strongest differentiation lies in the explicit formalization and systematic evaluation of UMA in HAR, as well as the robustness analysis to temporal noise, though empirical gains may stem from implementation details rather than conceptual advances. Overall, the submission is well-situated within the evolving literature on cross-modal transfer and multimodal HAR, but its technical contributions are incremental, and a more nuanced comparison to closely related works is warranted.",
    "candidate_b_text": "This paper proposes a cross-modal transfer method for human action recognition, based on the assumption that there is a shared latent representation space for different modalities, and includes three variants: student-teacher, contrastive alignment, and cross-modal transfer through time. The novelty of the paper is limited, as the assumption of a shared latent representation space for different modalities has been proposed by many previous works. While the idea of aligning modalities in a common latent space is well-established, the paper needs to more clearly articulate the specific novel contributions beyond the existing literature on shared latent spaces. For example, the 'cross-modal transfer through time' (C3T) method should be explained in more detail, highlighting how it differs from existing temporal alignment techniques, and the paper should emphasize the unique aspects of its approach.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "3": true
          },
          "3": {
            "1": true,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 42
  },
  {
    "id": 20,
    "paper_id": "tKFZ53nerQ",
    "reference_text": "This paper presents a method for Topic and Description Reasoning Generation (TDRG) that leverages fine-tuned large language models (LLMs) to generate topics and readable descriptions from user-contributed comments, aiming to improve interpretability over traditional topic modeling approaches. While the study offers valuable insights into the practical implementation and benefits of fine-tuning LLMs for this specific task, its novelty is limited. The research primarily builds upon established techniques in natural language processing, particularly the application of LLMs, and applies them to a well-defined problem space without introducing new algorithms or innovative methodologies that substantially differentiate it from prior work. The main contribution lies in demonstrating the effectiveness of fine-tuning for improving task-specific outcomes in TDRG, but this is considered an incremental advance rather than a novel contribution in the broader context of NLP research. To enhance its impact and originality, the paper would benefit from integrating novel elements, such as developing new fine-tuning strategies, introducing innovative evaluation metrics, or applying the TDRG method to unexplored domains. As it stands, the work functions more as a case study of LLM application than as a source of significant theoretical or methodological advancement.",
    "candidate_a_text": "This paper introduces Topic and Description Reasoning Generation (TDRG), a novel approach that leverages large language models for direct topic and description generation from user-contributed comments. I find the core idea of using LLMs for this purpose to be a significant strength and a valuable contribution, as TDRG offers a compelling alternative to traditional topic modeling techniques like LDA, which often struggle with short texts and lack interpretability. By moving beyond these traditional methods, the authors propose a well-motivated and innovative approach that generates human-readable descriptions to capture underlying topics, and I appreciate the exploration of both zero-shot and fine-tuning strategies, as well as the integration of multimodal information such as video captions. However, the paper lacks a direct comparison with existing topic modeling methods beyond mentioning LDA as a baseline, which makes it difficult to assess the relative advantages or unique contributions of TDRG within the existing literature. Although the authors have identified a real-world problem and proposed a novel solution, the omission of comprehensive comparisons with state-of-the-art techniques and a lack of thorough analysis of the generated content’s interpretability and quality limit the ability to fully evaluate the originality and significance of the proposed method. Overall, while I view the approach as innovative and potentially impactful, these limitations constrain my assessment of its contribution to the field.",
    "candidate_b_text": "This paper proposes a method called Topic and Description Reasoning Generation (TDRG) that leverages large language models (LLMs) to generate topics and descriptions based on user comments. The paper focuses on a new task of generating topics and descriptions from user comments, which is a novel approach in the field of text inference and generation. However, the authors do not compare their method with any baseline methods, making it difficult to evaluate the effectiveness of TDRG, and the advantages of their approach are not clearly demonstrated in relation to other topic modeling or text generation methods.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {}
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 43
  },
  {
    "id": 26,
    "paper_id": "9aIlDR7hjq",
    "reference_text": "The authors propose a frozen alternative to increase the diversity of synthetic training images generated by diffusion models, which conditions the diffusion model not only on few-shot images (done previously) but augmentations (novelty). The method itself is quite simplistic from a novelty perspective (simply adding augmentations to the conditioning). I would consider this a strength if the results were consistent and strong with a clear storyline for effective use-cases; however, I do not see this as being the case. I do not find the CFG scale experiments as adding significant value, as they are consistent with previous work and do not provide more interesting, surprising, or novel results, which in my opinion waters down the impact of the experimental section.",
    "candidate_a_text": "This paper demonstrates that conditioning the generation process on an augmented real image and a text prompt produces effective synthetic datasets, which benefit downstream tasks, particularly for long-tailed (LT) classification and few-shot classification. However, the technical novelty of this paper is unclear. The concept of combining both augmented images and text prompts seems useful for LT and few-shot classification but lacks novelty. If this approach is not technically original, the paper should at least show a broad variety of downstream tasks that benefit from it, which it did not. The contribution is not clearly articulated, as it’s evident that the synthetic dataset is effective, but it’s unclear for which specific tasks it is most useful, with the focus confined to LT and few-shot classification. Expanding the application scenarios would improve the paper’s impact.",
    "candidate_b_text": "This paper proposes a new method of generating synthetic training images by conditioning the generation process on an augmented real image and a text prompt. The idea of using augmented real images as a reference to guide the generation process is interesting. However, the paper lacks a comparison with a strong baseline that fine-tunes the diffusion model on the target dataset using a method like textual inversion, which makes it difficult to assess the true effectiveness of the proposed method.",
    "candidate_a_label": "Human Review 2",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": 2,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          2,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "2": true,
            "3": true
          },
          "2": {
            "0": true,
            "2": true,
            "3": true
          },
          "3": {
            "0": true,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 20
  },
  {
    "id": 62,
    "paper_id": "ueeqGvQozB",
    "reference_text": "This paper introduces ML4MILP, an original benchmark collection targeted to the MILP domain, aiming to fill the gap of having a large scale, standardized benchmark for evaluating mixed integer linear program solvers. The benchmark is relevant for the ML and DL community, especially as ML-based approaches are becoming popular in this field. The introduction and related work provide a well-written, self-contained overview of existing solvers, techniques, and related benchmarks, highlighting the general lack of a larger standardized benchmark for MILPs. Based on structural and embedding distance between instances, the authors show that existing benchmarks are often more heterogeneous than the MILP benchmark they propose, which allows for some insight into the structure of the proposed and existing benchmarks. However, the overall contribution—introduction of the benchmark and showing the lack of homogeneity of existing classes of instances based on similarity evaluation metrics with respect to structure and neural embedding distances—is somewhat limited in scientific insight and currently results in me having a hard time voting for acceptance at a main ML and DL conference such as ICLR; maybe submitting the paper to a Benchmark and Dataset track of a suitable conference might be more targeted.",
    "candidate_a_text": "This paper focuses on developing a benchmark ML4MILP to evaluate machine learning (ML) based methods for solving mixed-integer linear programs of MILPs. I have questions about how this benchmark compares and contrasts against the benchmark created for the ML4CO Neurips competition (Gasse et al., 2022), and how this competition benchmark (and the existence of MIPlib) affect the \"first open-source benchmark dataset\" claim in line 74.",
    "candidate_b_text": "This paper proposes a new benchmarking framework for solving mixed integer linear programs (MILPs) using machine learning (ML). The proposed ML4MILP framework is a significant contribution that can advance the development of ML-based methods for solving MILPs. The proposed framework is very comprehensive, including a large-scale dataset, appropriate similarity evaluation metrics, a reclassification algorithm, and a baseline library, ensuring that the framework can be used to evaluate a wide range of methods, from traditional optimization solvers to the latest ML-based approaches. The dataset contains instances from both real-world problems and synthetic instances, ensuring that the methods evaluated on it are robust and can be applied to a variety of practical scenarios.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "2": {
            "2": true,
            "3": true
          },
          "3": {
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 18
  },
  {
    "id": 33,
    "paper_id": "cfDbuobmU0",
    "reference_text": "This paper introduces SymTex, a benchmark for evaluating non-monotonic reasoning abilities of large language models, and proposes the MG-SymTex framework for generating non-monotonic reasoning samples in both symbolic and textual forms. While the work aims to address an underexplored area by providing new tasks—tri-state boolean querying and answer set computation—the novelty of the approach is limited. Several recent works, such as $\\delta$-NLI and LogicNMR, have already introduced benchmarks for logical reasoning, including non-monotonic reasoning, and the present paper does not clearly differentiate itself from these prior efforts. The main distinction appears to be the inclusion of symbolic samples, but this addition lacks a strong justification and does not convincingly establish a significant advance over existing benchmarks. Furthermore, the experimental results primarily confirm known limitations of LLMs in handling complex logical tasks, rather than uncovering new insights. As such, the contribution is incremental, and the paper would benefit from more clearly articulating how SymTex advances the state of the art beyond what has already been established in the literature.",
    "candidate_a_text": "This paper introduces SymTex, a new benchmark for evaluating the non-monotonic reasoning capabilities of large language models (LLMs). While the contribution of a large and detailed dataset is clear, I would have appreciated a direct comparison of the dataset construction method with related work, such as Feng et al. (2023), to better understand the uniqueness or originality of the approach. Without such a comparison, it is difficult to precisely determine how novel the benchmark or its construction methodology truly is relative to existing datasets.",
    "candidate_b_text": "SymTex introduces a benchmark for non-monotonic reasoning (NMR) in large language models, distinguished by its systematic, automated pairing of symbolic and textual samples and explicit predicate diversity. While these features represent a useful extension over prior benchmarks like LogicBench and LogicNMR, the advances are primarily incremental, focusing on dataset construction and granularity rather than fundamentally new tasks or evaluation protocols. The authors’ claims of being the \"first\" to provide paired modality evaluation are somewhat overstated, as similar capabilities exist in prior work, though perhaps less systematically. The tri-state querying and answer set computation tasks are routine adaptations of existing evaluation protocols, and the empirical findings largely confirm known limitations of LLMs in NMR. Reviewers should recognize SymTex as a valuable but incremental contribution and may wish to request more detailed, direct comparisons with closely related benchmarks to clarify its true novelty.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {}
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 46
  },
  {
    "id": 79,
    "paper_id": "9Orm76dUuT",
    "reference_text": "This paper presents an approach that extends visual universal adversarial attacks, originally designed for image classification, to large multi-modal models (MLLMs) by optimizing adversarial perturbations in conjunction with a predefined trigger token to elicit specific harmful responses. First, to the best of my knowledge, no prior work has explored attacks in this particular setting, combining adversarial perturbations on images with a triggering token. However, given that existing approaches, such as Dong et al. (2023b), have already demonstrated that visual universal adversarial perturbations (UAPs) can be applied to MLLMs (potentially in a black-box setting as well), the technical novelty of this work appears somewhat limited, especially so given the limited transferability. From a practical perspective, the lack of cross-model transferability significantly reduces the attack's relevance.",
    "candidate_a_text": "This paper proposes a test-time backdoor attack against multimodal large language models (MLLMs), without accessing training data or modifying parameters, where the attack is assigned to the visual modality to set up backdoors and the textual modality is responsible for activating them. The proposed method is straightforward and lacks technical contributions, and the authors do not compare the proposed method with existing methods.",
    "candidate_b_text": "This submission addresses test-time backdoor attacks on multimodal large language models (MLLMs) in black-box settings, claiming novelty in decomposing the attack across visual (setup) and textual (activation) modalities using universal adversarial perturbations. While the authors position their work as the first to demonstrate such attacks without access to training data or model parameters, several recent works (e.g., Jailbreak in Pieces, Visual Role-Play) have already explored similar test-time, black-box, and multimodal attack paradigms. The main technical refinement lies in the explicit separation of setup and activation across modalities, which offers operational flexibility but is conceptually similar to compositional or adaptive triggers in prior work. The submission tends to overstate its novelty and omits direct comparison with the most relevant recent literature, sometimes mischaracterizing prior work as limited to unimodal or training-time settings. Overall, the contribution is best viewed as an incremental technical extension rather than a paradigm shift, and claims of being \"first\" should be moderated in light of the rapidly evolving field.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          2
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "2": true
          },
          "2": {
            "0": true,
            "2": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "2": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 47
  },
  {
    "id": 97,
    "paper_id": "waIltEWDr8",
    "reference_text": "This paper proposes to combine B-cos networks as feature extractor and a relatively common version of few-shot learning. Masking limitations in novelty: lets be clear: The Nadaraya-Watson head is (a simplified version of) few-shot learning. It is a softmax over negative distances between support samples and the test image. While it is appreciated that Wang and Sabuncu, 2022, gave a name to their analysis in order to emphasize a predecessor of few-shot learning, using this term in this paper suggests a larger or different novelty than there actually is. It should be made prominently clear in the manuscript that the Nadaraya-Watson head is effectively few-shot learning (seemingly without sampling random subsets of classes). The evidence head is a standard few shot head. Taking the positive part is a ReLU applied on a feature map. Again, that is renaming common parts to sound uncommon / novel. Masking limitations in novelty in such a way is disliked by the reviewer. This results in a low score for presentation. By that one cannot distinguish whether the contributions are actually mostly from the B-cos network or whether the few-shot head plays any role in (a) predictive performance or (b) attribution map quality. In the worst case the B-cos network alone does all the heavy lifting.",
    "candidate_a_text": "This paper proposes a novel method that combines two existing methods—Nadaraya-Watson head for global explanations and B-cos networks for faithful local explanations—to provide both global and local explanations for neural network-based classification models. However, the novelty of the paper is limited, as the proposed method is simply a combination of two existing methods. The paper does not compare the proposed method with other explanation methods, which makes it difficult to evaluate its effectiveness against prior work.",
    "candidate_b_text": "This paper presents WASUP, an inherently interpretable neural network for image classification that combines a B-cos network with a classification head learning support vectors, classifying images based on similarity in the latent space. The proposed method could be seen as an extension of the B-cos network; however, the paper is not novel. The support vectors are, in essence, prototypes as in a ProtoPNet, and the comparison between an input image's latent representations and support vectors is also similar to the comparison between latent representations and prototypes in a ProtoPNet. The proposed method is simply a combination of a B-cos network and a ProtoPNet. Since the main ideas behind the paper are mostly explored in prior work and there is no novelty, the paper lacks significance.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "Human Review 3",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 3,
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          3,
          4
        ],
        "agreement_matrix": {
          "2": {
            "2": true,
            "3": true,
            "4": true
          },
          "3": {
            "2": true,
            "3": true,
            "4": true
          },
          "4": {
            "2": true,
            "3": true,
            "4": true
          }
        },
        "agreement_scores": {
          "2": 1.0,
          "3": 1.0,
          "4": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 48
  },
  {
    "id": 95,
    "paper_id": "icUCCz8pAu",
    "reference_text": "This paper introduces MultiTrust, a framework designed to enhance the safety and trustworthiness of large language models (LLMs) from multiple perspectives, including robustness, fairness, and truthfulness. MultiTrust integrates adversarial training data generation, safety auxiliary models, and an inference-time, perplexity-based routing mechanism to dynamically align base LLMs with specialized safety models, enabling the system to address several safety concerns in parallel without sacrificing general task performance. While the paper presents a novel multi-perspective approach to LLM safety by moving beyond the conventional focus on isolated safety aspects, the technical novelty of MultiTrust is somewhat limited. Both the use of synthetic data generation and inference-time routing mechanisms have been established in prior work, and the framework largely appears as a combination of these existing approaches rather than introducing substantial innovation in either area. Although the modular integration and dynamic alignment of models represent a flexible and scalable solution, the specific advances over previous methods are not sufficiently clear, and the contribution may be viewed as a relatively straightforward application of known techniques rather than a significant step forward in the field. The paper would benefit from more clearly articulating the unique aspects of its approach and providing a stronger justification for its novelty relative to established methods.",
    "candidate_a_text": "This paper introduces MultiTrust, a framework designed to enhance the safety and trustworthiness of Large Language Models (LLMs) across multiple dimensions, including robustness, fairness, and truthfulness. MultiTrust addresses safety and trustworthiness from multiple angles, providing a holistic approach that enhances the model's reliability in various aspects. The perplexity-based routing mechanism allows for dynamic adaptation to different safety perspectives, enabling efficient and context-specific model alignment without additional training. The framework maintains the base LLM's performance while improving trustworthiness, ensuring that the model's general capabilities are not sacrificed for safety improvements.",
    "candidate_b_text": "**Summary for Reviewer:**\n\nThe submission, *MultiTrust*, adapts established techniques—modular, inference-time model fusion (as in PackLLM), dynamic routing (inspired by Capsule Networks), and preference-based alignment (DPO, as in GRATH)—to the domain of LLM safety and trustworthiness, focusing on robustness, fairness, and truthfulness. Its main technical novelty lies in applying perplexity-based dynamic routing and modular fusion to safety model selection and combination, enabling flexible augmentation with new safety perspectives at inference time. However, the underlying mechanisms are not new, and the submission does not fully acknowledge conceptual overlap with PackLLM (for routing/fusion) or GRATH (for DPO-based alignment), potentially overstating its novelty. Empirical contributions include demonstrating DPO's effectiveness for safety alignment across multiple perspectives and observing cross-perspective data transfer effects, but these are incremental extensions of prior work. Overall, the work represents a reasonable integration of known methods into a new application area, but reviewers should be aware that its primary contribution is in domain adaptation and system integration rather than methodological innovation.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {}
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 49
  }
]