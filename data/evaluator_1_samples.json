[
  {
    "id": 53,
    "paper_id": "Ax3uliEBVR",
    "reference_text": "The authors propose an equivariant Topological Deep Learning framework that deals with geometric node features and can be generalized to many topological domains including simplicial, cell, combinatorial, and path complexes. The authors add an important piece of work for the Topological Deep Learning (TDL) community as there is not much literature on Equivariant TDL, and the novel benchmark based on geospatial information is novel. However, novelty is the key disadvantage of the paper, as it seems that the work just extends prior works on graphs to TDL; even though the theoretical insights are important, they are mostly an extension from graphs. Furthermore, the property of “heterogeneous interactions” was actually mentioned in prior literature, so it is not fair to claim that ETNNs are set up for this characteristic, but more like TDL in general already possesses this property.",
    "candidate_a_text": "This paper introduces an equivariant model within the framework of topological deep learning, generalizing the equivariant graph neural network architecture from Santorras et al. from the setting of graphs to message passing over combinatorial complexes, and allowing for message passing with cells that have heterogeneous node features over differing ranks. I really like that the authors introduced a novel geometric prediction task into the literature, as many of the benchmarks for TDL were somewhat old and outdated, and the new task appropriately features integration of data over different dimensional regions in a way that showcases the central feature of the paper—reconciling data with features on subspaces of differing dimension, i.e., the ‘heterogeneous interactions’ promised in the abstract. For me personally, I find it hard to understand the framing of this as a part of an entirely new conceptual field of topological deep learning beyond GNNs, and question the genuine novelty of papers like this; unless I’m mistaken, the basic content of proposition (1) is that an ETNN can be reformulated as an EGNN, meaning that the main novelty is the clever choice of ‘lifting' the data into a certain graph and some delineation of the learning based on ‘rank’, and even the proof of theorem (1) is basically a straightforward adaption of the corresponding result for EGNNs. I personally get the sense that the ‘topology’ part—and hence the novelty of these kinds of architectures—is overplayed a little, as it seems that the inclusion of domain-specific data along with the design of the graph is doing most of the work, and I don’t personally see a strong connection with the ‘lifted’ combinatorial complexes used in this paper and topology in the classical sense. The claim that this is the first work to explore combinatorial topological modeling of multi-resolution irregular geospatial data seems overstated given existing literature in Topological Data Analysis applied to geospatial data, and I would appreciate clarification on how this approach differs from or advances beyond these existing works.",
    "candidate_b_text": "The submission introduces ETNNs, a general framework for E(n)-equivariant message passing that extends prior models (notably EGNN and EMPSN) from graphs and simplicial complexes to arbitrary combinatorial complexes, thereby unifying and generalizing existing approaches in equivariant topological deep learning. The main technical novelty lies in this abstraction, which enables modeling of more general higher-order structures and allows geometric features to be incorporated at all cell orders, though the practical benefits of these extensions are only modestly demonstrated in standard domains. Claims of increased expressivity and efficiency over prior models are supported by theoretical and empirical analysis, but the improvements are incremental and may partly result from implementation choices rather than fundamental algorithmic advances. The application to a new geospatial dataset is a useful demonstration of generality, but does not constitute a methodological advance. Overall, the work is a technically sound and well-positioned generalization, but reviewers should be aware that its practical impact and novelty may be somewhat overstated unless further empirical evidence is provided for domains where combinatorial complexes offer clear advantages.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          0,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "2": true,
            "3": false
          },
          "2": {
            "0": true,
            "2": true,
            "3": true
          },
          "3": {
            "0": false,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 0.5,
          "2": 1.0,
          "3": 0.5
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 21
  },
  {
    "id": 26,
    "paper_id": "9aIlDR7hjq",
    "reference_text": "The authors propose a frozen alternative to increase the diversity of synthetic training images generated by diffusion models, which conditions the diffusion model not only on few-shot images (done previously) but augmentations (novelty). The method itself is quite simplistic from a novelty perspective (simply adding augmentations to the conditioning). I would consider this a strength if the results were consistent and strong with a clear storyline for effective use-cases; however, I do not see this as being the case. I do not find the CFG scale experiments as adding significant value, as they are consistent with previous work and do not provide more interesting, surprising, or novel results, which in my opinion waters down the impact of the experimental section.",
    "candidate_a_text": "This paper demonstrates that conditioning the generation process on an augmented real image and a text prompt produces effective synthetic datasets, which benefit downstream tasks, particularly for long-tailed (LT) classification and few-shot classification. However, the technical novelty of this paper is unclear. The concept of combining both augmented images and text prompts seems useful for LT and few-shot classification but lacks novelty. If this approach is not technically original, the paper should at least show a broad variety of downstream tasks that benefit from it, which it did not. The contribution is not clearly articulated, as it’s evident that the synthetic dataset is effective, but it’s unclear for which specific tasks it is most useful, with the focus confined to LT and few-shot classification. Expanding the application scenarios would improve the paper’s impact.",
    "candidate_b_text": "This paper proposes a new method of generating synthetic training images by conditioning the generation process on an augmented real image and a text prompt. The idea of using augmented real images as a reference to guide the generation process is interesting. However, the paper lacks a comparison with a strong baseline that fine-tunes the diffusion model on the target dataset using a method like textual inversion, which makes it difficult to assess the true effectiveness of the proposed method.",
    "candidate_a_label": "Human Review 2",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": 2,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          2,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "2": true,
            "3": true
          },
          "2": {
            "0": true,
            "2": true,
            "3": true
          },
          "3": {
            "0": true,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 20
  },
  {
    "id": 38,
    "paper_id": "bgcdO9lmug",
    "reference_text": "This paper proposes an approach for automatic prompt engineering in scenarios where end evaluation is expensive or infeasible, leveraging intermediate feedback and a step-by-step process that incorporates chat history, reflections, and internal LLM evaluation to iteratively improve prompts. While the authors claim that their method introduces a “summarization-based” mechanism for prompt improvement at each step, this core idea closely mirrors the Reflexion framework introduced by Shinn et al. (2024) [1], which also utilizes summarization to guide prompt refinement. The novelty of the proposed approach is therefore unclear, as the main distinction appears to be its application of Reflexion to the automatic prompt engineering (APE) setting, rather than a fundamentally new technique. Furthermore, Reflexion itself has known limitations, particularly its reliance on ReAct for exemplar-query similarity, as discussed by Verma et al. (2024) [2]. The current manuscript does not sufficiently clarify how its approach differs from or extends Reflexion beyond this application context. As such, the contribution seems to be primarily an adaptation of existing methods rather than a novel algorithmic advance. The paper would benefit from explicitly articulating the differences from prior work and clearly stating whether its main contribution is the application of Reflexion to APE, which may still be a novel use case but should be presented transparently.",
    "candidate_a_text": "This paper introduces REPROMPT, an innovative method for automatic prompt optimization in Large Language Model (LLM) agents, particularly in complex reasoning tasks where a final solution checker is not readily available. I find several aspects of this paper to be particularly strong. First, the core idea of REPROMPT, which is to optimize prompts through an iterative process based on interaction history, is both novel and well-motivated. This approach addresses a critical limitation of existing automatic prompt engineering (APE) methods that rely on a final solution checker, which is often unavailable in complex reasoning tasks. The analogy to 'gradient descent' is intuitive and helps to understand the iterative refinement process. The use of an LLM as a summarizer to extract key insights from interaction histories is a clever way to guide prompt optimization. Overall, the paper presents a novel approach to automatic prompt engineering that addresses a significant challenge in the field of LLM agents, offering a method that can iteratively improve prompts without relying on a final solution checker. The ability to automatically optimize prompts for these tasks has the potential to significantly enhance the capabilities of LLM agents in real-world applications.",
    "candidate_b_text": "This paper proposes a new method for automatic prompt engineering (APE) called RePrompt, which optimizes step-by-step instructions in prompts for LLM agents using intermediate feedback from interactions and reflections. The paper does not compare RePrompt with other APE methods, making it hard to assess its relative performance, and the contributions are not well-organized.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {}
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 2
  },
  {
    "id": 13,
    "paper_id": "rWIrdAo2xC",
    "reference_text": "This paper proposes to train a conditional diffusion model for generating novel views of humans from given single-view images, directly supervised by proxy ground truth 3D Gaussian attributes. The method outperforms other generalizable novel view synthesis techniques like LGM, and the idea of using a neural network to constrain the distribution of target 3D Gaussian attributes makes sense and is effective. However, although the method demonstrates improvement compared to previous methods like LGM, the image quality remains limited since only one single input image is used, and another potential direction in this field is incorporating a 2D diffusion prior to enhance information, as demonstrated in Human 3Diffusion. A comparison to these baselines is needed.",
    "candidate_a_text": "This paper introduces a method that renders novel views of human avatars, given only a single image, by proposing a pipeline of multiple stages that first learns a prior and then renders a human under novel views, highlighting their paradigm as \"direct\" and using extracted attributes for supervision in contrast with other \"indirect\" approaches. Even though the authors show good results, the overall contribution and novelty of the paper is not well supported. Components like transformers, stable diffusion, etc. have been used by other papers as well. Learning pixel-aligned features from different views has been proposed by previous works, like the generalizable NeRF ActorsNeRF (ICCV 2023) and MonoHuman (CVPR 2023). Stable diffusion has been used by works for humans, like DreamHuman (NeurIPS 2024), DreamAvatar (CVPR 2024), AvatarPopUp (ECCV 2024), and other works for 3D objects (e.g. DreamFusion). It is not clear to this reviewer what the additional challenges of monocular reconstruction are in the particular set up of this paper and how the particular method addresses them.",
    "candidate_b_text": "The paper presents a method for novel view synthesis of humans using a proxy-ground-truth creation process and a diffusion model. I provide a quantitative comparison of the proposed method with several existing methods for novel view synthesis, including those that utilize mesh-based models and other volumetric approaches. I highlight the strengths and weaknesses of our method relative to existing approaches and provide a clearer understanding of its advantages and limitations in the context of the current state-of-the-art.",
    "candidate_a_label": "Human Review 4",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": 4,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 0.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          4
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "4": false
          },
          "4": {
            "1": false,
            "4": true
          }
        },
        "agreement_scores": {
          "1": 0.0,
          "4": 0.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 3
  },
  {
    "id": 58,
    "paper_id": "uMEsKEiB7J",
    "reference_text": "This paper introduces NovelQA, a benchmark specifically designed to evaluate large language models (LLMs) on long-context question-answering tasks using texts that exceed 200,000 tokens and are drawn from a diverse selection of English novels. The benchmark is constructed with expert-annotated questions that require detailed comprehension of lengthy narratives, and it assesses multiple facets of understanding, including narrative comprehension, multi-hop reasoning, and detail retrieval. In terms of novelty, NovelQA represents a significant advancement over standard benchmarks by emphasizing extended context comprehension within complex narratives—a challenge that existing datasets have not adequately addressed. The inclusion of tasks that evaluate narrative coherence and multi-step reasoning further distinguishes NovelQA as a substantial contribution to long-context evaluation. Overall, the benchmark fills an important gap in the evaluation of LLMs’ abilities to manage large, complex contexts, marking a notable step forward in the field.",
    "candidate_a_text": "This paper introduces NovelQA, a benchmark designed to evaluate the performance of LLMs on extremely long and complex texts. NovelQA is the first long-context QA benchmark featuring manually crafted questions, golden answers, and evidence, with contexts exceeding 200,000 tokens. The questions are manually annotated by literature experts, ensuring high-quality, nuanced questions and answers that reflect complex human thought processes. The dataset includes a diverse range of question types, testing different comprehension abilities of LLMs, and the evaluation of current LLMs on NovelQA reveals their strengths and weaknesses, providing valuable insights for further research.",
    "candidate_b_text": "This paper introduces a novel benchmark, NovelQA, which addresses a critical gap in evaluating LLMs' long-context capabilities, particularly for texts exceeding 200,000 tokens. This is a significant contribution as existing benchmarks often fall short in assessing models' abilities to handle extremely long and complex narratives. NovelQA represents the first benchmark specifically designed to assess LLMs' abilities in the literary domain, offering a rigorous tool for advancing natural language processing and computational literary studies.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {}
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 4
  },
  {
    "id": 93,
    "paper_id": "CKYsXi0dOV",
    "reference_text": "This paper presents BLIP-3-Video, a novel multimodal language model for video understanding that introduces an innovative temporal encoder to significantly reduce the number of visual tokens needed to represent videos, offering a new approach to efficiency in video understanding models. While the integration of the temporal encoder is highlighted as a key innovation, I question the core novelty of this paper compared to prior works such as LLaMA-VID, Video-LLaVA, LLaVA-VID, and LLaVA-NEXT, as similar architectures have already been explored in these works. Although the experimental results show that 32 tokens achieved better performance on four short video benchmarks, this standard may change with different video lengths, scenarios, and question complexity, raising concerns about the scalability and generalizability of the method. Perhaps a more effective mechanism for accommodating more frames and selecting key information for video question answering from a large number of visual tokens is worth exploring, rather than focusing on a specific numerical value of visually overfitted tokens on a few benchmarks.",
    "candidate_a_text": "BLIP-3-Video presents an incremental advance in token-efficient video representation for vision-language models, primarily through the architectural design and explicit integration of a learnable spatio-temporal pooling module. While the submission claims fundamental novelty and to be the first to demonstrate high-quality video QA with only 16–32 tokens, similar results and approaches have already been reported in recent works such as TESTA, SMAUG, and LLaVA-PruMerge. The use of sequential models like Token Turing Machines as temporal encoders is a direct application of existing methods rather than a conceptual innovation. The main technical contribution is the specific pooling architecture, but the overall approach aligns closely with the field’s ongoing trend toward aggressive token reduction and learnable aggregation. Reviewers should recognize BLIP-3-Video as a well-executed, incremental improvement within an established research paradigm, rather than a fundamentally novel or paradigm-shifting contribution.",
    "candidate_b_text": "This paper introduces BLIP-3-Video, a novel multimodal language model designed for efficient video understanding, with the core innovation lying in its temporal encoder, which significantly reduces the number of visual tokens required to represent a video and directly addresses the computational challenges of processing long videos in large vision-language models. I find that the core idea of using a temporal encoder to reduce the number of visual tokens is a significant contribution, and the exploration of two distinct temporal encoders—spatio-temporal attentional pooling and Token Turing Machines (TTM)—is another strength that allows for a comparison of different approaches to temporal modeling within the same framework. However, the paper's novelty is somewhat limited by its reliance on existing techniques, as the core idea of using a temporal encoder to reduce the number of visual tokens is not entirely new and similar approaches have been explored in prior works. The authors acknowledge this to some extent, but a more detailed comparison with existing methods, such as token pruning or token merging techniques, would help to better contextualize the contributions of the proposed model and highlight its unique aspects. Despite this, the fact that BLIP-3-Video can abstract a video into as few as 32 tokens while maintaining high accuracy, and achieves competitive performance compared to much larger state-of-the-art models, is a remarkable achievement and constitutes a significant step towards more efficient video understanding. Overall, while the approach offers practical and efficiency improvements, the lack of a thorough analysis of how the proposed temporal encoder differs from existing token reduction methods makes it difficult to fully assess the model's unique contributions.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "3": true
          },
          "3": {
            "0": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 14
  },
  {
    "id": 43,
    "paper_id": "e8c7XDRJcg",
    "reference_text": "The paper presents three methods to perform Unsupervised Modality Adaptation (UMA): Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer Through Time (C3T). Unsupervised Modality Adaptation is frequently used for cross-modal learning in many applications and cannot be regarded as a new contribution of this paper. Compared to ST and CA, the Cross-modal Transfer Through Time (C3T) performs pseudo label generation and alignment in temporal sequences, while other operations, including feature learning, RGB-IMU alignment, and classification, follow the same procedures, so the novelty of the proposed method needs to be reconsidered. The experiment comparison is not reliable since the proposed approach is not compared with related frameworks, and there are many existing works as listed in Section 2. I suggest considering refining the key innovations and redefining the key representations for innovation.",
    "candidate_a_text": "This paper proposes a cross-modal transfer method for human action recognition, based on the assumption that there is a shared latent representation space for different modalities, and includes three variants: student-teacher, contrastive alignment, and cross-modal transfer through time. The novelty of the paper is limited, as the assumption of a shared latent representation space for different modalities has been proposed by many previous works. While the idea of aligning modalities in a common latent space is well-established, the paper needs to more clearly articulate the specific novel contributions beyond the existing literature on shared latent spaces. For example, the 'cross-modal transfer through time' (C3T) method should be explained in more detail, highlighting how it differs from existing temporal alignment techniques, and the paper should emphasize the unique aspects of its approach.",
    "candidate_b_text": "The paper investigates an unsupervised modality adaptation method for the action recognition model, proposing Cross-modal Transfer Through Time (C3T), which performs alignment on each temporal step rather than on video-level feature representation as in Contrastive Alignment (CA). The novelty is minor, which is only on doing the alignment per temporal step, without proposing a new alignment method or an architectural change to current state-of-the-arts. The lack of state-of-the-art comparison also makes it hard to justify the significance of the novelty besides its improvement upon Student-Teacher (ST) and CA methods.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "Human Review 3",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 3,
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "3": true
          },
          "3": {
            "1": true,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 12
  },
  {
    "id": 32,
    "paper_id": "DSsSPr0RZJ",
    "reference_text": "This paper introduces DSBench, a comprehensive benchmark designed to evaluate data science agents on realistic tasks derived from ModelOff and Kaggle competitions. By integrating a wide range of data analysis and modeling challenges, DSBench aims to provide a rigorous and practical environment for assessing agent performance under real-world conditions. The benchmark is further distinguished by the introduction of the Relative Performance Gap (RPG) metric, which seeks to normalize evaluation across diverse modeling tasks. The contribution is notable in that it pushes the boundaries of traditional benchmarking by moving beyond synthetic or narrowly scoped tasks, instead offering a fresh and more holistic approach to evaluating data science agents. DSBench sets a new standard in the field, driving advancements in realistic, end-to-end task performance and supporting the development of intelligent, autonomous data science agents. Overall, the paper’s novelty lies in both the scale and realism of the benchmark, as well as the effort to address evaluation inconsistencies through the RPG metric, marking a significant and original step forward in data science benchmarking.",
    "candidate_a_text": "This paper introduces DSBench, a comprehensive benchmark designed to evaluate the performance of data science agents on realistic tasks sourced from ModelOff and Kaggle competitions. The benchmark consists of 466 data analysis tasks and 74 data modeling tasks, and provides a thorough evaluation of multiple state-of-the-art LLMs, LVLMs, and agents, offering a comprehensive comparison of their performance on these tasks. The benchmark tasks are sourced from real-world competitions, making the evaluation more realistic and relevant to practical applications in the field of data science.",
    "candidate_b_text": "DSBench is a new benchmark for data science agents that distinguishes itself primarily by sourcing tasks from real-world competitions (ModelOff and Kaggle), offering a large and diverse set of data analysis and modeling challenges. While the authors claim comprehensive realism and multimodality, these aspects are already addressed by recent benchmarks such as Spider2-V, OSWorld, and InfiAgent-DABench, making DSBench’s main novelty its specific task sourcing and explicit coverage of both analysis and modeling. The introduction of the Relative Performance Gap (RPG) metric for normalized evaluation across heterogeneous tasks is a useful, though incremental, methodological contribution. Some claims of uniqueness—particularly regarding multimodality, end-to-end workflows, and revealing agent-expert performance gaps—are overstated, as similar features and findings exist in prior work. Overall, DSBench is a timely and relevant addition to the field, but reviewers should recognize that its advances are incremental rather than fundamentally transformative within the current landscape of data science agent benchmarks.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {}
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 49
  },
  {
    "id": 31,
    "paper_id": "qrTOtUdz4Z",
    "reference_text": "This paper proposes a novel framework, ConceptGCD, that uses a covariance loss to learn distinctive concepts to facilitate category discovery in the Generalized Category Discovery (GCD) task. The novelty of the proposed framework is limited: the major contribution is the application of covariance loss in GCD. While the framework is well-designed to allow the model to use both \"derivable concepts\" to aid novel class discovery and \"underivable concepts\" customized to novel classes, the main originality lies in leveraging covariance loss to promote diversity in concept learning. The method achieves strong performance across six benchmarks, outperforming previous approaches, but the contribution is primarily incremental in applying covariance loss within the GCD context.",
    "candidate_a_text": "This paper proposes a novel framework for Generalized Category Discovery (GCD), named ConceptGCD, which introduces a new way of learning with known classes by derivable and underivable concepts. The proposed method is novel. It is interesting and effective to introduce the concept of concepts into the GCD field and learn the different concepts separately. The experimental results show that the proposed method outperforms the previous state-of-the-art methods on several GCD datasets.",
    "candidate_b_text": "This paper introduces ConceptGCD, a novel framework for Generalized Category Discovery (GCD) that leverages a concept-based approach to improve the discovery of novel classes. The introduction of the ConceptGCD framework, which classifies concepts into derivable and underivable classes and addresses them in a staged manner, is innovative and well-justified.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.5,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          2
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "2": false
          },
          "1": {
            "0": true,
            "1": true,
            "2": false
          },
          "2": {
            "0": false,
            "1": false,
            "2": true
          }
        },
        "agreement_scores": {
          "0": 0.5,
          "1": 0.5,
          "2": 0.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 32
  },
  {
    "id": 88,
    "paper_id": "aueXfY0Clv",
    "reference_text": "The proposed Depth Pro model employs a ViT architecture for zero-shot metric monocular depth estimation, targeting applications such as novel view synthesis. While Depth Pro benefits from pretrained ViT backbones, its architecture primarily builds on existing elements rather than introducing fundamentally new mechanisms for depth estimation, which limits its architectural novelty. The paper introduces a two-stage training approach that integrates synthetic and real-world datasets, enhancing depth boundary accuracy, and includes new metrics for evaluating depth boundaries that address a gap in existing benchmarks by focusing on boundary precision, which is critical for applications like view synthesis that demand fine details.",
    "candidate_a_text": "This paper proposes a new method for zero-shot monocular depth estimation that is efficient, fast and accurate, and is also capable of predicting metric-scale depth, which is a challenging task. The experiments demonstrate the superiority of the proposed method over the existing methods.",
    "candidate_b_text": "This paper presents a foundation model for zero-shot metric monocular depth estimation that is capable of producing metric depth maps with absolute scale on arbitrary images without requiring metadata such as camera intrinsics. The model's ability to produce metric depth maps with absolute scale on arbitrary images without requiring metadata such as camera intrinsics is a significant advancement in the field of monocular depth estimation. The high-resolution output and fast processing time make it suitable for real-time applications. The model's superior performance in sharp delineation of object boundaries and focal length estimation across multiple datasets demonstrates its robustness and accuracy.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "2": {
            "2": true,
            "3": true
          },
          "3": {
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 9
  },
  {
    "id": 78,
    "paper_id": "uuvujfQXZy",
    "reference_text": "This paper proposed a novel type of CBM called UCBM, which utilizes concept discovery methods to discover concepts learnt by pretrained, black-box models and convert them into CBM. The paper proposed an unsupervised concept discovery mechanism for CBMs, which is novel in the field of CBM, as the method does not need pre-defined concept sets. Additionally, a novel input-dependent concept selection mechanism is proposed to dynamically retain a sparse set for prediction; however, the idea of input-dependent concept selection is not novel, as, for example, Panousis et al. (2023) introduced a binary latent indicator to dynamically select activating concepts according to input embedding and mask out other concepts. I agree that the introduction of unsupervised concept discovery into CBM is interesting, but the missing of text representation limits the novelty.",
    "candidate_a_text": "This paper proposes learning post-hoc Concept Bottleneck Models (CBMs) in an unsupervised manner, using an unsupervised non-negative matrix factorization to discover concept vectors, an input-dependent alignment score selection, and a sparse linear layer for class prediction. Unlike previous methods, UCBMs do not assume a set of concepts learned by black-box models, but instead discover the concept vectors in an unsupervised manner. The idea of input-dependent sparsity constraint is interesting and non-trivial. However, the proposed method has limited novelty, as it uses an existing matrix decomposition method called CRAFT for feature generation, and the novelty is limited to proposing input-dependent selection applied before learning the sparse linear layer. The paper lacks comparisons with newer methods, particularly with [2], which trains a learnable dictionary to approximate the embedding space of VLMs in a supervised manner without using a pre-defined concept set.",
    "candidate_b_text": "This paper presents a novel approach to converting a black-box model into a concept bottleneck model (CBM) using unsupervised concept discovery, and proposes an input-dependent concept selection mechanism where only a sparse set of concepts relevant to the input are dynamically retained. The idea of using concept discovery for CBM is interesting and novel. However, the paper does not provide comparisons with other concept discovery methods such as [1], other methods that convert a black-box model into a CBM such as [2,3], or with CBMs trained from scratch, which limits the ability to fully assess the originality and significance of the contribution.",
    "candidate_a_label": "Human Review 1",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": 1,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          2
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "2": true
          },
          "1": {
            "0": true,
            "1": true,
            "2": true
          },
          "2": {
            "0": true,
            "1": true,
            "2": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "1": 1.0,
          "2": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 6
  },
  {
    "id": 14,
    "paper_id": "RtFWWAXIyH",
    "reference_text": "This paper proposes a method for both image-based and text-based 3D generation, introduces new metrics for evaluating 3D consistency, and claims to achieve multi-view image generation with fast convergence speed using pre-trained video generation models. However, fine-tuning a video generation model for 3D generation is not novel, as leveraging the implicit knowledge of video generation models to help 3D generation has been investigated in recent works (e.g., SV3D and IM-3D) and achieves significant performance. Although the paper introduces IM-3D and SVD (likely SV3D), it does not provide a clear comparison with them, and the differences claimed—such as optimization requirements and view conditioning—are not convincingly established. Therefore, I feel the novelty of this paper is limited and it also does not comprehensively discuss related works. The reconstruction network is presented as another main contribution, but its role as an intermediate feature rather than the final 3D representation raises questions about its significance.",
    "candidate_a_text": "This paper proposes a novel framework for consistent multi-view generation, built upon pretrained video diffusion models and fine-tuned with object-centric videos. The authors introduce a 3D-aware denoising sampling strategy involving a feed-forward reconstruction module that obtains an explicit global 3D model, which replaces the original multi-view images in the denoising sampling loop. The proposed method is novel and achieves state-of-the-art performance in both text-based and image-based multi-view generation tasks, and the 3D-aware denoising sampling strategy is interesting.",
    "candidate_b_text": "The submission, VideoMV, presents a technically solid approach for multi-view image synthesis by fine-tuning video diffusion models and integrating a feed-forward 3D reconstruction module into the denoising loop. While the method demonstrates impressive training efficiency and competitive multi-view consistency, its core ideas—leveraging video diffusion models and explicit 3D feedback—are shared with several recent works (e.g., IM-3D, SV3D, VFusion3D), making the claimed novelty incremental rather than fundamental. The main technical differentiation lies in the specific procedural integration of 3D reconstruction within the denoising process, but similar concepts have been explored in concurrent literature. The authors’ characterizations of related work tend to group prior methods by single features and sometimes overstate the uniqueness of their own contributions, omitting detailed, head-to-head comparisons with the most similar approaches. Overall, VideoMV is a strong, efficient variant in a rapidly evolving field, but its novelty claims should be calibrated against the backdrop of parallel, incremental advances and substantial methodological overlap with recent work.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "3": true
          },
          "1": {
            "0": true,
            "1": true,
            "3": true
          },
          "3": {
            "0": true,
            "1": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "1": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 11
  },
  {
    "id": 39,
    "paper_id": "skJLOae8ew",
    "reference_text": "This paper proposes a new pipeline for generating architectural floor plans by applying diffusion models, fine-tuning them to learn implicit design concepts in architectural design, and generating detailed and functional floor plans. However, the technical innovation in this paper is quite insufficient, and the introduction to the U-Net architecture is entirely superfluous. For ICLR, this paper clearly lacks innovation and systematic methodology, and it does not quite meet the threshold for current AI conferences.",
    "candidate_a_text": "This assessment finds that the submission adapts diffusion models—specifically a customized U-Net architecture with integrated upscaling—for efficient, high-resolution architectural floor plan generation, situating it within a well-established trend of domain adaptation and engineering optimization in generative modeling. While the authors claim to be the first to apply diffusion models to this domain and to uniquely capture architectural semantics, these claims are not fully substantiated, as prior works such as HouseDiffusion and Tell2Design have already explored similar territory. The main technical contributions appear to be in the specific engineering choices and integration of upscaling for floor plans, representing an incremental rather than a fundamental advance. The submission omits direct comparison with the most relevant prior work and tends to overstate its novelty, particularly regarding \"firstness\" and semantic capability. Reviewers are advised to focus on the empirical results and robustness of the engineering, but to calibrate expectations regarding conceptual innovation and to request more thorough citation and comparison with existing literature.",
    "candidate_b_text": "This paper introduces a new diffusion model designed for generating architectural floor plans, adapting the U-Net architecture and employing upscaling techniques to enhance efficiency and detail. The application of diffusion models to architectural design is a relatively new and promising field. However, the proposed method lacks novelty, as it primarily adapts an existing diffusion model (U-Net) for floor plan generation without introducing significant innovations. The paper does not cite several important works in the field, such as DreamPlan, HouseDiffuser, and HouseGan++, which further highlights the absence of originality and differentiation from prior work.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "3": true
          },
          "3": {
            "0": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 41
  },
  {
    "id": 7,
    "paper_id": "WlKGZuolEk",
    "reference_text": "This paper proposes a method called HMKM to match image region features with categories, representing categories at both the object level and attribute level, and serving as a plug-and-play module to improve detection performance of novel categories in Open-Vocabulary Object Detection models. However, the innovative contribution relative to existing studies appears limited. While this work has improved the performance of the existing OVOD architecture, the novelty of this study is somewhat difficult to identify. Numerous related works already utilize visual prototype knowledge to enhance model classification ability, and a more explicit discussion on how it differs significantly from related methods should be conducted.",
    "candidate_a_text": "This paper introduces a new approach called Hierarchical Multimodal Knowledge Matching (HMKM) for training-free open-vocabulary object detection. The idea of combining object-level and attribute-level knowledge for open-vocabulary object detection is interesting, and the proposed method is training-free and can be easily integrated into existing object detection models. However, the paper only compares HMKM with a few baseline methods, and it would be better if more recent and relevant methods, particularly those that also leverage multimodal knowledge for open-vocabulary object detection, could be included for comparison to more accurately assess the proposed method’s contributions and relative performance. A detailed comparison of the proposed method with these approaches would help to highlight its unique contributions and limitations.",
    "candidate_b_text": "This assessment reviews a submission proposing a Hierarchical Multimodal Knowledge Matching (HMKM) method for open-vocabulary object detection (OVOD), emphasizing a training-free, plug-and-play approach that combines object and attribute prototype knowledge for improved detection of novel categories. The submission is well-positioned within the multimodal fusion and prototype-based generalization literature, with its main novelty being the explicit hierarchical (object + attribute) matching, which is less common in OVOD but conceptually present in related few-shot/zero-shot learning work. While the authors accurately characterize most prior methods as training-based, they overstate the uniqueness of their training-free approach and do not cite several relevant recent works (e.g., OVMR, CPN, Prototype Completion) that share similar goals or techniques. Empirical improvements are reported, but the assessment notes that these may stem from better engineering or prototype selection rather than fundamentally new concepts. Overall, the submission offers an incremental advance—primarily through hierarchical prototype integration—within a rapidly evolving field, but would benefit from a more comprehensive engagement with related prototype/attribute-based literature.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 0.5,
        "num_reviews": 5,
        "consensus_type": "majority",
        "other_reviews": [
          0,
          1,
          3,
          4
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": false,
            "2": true,
            "3": false,
            "4": false
          },
          "1": {
            "0": false,
            "1": true,
            "2": true,
            "3": false,
            "4": false
          },
          "2": {
            "0": true,
            "1": true,
            "2": true,
            "3": false,
            "4": false
          },
          "3": {
            "0": false,
            "1": false,
            "2": false,
            "3": true,
            "4": false
          },
          "4": {
            "0": false,
            "1": false,
            "2": false,
            "3": false,
            "4": true
          }
        },
        "agreement_scores": {
          "0": 0.25,
          "1": 0.25,
          "2": 0.5,
          "3": 0.0,
          "4": 0.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 13
  },
  {
    "id": 44,
    "paper_id": "JytL2MrlLT",
    "reference_text": "This paper explores the potential of modernizing the classical Nearest Neighbor approach for tabular data by leveraging a differentiable K-nearest neighbors variant, Neighborhood Components Analysis (NCA), and introduces MODERNNCA, an improved version of NCA that integrates deep learning techniques such as stochastic gradient descent (SGD), nonlinear embeddings, and a Stochastic Neighborhood Sampling (SNS) strategy to boost computational efficiency and model performance. Revisiting a classic nearest-neighbor approach with contemporary deep learning techniques is a novel approach, particularly since NCA had been previously limited by computational efficiency and scalability, and this approach aims to unify insights from both traditional and modern tabular prediction methods. However, the paper’s contribution feels incremental rather than pioneering, as the improvements in MODERNNCA rely on established techniques (SGD, SNS, and nonlinear embeddings) without introducing a fundamentally new concept or method, making the novelty limited and essentially optimizing an existing algorithm rather than providing a unique advancement. The paper also lacks direct comparisons with other KNN-inspired deep learning methods that have similarly benefited from modern optimization strategies, which weakens the argument for MODERNNCA’s distinctiveness and impact.",
    "candidate_a_text": "This paper revisits the Neighborhood Components Analysis (NCA) and adapts it for tabular data learning, proposing ModernNCA as an enhanced approach. While the approach effectively leverages modern deep learning techniques to enhance classical NCA and demonstrates strong empirical results, the core modifications—using a representation space for distance calculations, employing SGD, and mini-batch training—have already been explored in prior research. This raises concerns regarding the originality of the contribution, as the changes appear more like tunings of established techniques rather than introducing a fundamentally new method.",
    "candidate_b_text": "This paper revisits the classic KNN method and explores ways to improve it using modern deep learning techniques. The proposed MODERNNCA is a deep learning version of KNN, which achieves good performance in both classification and regression tasks, essentially serving as a strong deep baseline for tabular tasks. However, the idea of using a deep learning version of KNN for tabular data is not novel, as it has been explored in previous works like TabR.",
    "candidate_a_label": "Human Review 2",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": 2,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          2,
          4
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "2": true,
            "4": true
          },
          "2": {
            "1": true,
            "2": true,
            "4": true
          },
          "4": {
            "1": true,
            "2": true,
            "4": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "2": 1.0,
          "4": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 39
  },
  {
    "id": 24,
    "paper_id": "yYQLvofQ1k",
    "reference_text": "The paper introduces VIRSCI, a multi-agent, LLM-based system designed to simulate teamwork-driven scientific research, organizing agents to mimic collaborative processes such as selecting collaborators, generating research ideas, assessing novelty, and drafting abstracts. The multi-agent approach proposed in this paper has the potential to greatly enhance the quality and breadth of scientific research, with discussion-oriented idea generation that closely mirrors real scientific processes. The 5-step approach—comprising Collaborator Selection, Topic Selection, Idea Generation, Idea Novelty Assessment, and Abstract Generation—presents a promising and robust framework for idea generation. VIRSCI, as a multi-agent system for scientific collaboration, shows clear advantages over single-agent methods. While VIRSCI may generate highly unique or novel ideas, these are less valuable if experimental designs cannot support them within practical constraints.",
    "candidate_a_text": "This assessment reviews a submission introducing VIRSCI, a multi-agent LLM system for scientific idea generation that uniquely integrates real scientist role-play and collaboration network data for team formation, aiming for objective, data-driven evaluation of idea novelty and impact. The work is methodologically closest to recent multi-agent LLM frameworks but distinguishes itself by grounding agent behavior in real-world data and employing rigorous, scalable evaluation metrics, rather than relying solely on subjective human review. While the authors claim to be the first to deliver an end-to-end, multi-agent scientific ideation pipeline, this novelty is somewhat overstated, as similar frameworks exist, though not with this exact integration of social network data and agent role-play. The most substantive contribution is the use of real scientist data for agent instantiation and team selection, which is a genuine methodological advance, whereas the multi-agent and end-to-end aspects are more incremental given current field trends. Overall, the submission advances the field by combining established multi-agent LLM techniques with social science-informed team modeling and objective evaluation, but reviewers should note that some claims of novelty are exaggerated and the precise source of empirical improvements is not fully disentangled from other system components.",
    "candidate_b_text": "The paper presents a novel approach to scientific idea generation using a multi-agent system, addressing the collaborative nature of scientific research. VIRSCI consists of a team of agents that collaborate to generate, evaluate, and refine research ideas, and is evaluated using real-world data with improved performance over single-agent systems. The findings suggest that multi-agent collaboration can enhance the novelty and impact of generated scientific ideas. The use of real-world data and the comparison with single-agent systems demonstrate the practical relevance and effectiveness of the proposed approach.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 0.6666666666666666,
        "num_reviews": 4,
        "consensus_type": "majority",
        "other_reviews": [
          0,
          1,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": false,
            "2": false,
            "3": false
          },
          "1": {
            "0": false,
            "1": true,
            "2": true,
            "3": false
          },
          "2": {
            "0": false,
            "1": true,
            "2": true,
            "3": true
          },
          "3": {
            "0": false,
            "1": false,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 0.0,
          "1": 0.3333333333333333,
          "2": 0.6666666666666666,
          "3": 0.3333333333333333
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 15
  },
  {
    "id": 62,
    "paper_id": "ueeqGvQozB",
    "reference_text": "This paper introduces ML4MILP, an original benchmark collection targeted to the MILP domain, aiming to fill the gap of having a large scale, standardized benchmark for evaluating mixed integer linear program solvers. The benchmark is relevant for the ML and DL community, especially as ML-based approaches are becoming popular in this field. The introduction and related work provide a well-written, self-contained overview of existing solvers, techniques, and related benchmarks, highlighting the general lack of a larger standardized benchmark for MILPs. Based on structural and embedding distance between instances, the authors show that existing benchmarks are often more heterogeneous than the MILP benchmark they propose, which allows for some insight into the structure of the proposed and existing benchmarks. However, the overall contribution—introduction of the benchmark and showing the lack of homogeneity of existing classes of instances based on similarity evaluation metrics with respect to structure and neural embedding distances—is somewhat limited in scientific insight and currently results in me having a hard time voting for acceptance at a main ML and DL conference such as ICLR; maybe submitting the paper to a Benchmark and Dataset track of a suitable conference might be more targeted.",
    "candidate_a_text": "This paper focuses on developing a benchmark ML4MILP to evaluate machine learning (ML) based methods for solving mixed-integer linear programs of MILPs. I have questions about how this benchmark compares and contrasts against the benchmark created for the ML4CO Neurips competition (Gasse et al., 2022), and how this competition benchmark (and the existence of MIPlib) affect the \"first open-source benchmark dataset\" claim in line 74.",
    "candidate_b_text": "This paper proposes a new benchmarking framework for solving mixed integer linear programs (MILPs) using machine learning (ML). The proposed ML4MILP framework is a significant contribution that can advance the development of ML-based methods for solving MILPs. The proposed framework is very comprehensive, including a large-scale dataset, appropriate similarity evaluation metrics, a reclassification algorithm, and a baseline library, ensuring that the framework can be used to evaluate a wide range of methods, from traditional optimization solvers to the latest ML-based approaches. The dataset contains instances from both real-world problems and synthetic instances, ensuring that the methods evaluated on it are robust and can be applied to a variety of practical scenarios.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "2": {
            "2": true,
            "3": true
          },
          "3": {
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 18
  },
  {
    "id": 66,
    "paper_id": "6akuzEqP38",
    "reference_text": "This paper proposes a novel pipeline that enables the creation of articulated objects from arbitrary input mesh, addressing a critical research gap in 3D generation for articulated objects and contributing to an increasingly important area. The main contribution is this pipeline that enables the generation of diverse articulated objects by taking arbitrary 3D mesh as input.",
    "candidate_a_text": "The submission \"ARTICULATE ANYTHING\" presents a novel pipeline that integrates open-vocabulary segmentation, LLM-based articulation estimation, and diffusion-based generative refinement to convert any rigid 3D mesh into an articulated object, distinguishing itself from prior works that address only subsets of this problem or are limited to closed-set categories. The main technical advances include the use of GPT-4o for direct joint parameter estimation from geometry and language, and a diffusion-based optimization strategy with random part transformations to preserve part semantics. While the integration of these components into a single, open-vocabulary, category-agnostic pipeline is a substantive contribution, the individual elements (segmentation, LLM reasoning, diffusion generation) are incremental extensions of existing methods, and some claims regarding the limitations of prior work are somewhat overstated. The authors' characterization of their novelty is generally accurate for the full pipeline, but less so for individual components, and the omission of some recent related works (e.g., OpenObj, Kinematic-aware Prompting) leaves the comparison incomplete. Overall, the work represents a significant step forward in open-vocabulary articulated object modeling, though its impact is primarily in the integration and scaling of recent advances rather than in fundamental algorithmic breakthroughs.",
    "candidate_b_text": "This paper introduces an interesting method called \"Articulated Anything\" to address the problem of articulated object generation. While the method is reasonable, it essentially relies on the power of various large models and diffusion models, which may limit the novelty of the proposed framework. The performance of the proposed method is not particularly impressive, and it is difficult to observe a significant improvement compared to existing methods, such as CAGE.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "Human Review 2",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 2,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          2
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "2": false
          },
          "2": {
            "0": false,
            "2": true
          }
        },
        "agreement_scores": {
          "0": 0.0,
          "2": 0.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 17
  },
  {
    "id": 75,
    "paper_id": "miIE56qM10",
    "reference_text": "This paper introduces an approach that trains an external lightweight model to mitigate inherent biases in LLMs, such as overconfidence and underconfidence. While the proposed model demonstrates enhanced performance and generalization capabilities across multiple models and datasets, the novelty is limited by the fact that existing work has explored training an external calibrator specifically for LLMs. The paper proposes a post-processing calibration method that requires task-specific training data, but the baselines used for comparison are confidence elicitation techniques (pre-processing methods) that do not require training data, making the comparison potentially unfair. Established post-processing techniques like isotonic regression, Platt scaling, and recalibration with Bayesian neural networks should be considered, and the authors should cite relevant work, include applicable baselines, and justify any inapplicable baselines.",
    "candidate_a_text": "This paper proposes a novel approach to uncertainty estimation in LLMs by training a corrector model to fix the miscalibration of logit-based uncertainty scores. The method is novel and addresses a significant challenge in uncertainty estimation for LLMs. The method can be integrated with various logit-based uncertainty methods, and the empirical results show consistent improvements on TriviaQA and SciQA.",
    "candidate_b_text": "This paper focuses on uncertainty estimation of LLMs and proposes to introduce an external corrector for refining uncertainty estimation, which is a Roberta-based regression model predicting a value added to the uncertainty score estimated by baselines. However, Liu et al. [3] generated their training set in a manner similar to this work and introduced a calibration mechanism to adjust logits, reducing the novelty of the proposed data construction process. Additionally, Ulmer et al. [4] propose an external model to estimate the confidence of LLM outputs, closely resembling the proposed method, which also trains an external model for confidence prediction. This potentially diminishes the novelty of the external corrector.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "Human Review 2",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 2,
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          2,
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "2": true,
            "3": true
          },
          "2": {
            "1": true,
            "2": true,
            "3": true
          },
          "3": {
            "1": true,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 9
  },
  {
    "id": 42,
    "paper_id": "pbDqZBn2X2",
    "reference_text": "This paper proposes a cost-aware diffusion solver (CADO) for combinatorial optimization and a fine-tuning framework based on reinforcement learning for diffusion models in this domain. To the best of my knowledge, current techniques using diffusion models as well as RL fine-tuning are not common in combinatorial optimization, and this paper provides a detailed open-source implementation that serves as a baseline for subsequent similar studies. However, the innovation of the method is relatively low; the method of training and fine-tuning is very simple, which is slightly lacking as the innovation point of this paper. I do not see the authors' approach as a significant change from existing algorithms based on reinforcement learning fine-tuning. The method of supervised learning and fine-tuning is more like a trick than sufficient to support the innovation of the article, and taking feasible punishment and optimization goal as reinforcement learning reward is a very simple idea in the field of combinatorial optimization. There is no obvious groundbreaking in addressing the difficult problems in combinatorial optimization, and I am still not sure why this framework is necessary or what necessary adjustments have been made to ensure its applicability to combinatorial optimization problems.",
    "candidate_a_text": "The paper introduces CADO, a framework that combines supervised learning (SL) pre-training and reinforcement learning (RL) fine-tuning for large-scale combinatorial optimization (CO) problems. However, the novelty of the proposed approach is limited, as it combines existing techniques in a straightforward way. The use of RL for fine-tuning is not new, and the paper does not provide a clear motivation for why this particular combination of SL and RL is superior to other possible approaches.",
    "candidate_b_text": "This paper proposes a new Cost-Aware Diffusion solver for combinatorial Optimization (CADO) method via RL finetuning. Overall, the method is really simple, and lacks a strong sense of novelty. In two-phase training, the first phase is the same as DIFUSCO, and the second phase is just adoption of the existing decoder and RL algorithm. The proposed method incorporates the cost information and the decoder, which enhance the overall performance with significant efficacy, but the approach does not present a substantial departure from existing methods.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "Human Review 3",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 3,
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "2": {
            "2": true,
            "3": true
          },
          "3": {
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 2
  },
  {
    "id": 76,
    "paper_id": "ziB549CQ30",
    "reference_text": "This paper proposes a self-supervised algorithm (SS-FJSSP) that employs neural combinatorial optimization to handle fuzzy data and solve the Fuzzy Job Shop Scheduling Problem (FJSSP). My primary concern about this paper is regarding its novelty. While using neural combinatorial optimization (NCO) techniques to solve FJSSP problems might be novel, the high-level graph representation of the FJSSP problem adopted in this paper is very similar to many existing studies. It seems that the key difference is the introduction of fuzzy model related features and feature vectors; however, the technical novelty associated with these features has not been properly highlighted and justified. Meanwhile, the training strategy does not seem to be new. Perhaps random solution selection is a novel element of the training algorithm, but the effectiveness of this selection mechanism is only discussed intuitively. The analogy with genetic algorithms does not reveal the true advantages of using random solution selection techniques, since the proposed NCO system is not closely related to genetic algorithms. The baseline algorithms adopted in this paper were published about 8 to 9 years ago, and it is important to include more recent baselines in the experimental comparison to truly understand the technical advancement introduced by the new approach. In particular, to clearly demonstrate the advantages of the newly developed encoder-decoder network architecture, several existing approaches using different GNN architecture designs should be experimentally examined and compared in this paper.",
    "candidate_a_text": "This paper proposes a self-supervised algorithm (SS-FJSSP) for the FJSSP, which employs an iterative mechanism to refine pseudo-labels, transitioning from suboptimal to optimal solutions. The paper lacks a discussion on the novelty of the proposed method, and I am unclear about the key differences between SS-FJSSP and existing self-supervised learning methods for JSSP, such as [1]. Please clarify the unique contributions of SS-FJSSP. The paper also lacks an introduction to the related work on learning-based methods for JSSP, such as [2-4], and does not discuss how SS-FJSSP compares to these methods or its relevance in the context of existing research.",
    "candidate_b_text": "This submission presents the first application of self-supervised neural combinatorial optimization (NCO) to the Fuzzy Job Shop Scheduling Problem (FJSSP), adapting iterative pseudo-label refinement to handle fuzzy objectives. The main contribution is a domain-specific extension of existing self-supervised NCO methods (notably SLIM) to the fuzzy scheduling context, rather than a fundamentally new NCO methodology. While the authors claim methodological novelty, the core techniques are direct adaptations, and the most significant advance is demonstrating that NCO can assimilate fuzzy information for FJSSP. The reported 100x speedup over state-of-the-art methods is empirically impressive, but may be largely attributable to neural inference efficiency rather than algorithmic innovation. The submission omits several recent, highly relevant self-supervised NCO works and sometimes overstates its methodological distinctiveness, so reviewers should weigh the true novelty as primarily in application domain rather than in conceptual advances.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          2
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "2": true
          },
          "1": {
            "0": true,
            "1": true,
            "2": true
          },
          "2": {
            "0": true,
            "1": true,
            "2": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "1": 1.0,
          "2": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 20
  },
  {
    "id": 23,
    "paper_id": "Y5mm3Yb36I",
    "reference_text": "This paper proposes a framework to measure originality in diffusion models, introducing a metric that, unlike previous memorization studies which need access to training data, is computed at test time and could be quite useful in practice. Understanding originality in T2I generated images is an important and timely topic, and the synthetic data experiments are well motivated and very well-done. However, the authors skipped citing a large number of relevant memorization and prompt inversion literature, and a major concern I have is the authors' definition of originality, which I believe is not quite rigorous and is associated with a lot of subjectivity. The metric’s reliance on dreamsim score as a measure of originality is not fully justified, and even in cases with low dreamsim scores, I still feel the reconstructions are original. The claims are only demonstrated on a small dataset and the earliest SD models, so they might not extend to more recent models with alternative architectures and training data. Some contradictions to the proposed framework might exist, and it would be useful to see how the method performs on challenging cases from prior work to better establish its contribution.",
    "candidate_a_text": "This paper proposes a framework for evaluating the originality of text-to-image (T2I) generative models, particularly in the context of copyright considerations. The method leverages the number of hidden states in a new and innovative way, presenting a fresh and interesting idea. While the paper introduces some interesting ideas, there are concerns regarding novelty, as existing methods can reproduce the original image by selecting specific noise seeds, and several studies have explored this area [1,2], which raises questions about using the number of hidden states as a measure of originality. Additionally, even if the relationship between \"easier to describe\" and \"shorter token length\" holds, the model appears to measure relative originality/creativity [3], which depends on the training dataset of the diffusion model and raises questions about its effectiveness for applications such as copyright infringement detection.",
    "candidate_b_text": "This paper introduces a new approach to quantify the originality of images generated by text-to-image (T2I) diffusion models, focusing on copyright originality. The paper introduces a novel method to quantify image originality in T2I models by correlating it to the number of tokens required for reconstruction. The authors argue that stable diffusion models can create novel, unseen elements when trained on diverse data. However, the paper does not provide a detailed comparison with existing methods for measuring originality or similarity in generative models, nor does it address how this method compares with existing techniques for assessing the originality or similarity of generated images, such as similarity metrics based on feature extraction.",
    "candidate_a_label": "Human Review 1",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 3,
      "candidate_a_system": 1,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 3,
        "consensus_score": 1.0,
        "num_reviews": 4,
        "consensus_type": "majority",
        "other_reviews": [
          0,
          1,
          2
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "2": false,
            "3": true
          },
          "1": {
            "0": true,
            "1": true,
            "2": false,
            "3": true
          },
          "2": {
            "0": false,
            "1": false,
            "2": true,
            "3": true
          },
          "3": {
            "0": true,
            "1": true,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 0.6666666666666666,
          "1": 0.6666666666666666,
          "2": 0.3333333333333333,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 21
  },
  {
    "id": 40,
    "paper_id": "PYmrUQmMEw",
    "reference_text": "This paper introduces LLaMA-Omni, an end-to-end model architecture designed for low-latency, high-quality speech interaction with large language models (LLMs), and presents InstructS2S-200K, a large dataset of conversational, oral-style speech instructions and responses to support more natural speech-based dialogue. While the InstructS2S-200K dataset addresses a notable gap in conversational speech instruction data, the novelty of the model architecture itself is limited. The proposed system primarily combines existing components—namely, a pre-trained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder—without a clear breakthrough in architectural design. Specifically, connecting a speech encoder to an LLM via a speech adaptor has been widely explored in prior research, and the streaming TTS module closely follows previous designs, particularly the approach outlined in Fang et al. (2024) [1]. As such, although LLaMA-Omni successfully assembles a functional pipeline for speech-based interactions with LLMs, it largely builds upon established methods and does not deliver significant innovation in model design. This reliance on prior work somewhat limits the paper’s impact from a research innovation standpoint, and further clarification of the unique contributions would be necessary to better establish its novelty.",
    "candidate_a_text": "LLaMA-Omni is positioned within the rapidly evolving field of end-to-end, low-latency speech interaction models that integrate LLMs with streaming speech processing, closely related to recent works like Mini-Omni and Moshi. While the authors claim novelty in eliminating intermediate transcription and enabling simultaneous text and speech generation, these features are already present in the most relevant prior art, making the conceptual advances largely incremental. The main technical distinction is the use of a CTC-based non-autoregressive streaming decoder with a LLaMA backbone, along with notable resource efficiency, though these improvements may stem from implementation choices rather than fundamental innovation. The introduction of the InstructS2S-200K dataset is a useful contribution, but similar datasets exist and direct comparisons are lacking. Overall, the submission is a strong instantiation of current trends, but its core contributions should be more rigorously contextualized against closely related recent works, as the claimed novelty is often overstated and the technical delta is modest.",
    "candidate_b_text": "This paper introduces LLama-Omni, an innovative end-to-end model architecture for real-time speech interaction with large language models (LLMs). The method is similar to previous work such as SALMONN, except for the TTS module, which is replaced by a vocoder in this study. However, the authors do not sufficiently discuss the differences between the proposed method and SALMONN.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {}
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 1
  },
  {
    "id": 54,
    "paper_id": "ueeqGvQozB",
    "reference_text": "This paper introduces ML4MILP, an original benchmark collection targeted to the MILP domain, aiming to fill the gap of having a large scale, standardized benchmark for evaluating mixed integer linear program solvers. The benchmark is relevant for the ML and DL community, especially as ML-based approaches are becoming popular in this field. The introduction and related work provide a well-written, self-contained overview of existing solvers, techniques, and related benchmarks, highlighting the general lack of a larger standardized benchmark for MILPs. Based on structural and embedding distance between instances, the authors show that existing benchmarks are often more heterogeneous than the MILP benchmark they propose, which allows for some insight into the structure of the proposed and existing benchmarks. However, the overall contribution—introduction of the benchmark and showing the lack of homogeneity of existing classes of instances based on similarity evaluation metrics with respect to structure and neural embedding distances—is somewhat limited in scientific insight and currently results in me having a hard time voting for acceptance at a main ML and DL conference such as ICLR; maybe submitting the paper to a Benchmark and Dataset track of a suitable conference might be more targeted.",
    "candidate_a_text": "This paper proposes a benchmark for evaluating machine learning-based mixed-integer linear programming solvers, consisting of a dataset of 100,000 MILP instances classified into 70 classes, and provides a baseline of different algorithms on the dataset. While the dataset is significantly larger than previous ones, I find that the proposed dataset is not significantly different from the existing ones, and the authors do not provide a clear motivation for why the existing benchmarks are not sufficient. The authors claim that existing benchmarks are not homogeneous, but they do not provide a clear definition of homogeneity in the context of MILP instances, nor sufficient evidence that the proposed dataset is indeed more homogeneous than the existing ones. The argument that the similarity between instances in the same class is higher than in existing benchmarks is not convincing without a clear explanation of how similarity is measured and why it matters. Although the proposed dataset’s larger size is emphasized, the authors do not provide a clear justification for why a larger dataset is necessarily better. Overall, the novelty and significance of the contribution are unclear due to a lack of rigorous comparisons to prior benchmarks, definitions of claimed improvements, and justifications for the benefits claimed.",
    "candidate_b_text": "This submission introduces ML4MILP, a large-scale, open-source MILP benchmark featuring over 100,000 instances and a novel, embedding-based instance classification scheme that combines graph statistics and GNN-derived embeddings. Its main methodological advance is the use of GNN-based embeddings for fine-grained instance categorization, which, while new for MILP datasets, is an incremental extension of existing clustering and representation learning techniques. The dataset’s scale and the embedding-based classification provide more nuanced groupings than prior benchmarks (e.g., Distributional MIPLIB, ML4CO), though the underlying problem diversity remains similar. Some claims of “first” or uniquely fine-grained classification are somewhat overstated, as prior datasets do offer domain and hardness labels, and the provision of high-quality solutions and benchmarking protocols largely follow established practice. Overall, the submission’s primary contribution is infrastructural—advancing benchmark scale and classification methodology—rather than introducing fundamentally new algorithmic concepts.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "2": {
            "2": true,
            "3": true
          },
          "3": {
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 47
  },
  {
    "id": 90,
    "paper_id": "HyN9POiYhN",
    "reference_text": "This paper investigates gender bias in autoregressive large language models (LLMs) by examining bias transfer—the correlation between models’ intrinsic bias (as measured by next-token prediction) and their bias in zero- or few-shot prompting—across a range of experimental settings and open-source models, with a particular focus on the effectiveness of prompt engineering techniques for mitigating such bias. However, the novelty of the work is limited, as there is already a substantial body of literature addressing similar questions regarding zero- and few-shot evaluation of causal LLM bias (see, for example, Ganguli et al., 2023; Bai et al., 2024; Lin et al., 2024; Huang et al., 2023; and the comprehensive survey by Ranjan et al., 2024). The paper does not cite this closely related prior work and is written as if previous studies of LLM bias were restricted to masked language models or fine-tuned autoregressive models, which artificially inflates its contribution. While the specific focus on bias transfer as a metric and the detailed experimental findings may offer some incremental novelty—particularly since prior work such as Ladhak et al. (2023) has only considered bias transfer in the context of summarization tasks and fine-tuning—the overall research area is already crowded. Thus, the main contribution appears to be the application of the bias transfer metric to a broader set of tasks and models, but this advance is modest in light of the extensive existing literature. The paper would benefit from a more thorough engagement with prior work and a clearer articulation of how its findings differ from or build upon established research in the field.",
    "candidate_a_text": "This submission systematically investigates whether gender bias present in pre-trained causal LLMs (such as Mistral, Falcon, and Llama) transfers to downstream tasks (pronoun co-reference resolution) when models are adapted via prompting, rather than fine-tuning. The key empirical finding is a strong, robust correlation (≥0.90) between intrinsic and downstream gender bias in prompt-adapted causal LLMs, in contrast to prior work on fine-tuned masked language models, which found weak or no such transfer. The study’s methodological novelty lies in its focus on prompt-based adaptation in causal LLMs and its systematic variation of prompt and few-shot factors, providing robustness evidence for the main result. While the authors’ claims of being the first to systematically analyze bias transfer in this adaptation regime are largely accurate, the distinction from some recent related work (e.g., Ladhak et al., 2023) could be more clearly articulated. Overall, the submission offers a substantive empirical advance in understanding bias transfer dynamics in modern LLMs, though its conceptual novelty is primarily in the adaptation regime and model type rather than in the general framework of bias transfer analysis.",
    "candidate_b_text": "This paper investigates the transfer of gender bias from pre-trained causal language models to prompted models using the WinoBias dataset, focusing on prompt-adapted causal models, which is timely given the increasing use of prompting in deploying language models. However, the paper's scope is limited to gender bias and the WinoBias dataset, which may not fully capture the complexity of bias transfer in language models, and it does not extensively compare its findings with previous work on bias transfer in masked language models (MLMs). Given the conflicting findings in prior research, a more detailed discussion of how the results relate to and differ from previous work is needed to establish the paper's originality. The study's contribution lies in highlighting the strong correlation between intrinsic biases in pre-trained models and those in prompted models, but the lack of a thorough comparison with existing literature and exploration of a wider range of biases and datasets somewhat limits the significance and uniqueness of the contribution.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {}
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 13
  },
  {
    "id": 99,
    "paper_id": "9I6UOIfbwf",
    "reference_text": "This paper presents a novel synthesis video dataset for the video face re-aging task, created via a proposed pipeline that features many subjects covering a diverse range of age groups, along with a baseline video face re-aging architecture and two tailored novel metrics for evaluating temporal consistency. However, overall, lack of novelty is a disadvantage of this manuscript. The video face re-aging dataset is constructed by a pipeline with three stages, each relying on off-the-shelf methods: Style-based Age Manipulation (SAM) for image-based face re-aging, OSFV for key frame generation, and FILM for motion generation, resulting in a general pipeline for constructing video datasets. The proposed baseline architecture is composed of off-the-shelf building block stacks, such as a recurrent block (RB) and Unet-based Encoder-Decoder, and even the input fashion is borrowed from Zoss et al., such as 5 channels with age masks, as well as the discriminator with PatchGAN proposed by Isola et al. The proposed Temporal-Age (T-Age) metric measures the age difference between two adjacent frames utilizing an off-the-shelf age classifier from Rothe et al. In short, this manuscript can be considered as a regular technical report, and it has a gap to meet the novelty requirement for acceptance.",
    "candidate_a_text": "This assessment finds that the submission addresses video face re-aging with a focus on temporal consistency, but its claimed contributions—namely a synthetic paired video dataset, a baseline architecture, and new temporal metrics—are largely incremental extensions of recent work rather than fundamental advances. The authors overstate the novelty of their dataset and evaluation metrics, as similar resources and approaches (e.g., Re-Aging GAN++, STIT, Zoss et al., 2022) already exist and are not adequately cited or compared against. The baseline architecture is a routine recombination of established modules, and the proposed metrics are adaptations of prior identity/attribute consistency measures, with differences mainly in terminology and regional focus. Empirical improvements reported may stem from dataset scale or implementation details rather than conceptual innovation, and the lack of direct comparison to the most relevant recent methods weakens the claims of superiority. Overall, the submission’s main delta lies in the specific combination and scale of existing techniques, with technical novelty that is incremental rather than groundbreaking.",
    "candidate_b_text": "This paper presents a solution to the temporal consistency issue in video face-aging approaches by introducing a video data generation pipeline for synthetic video datasets, a video face aging framework with a recurrent U-Net structure, and new metrics (TRWC and Temporally Age Preservation) to validate temporal consistency and age transformation. The novelty of the paper is limited as most sections are \"inspired\" or \"motivated\" from previous approaches. For the data generation process, it relies on StyleGAN and SAM for single-frame aging results, and adopts the OSFV technique for generating faces at different poses and expressions as well as motion generation for temporal smoothing. The video aging architecture is not novel, as it is just a recurrent U-Net with commonly used losses. The structure of the paper is more about putting multiple (previous) approaches together in an engineering manner rather than emphasizing novelty.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "Human Review 2",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 2,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          2
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "2": true
          },
          "2": {
            "0": true,
            "2": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "2": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 25
  },
  {
    "id": 25,
    "paper_id": "xtlMtbVfWu",
    "reference_text": "This paper introduces EDiT, a distributed training method for large language models that leverages local SGD within a 2D-grid topology to reduce communication overhead, incorporates a novel gradient penalty to stabilize training with smaller batch sizes, and proposes an asynchronous extension to address stragglers. While the paper provides a detailed implementation and highlights practical advantages in throughput, the overall novelty of the distributed implementation is limited. The sharding strategy and distributed setup closely follow established approaches such as the ZeRO series and FSDP, and similar distributed local SGD methods have already been explored in prior work, including CO2 (Sun et al., 2024), which utilizes ZeRO for autoregressive language modeling, and SlowMo (Wang et al., 2020), which treats all GPUs within a cluster node as a single local worker to achieve larger batch sizes via data parallelism. As such, the distributed implementation itself is a straightforward and natural extension of existing techniques rather than a fundamentally new contribution. The primary novel aspect appears to be the introduction of the gradient penalty method, which is empirically shown to stabilize training—a contribution that stands out more clearly than the distributed implementation. Therefore, the paper’s main advance lies in this stabilizing technique, and the novelty claims regarding the distributed implementation should be moderated in light of substantial prior work.",
    "candidate_a_text": "This paper introduces EDiT, a novel approach for training large language models (LLMs) using distributed training techniques. The paper presents a novel approach that combines Local SGD with model sharding, addressing the limitations of existing distributed training methods in large-scale settings. EDiT is designed to overcome communication bottlenecks, straggler issues, and poor scalability, and employs layer-wise parameter synchronization during the forward pass to reduce communication overhead and enable overlap between computation and communication. Additionally, EDiT uses a pseudo-gradient penalty strategy to address training instability and improve performance, and presents an asynchronous variant, A-EDiT, tailored for heterogeneous clusters.",
    "candidate_b_text": "This assessment finds that the EDiT submission primarily advances the field through a specific integration of Local SGD, model sharding, and layer-wise parameter synchronization, targeting communication efficiency and heterogeneity in large-scale LLM training. While the pseudo gradient penalty and fully asynchronous variant (A-EDiT) are presented as novel, similar stability mechanisms and asynchrony strategies are well-established in prior literature; the main contribution is their tailored application and empirical validation at LLM scale. The authors’ characterizations of related work sometimes understate the sophistication of existing solutions, and several relevant works (e.g., Qsparse-Local-SGD, SHAT, Pisces) are omitted, potentially overstating EDiT’s uniqueness. The most substantive differentiation lies in the system-level engineering and comprehensive large-scale experiments, rather than in fundamental algorithmic innovation. Reviewers should calibrate novelty expectations accordingly, recognizing that the field increasingly values robust system integration and empirical scale, with many recent advances being incremental combinations of established techniques.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {}
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 26
  },
  {
    "id": 83,
    "paper_id": "jCNRcHrfLo",
    "reference_text": "This paper introduces a novel approach, Hierarchical Prompts with Context-Aware Calibration (HiCA), to enhance Open Vocabulary Object Detection (OVD). HiCA improves generalization by utilizing hierarchical prompts that map object regions through both coarse- and fine-grained knowledge, capturing shared information across base and novel classes. Additionally, context-aware calibration refines detection by linking contextual information with object categories, reducing background interference. Despite the emphasis on enhancing detection for novel classes in OVD, the improvement shown in Tables 1 and 4 is relatively minor, and most modifications do not outperform simpler, template-based methods for novel classes, which raises concerns about the practicality and overall impact of the proposed approach for new class detection.",
    "candidate_a_text": "This paper proposes hierarchical prompts with context-aware calibration (HiCA) for open-vocabulary object detection, leveraging both coarse-grained superclass knowledge and fine-grained category knowledge to improve generalization to novel classes while utilizing the visual context of the image to enhance these effects. However, I find that the proposed hierarchical prompts method is quite similar to the learnable multi-modal prompts method in [1], where multiple learnable text tokens are also used to prompt the text encoder for generating text embeddings. While the authors do not use learnable visual prompts as in [1], I still think the proposed hierarchical prompts method is an incremental work based on [1]. The originality of the method appears limited by this similarity, and a comparison experiment with [1] is necessary to better establish the specific contribution.",
    "candidate_b_text": "This paper proposes HiCA, a novel approach for open-vocabulary object detection. In HiCA, Hierarchical prompts leverage coarse-grained superclass knowledge to avoid biasing the model towards base classes, and Context-Aware calibration revises detector results by learning category distribution through environmental context knowledge. The idea of using Hierarchical prompts to capture the shared knowledge between both base and novel classes is interesting and proven. Experimental results demonstrate that HiCA consistently outperforms state-of-the-art methods in detecting novel objects.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "Human Review 1",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 1,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.5,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": false,
            "3": true
          },
          "1": {
            "0": false,
            "1": true,
            "3": false
          },
          "3": {
            "0": true,
            "1": false,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 0.5,
          "1": 0.0,
          "3": 0.5
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 27
  },
  {
    "id": 69,
    "paper_id": "RiQRUcjXBD",
    "reference_text": "This paper introduces a system called SciPIP, designed to help researchers come up with new ideas more effectively by retrieving relevant studies from a research database and leveraging a model to brainstorm entirely new ideas, aiming to balance novelty and feasibility. While SciPIP generates promising, innovative concepts and combines literature retrieval with brainstorming, I have concerns about the true originality of the generated ideas. Large language models may rely on knowledge from their training data, potentially producing ideas that are essentially variations of existing research rather than truly novel contributions. The current evaluation mainly relies on comparing generated ideas to existing papers using similarity scores, but this approach has limitations in assessing true novelty and academic value, as high similarity scores might indicate copying or minor tweaks, and low scores do not necessarily signal a breakthrough. Since similarity scores are based on semantic overlap, they do not capture the underlying logic, originality, or technical feasibility, making it difficult to measure the actual contribution or innovation of the ideas. The reliability of using large language models to evaluate originality is questionable, and supplementing the evaluation with expert reviews or alternative methods could better ensure that SciPIP’s outputs demonstrate true innovation and relevance to the research community.",
    "candidate_a_text": "This paper introduces SciPIP, a framework that leverages large language models to generate scientific paper ideas by integrating literature retrieval and brainstorming. The dual-path idea proposal method is an interesting approach that combines inspiration from existing literature with pure LLM-based brainstorming, aiming to balance novelty and feasibility, which are critical in scientific idea generation. However, the paper does not provide a detailed comparison of SciPIP with existing tools or methodologies, making it difficult to assess its unique contributions and advantages. Additionally, the idea proposal section needs a clearer description of how novelty and feasibility are quantified and assessed, and the paper does not provide enough detail on the specific metrics or criteria used for evaluating the novelty of generated ideas.",
    "candidate_b_text": "This paper introduces SciPIP, an automated system for literature retrieval and scientific idea generation that demonstrates notable improvements over previous baselines and introduces several techniques enhancing both the literature retrieval and idea generation modules. Although SciPIP extends SciMON's framework with additional techniques and demonstrates improved performance, the enhancements are generally minor and come with extra computational costs and complexity. In SciMON, a related paper on scientific idea discovery, semantic, entity, and citation signals are also leveraged during literature retrieval, so the novelty of SciPIP’s approach is incremental. Furthermore, the reviewer questions the quality and originality of the generated ideas, noting that, for example, SciPIP generates an idea for multimodal learning involving “minimizing a contrastive loss function to bring together similar instances of different modalities,” which is a widely accepted approach in the field (e.g., CLIP [1], published in 2021), suggesting that the generated ideas may not be particularly novel. Additionally, the reviewer expresses concern that scoring generated ideas solely based on a novelty score from an LLM may not be reliable, as it seems unlikely that LLMs alone can accurately determine novelty.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "Human Review 1",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 1,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 4,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          2,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "2": true,
            "3": true
          },
          "1": {
            "0": true,
            "1": true,
            "2": true,
            "3": true
          },
          "2": {
            "0": true,
            "1": true,
            "2": true,
            "3": true
          },
          "3": {
            "0": true,
            "1": true,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "1": 1.0,
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 28
  },
  {
    "id": 45,
    "paper_id": "xy9yv5siYQ",
    "reference_text": "This paper addresses the task of dynamic scene reconstruction, specifically developing a deformable 3DGS representation from an unposed monocular video. However, there is a lack of novelty, as the proposed method is more like a mixed bag which combines [1], [2], [3], and [4]. Specifically, the hexplane-based deformable Gaussian field has been explored by [1], the relative pose initialization is adopted from [2], the reprojection loss in Eq. 12 is similar to that of [3], and the depth alignment loss in Eq. 13 is similar to Eq. 15 in [4]. The authors may have to make their contributions more explicit.",
    "candidate_a_text": "This work presents a framework to efficiently reconstruct dynamic scenes from casually captured monocular videos, introducing a camera estimation module for frame-wise camera poses and representing deformations of Gaussians using a HEX-Plane representation. While camera pose estimation with relative initialization and joint optimization is novel, the technical novelty is limited overall; combining HexPlane representation with Gaussian Splatting does not seem novel, as many published works have combined TriPlane representation with Gaussian Splatting. Although disentangling dynamic and static objects in scenes is sound, the adoption of HexPlane representation is not well-presented, especially since a simple Fourier series, as used in Splatter-a-Video, can also represent Gaussian dynamics in the center position and rotations, and HexPlane introduces significantly more computation and storage overhead. This work is concurrent with similar Gaussian video representations, such as Splatter-a-Video and GFlow, and discussion on these works is still necessary, as these two works have been publicly available on arXiv for about four months before the ICLR submission deadline. The relative camera pose module is designed only with depth priors, focusing on relative camera movement between two frames, which is consistent with that in Dust3R; it is unclear why Dust3R was not directly applied.",
    "candidate_b_text": "The submission presents an incremental advance in dynamic, pose-free 3D Gaussian Splatting (3DGS) for monocular video-based scene reconstruction, introducing a Hexplane encoder and SE(3)-based pose initialization as its main technical novelties. While the authors claim to be the first to achieve unified, high-quality dynamic 3DGS reconstruction without known camera poses, similar capabilities have already been demonstrated in recent works such as MoSca and Shape of Motion, with differences primarily in architectural and optimization details. The claimed contributions—unified static/dynamic modeling, pose-free operation, and geometric regularization—are routine in the field, with the main delta being the specific implementation of the Hexplane encoder and pose estimation module. The authors’ positioning of prior work sometimes exaggerates their own novelty and underrepresents the unification and capabilities present in related methods. Overall, the work is a solid, well-executed technical improvement within a rapidly evolving area, but reviewers should recognize that its advances are incremental rather than fundamentally new.",
    "candidate_a_label": "Human Review 2",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": 2,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          2
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "2": true
          },
          "2": {
            "0": true,
            "2": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "2": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 29
  },
  {
    "id": 98,
    "paper_id": "A72sZWB66Q",
    "reference_text": "This paper presents HyperDet, a novel and generalizable detection framework designed to effectively identify synthetic images by integrating shared knowledge from lightweight expert detectors and leveraging a large pretrained vision model. I find the approach novel, particularly in its incorporation of hypernetworks that generate optimized weights for specialized LoRA experts, which enhances the extraction of generalized discernible artifacts. The authors also propose an SRM filter grouping strategy to capture varying levels of pixel artifacts and introduce a novel objective function to balance pixel and semantic artifacts. However, I note that the claimed novel objective function is simply a weighted sum of the binary cross-entropy loss of the original image and the filtered image.",
    "candidate_a_text": "This paper presents HyperDet, a novel and effective framework for detecting synthesized images with high generalization capabilities. While the proposed method combines low-level and semantic features and uses a mixture of experts (MoE) for selection, the paper lacks sufficient motivation and insight into why this combination is novel and significant. Simply combining existing techniques without providing a compelling rationale or analysis of the advantages over prior work may not meet the bar for novelty and significance required by ICLR.",
    "candidate_b_text": "HyperDet is an incremental advance in the field of AI-generated image detection, building most directly on the MoLE paradigm by introducing a hypernetwork to generate and merge LoRA weights for expert detectors—a technical extension rather than a conceptual leap. While the use of a hypernetwork for LoRA weight generation is new in this context, other contributions such as grouping SRM filters and balancing pixel/semantic objectives are routine adaptations of established ideas. The empirical results show strong performance gains, but these may be attributable to ensembling and large-scale training rather than the specific hypernetwork mechanism. The authors’ characterizations of prior work are generally accurate, though they somewhat overstate the novelty and omit several recent, relevant feature fusion and hierarchical detection methods. Reviewers should recognize HyperDet as a solid technical extension with notable empirical results, but not as a fundamentally new direction in the field.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "3": true
          },
          "3": {
            "0": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 0
  },
  {
    "id": 41,
    "paper_id": "aueXfY0Clv",
    "reference_text": "The proposed Depth Pro model employs a ViT architecture for zero-shot metric monocular depth estimation, targeting applications such as novel view synthesis. While Depth Pro benefits from pretrained ViT backbones, its architecture primarily builds on existing elements rather than introducing fundamentally new mechanisms for depth estimation, which limits its architectural novelty. The paper introduces a two-stage training approach that integrates synthetic and real-world datasets, enhancing depth boundary accuracy, and includes new metrics for evaluating depth boundaries that address a gap in existing benchmarks by focusing on boundary precision, which is critical for applications like view synthesis that demand fine details.",
    "candidate_a_text": "This paper presents a foundation model for zero-shot metric monocular depth estimation that is capable of producing metric depth maps with absolute scale on arbitrary images without requiring metadata such as camera intrinsics. The model's ability to produce metric depth maps with absolute scale on arbitrary images without requiring metadata such as camera intrinsics is a significant advancement in the field of monocular depth estimation. The high-resolution output and fast processing time make it suitable for real-time applications. The model's superior performance in sharp delineation of object boundaries and focal length estimation across multiple datasets demonstrates its robustness and accuracy.",
    "candidate_b_text": "This paper introduces a foundational model for single-image depth estimation, trained on a large collection of datasets, resulting in high-resolution depth maps with sharp object boundaries and enabling the estimation of camera intrinsic parameters. Although effective, the proposed method's novelty is somewhat constrained as it leverages existing approaches; the loss functions and training strategy draw heavily from DPT (Ranftl et al., 2022), and the metric depth estimation follows ideas from Metric3D (Yin et al., 2023). The contributions of this work are significant, with the pre-trained model holding substantial potential for a range of downstream tasks, but more discussion on the unique aspects would strengthen the paper.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "Human Review 3",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 3,
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "2": {
            "2": true,
            "3": true
          },
          "3": {
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 11
  },
  {
    "id": 50,
    "paper_id": "2ofVtMvRil",
    "reference_text": "This study demonstrates that predictive coding can effectively train neural networks to develop hexagonal grid representations from spatial inputs, providing a biologically plausible explanation for the emergence of grid cells in the medial entorhinal cortex. However, my major concern is that the work may lack novelty. The use of non-negative and sparse network designs to produce grid cell-like patterns has been extensively discussed, with prior work reporting that non-negative and sparse properties can generate grid cell-like patterns and theoretically demonstrating why non-negativity is the main driver of grid cell formation, which the author's paper does not address, instead of sparsity. Similar findings have also been reported elsewhere, and earlier work proves that a nonnegativity constraint on firing rates induces a symmetry-breaking mechanism favoring hexagonal firing fields, with further studies exploring the necessary conditions for generating grid cells. Prediction tasks, including path integration, that produce grid cell-like patterns have also been widely reported, especially when the input data takes a place cell-like form, and other studies have used place cell-like input and path integration tasks to train networks and generate grid cells, while some have theoretically analyzed the role of predictive learning in forming low-dimensional representations. In my understanding, tPCN is very similar to a one-step RNN (apart from the difference in local learning rules), so the fact that its training process resembles that of one-step tBPTT is not surprising; as previously noted, the key to forming grid cells lies in the predictive task, not the RNN network itself, and therefore, the similarity between tPCN and RNN does not offer significant insight into the generation of grid cells. For these reasons, I believe this paper does not offer substantial novelty or make a clear contribution to the field.",
    "candidate_a_text": "The authors investigate how a temporally-dependent version of predictive coding can extract compact latent spaces in the form of periodic grid activity from temporally structured place cell input. Overall, the study seems like an incremental follow on of the tPCN paper applied to a new domain, but which does not require fundamental changes to the original algorithm. Several existing studies, largely cited in the paper, already investigate the formation of such successor representations by predictive coding. My primary concerns center around the novelty of the algorithm beyond tPCN itself; simply applying a non-negative constraint and applying to a new task does not seem like a sufficiently novel contribution for ICLR.",
    "candidate_b_text": "This paper introduces a novel approach to modeling grid cell formation in the medial entorhinal cortex using predictive coding networks (PCNs). I find the use of predictive coding to model grid cell emergence to be a novel and compelling approach, offering a biologically plausible alternative to models that rely on backpropagation, which is unlikely to be implemented in the brain. The paper’s main contribution lies in its novel application of predictive coding to model grid cell emergence, providing a unified learning algorithm for diverse cortical representations. The authors successfully demonstrate that grid cells can arise as latent representations learned through predictive coding in both static and dynamic environments—a significant contribution to the field—and show that the learning rule of temporal predictive coding networks (tPCNs) implicitly approximates truncated backpropagation through time (BPTT). They compare their model with recurrent neural networks trained with BPTT, showing that both models can achieve similar performance in terms of path integration and grid score, while emphasizing the biological plausibility of their approach. However, I note several weaknesses that limit the clarity of the paper’s originality: the work lacks a detailed comparison with prior models, particularly recurrent neural networks and attractor networks, making it difficult to fully appreciate the unique contributions of the proposed model. While the authors mention advantages such as not requiring biologically implausible backpropagation and addressing the need for local learning rules, they do not provide an in-depth analysis of how their model mechanistically diverges from or improves upon existing approaches. This leaves the reader with an incomplete understanding of how the PCN model advances the field beyond existing models, and undermines the assessment of its true novelty and significance. Overall, I believe the paper makes a significant contribution by presenting a novel and biologically plausible model for grid cell formation, but the originality would be better established with a more thorough and mechanism-level comparison to prior work.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "3": true
          },
          "3": {
            "1": true,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 32
  },
  {
    "id": 68,
    "paper_id": "QoDDNkx4fP",
    "reference_text": "This paper proposes a novel framework, ETA (Evaluating Then Aligning), which addresses a critical gap in the safety of Vision Language Models (VLMs) by focusing on inference-time alignment. Unlike existing approaches that primarily rely on extensive fine-tuning or are limited to specific types of inputs, ETA offers a fresh perspective by combining multimodal evaluation and bi-level alignment without requiring additional training. This plug-and-play nature makes it a highly original contribution, providing a more flexible and scalable solution for enhancing VLM safety. The significance of this work is substantial, as it introduces a method that does not require fine-tuning and can be easily integrated into existing systems, making it practical for widespread use. By improving safety while maintaining model efficiency and general capabilities, ETA could encourage broader adoption of VLMs.",
    "candidate_a_text": "ETA proposes a modular, plug-and-play, inference-time safety alignment framework for vision-language models (VLMs), combining explicit multimodal evaluation (visual and textual) with bi-level alignment (interference prefix and best-of-N search). Its main contribution is the systematic integration and empirical validation of these established techniques, rather than introducing fundamentally new algorithms. While ETA demonstrates improved safety and utility over prior methods, its novelty is incremental, as similar modular inference-time defenses (e.g., ECSO, AdaShield, MLLM-Protector) exist, and several relevant recent works are omitted from the comparison. The authors’ claims of unique novelty and prior work limitations are somewhat overstated, though their engineering advance is meaningful. Reviewers should weigh ETA’s practical integration and robust evaluation against the incremental nature of its contribution and the need for more comprehensive contextualization within the rapidly evolving field.",
    "candidate_b_text": "This paper proposes a new safeguard mechanism for the Vision Language Model (VLM) during the inference phase. However, although the simplicity of the proposed methods, the novelty seems significantly limited, as the proposed methods are based on the simple application of existing models such as CLIP and Reward Models.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "Human Review 1",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 1,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          1
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": false
          },
          "1": {
            "0": false,
            "1": true
          }
        },
        "agreement_scores": {
          "0": 0.0,
          "1": 0.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 24
  },
  {
    "id": 37,
    "paper_id": "m8Rk3HLGFx",
    "reference_text": "This paper presents a multi-view video generation approach based on an input textual prompt or monocular video, incorporating a multi-view synchronization module into a pre-trained text-to-video model to maintain appearance and geometry consistency across multiple viewpoints. The multi-view video generation is a novel task, and this paper shows some promising results in this direction, where the temporal consistency within each single view and spatial consistency across different views are maintained reasonably well. The proposed way of using multi-view images, synthetic multi-view videos, and real-world monocular videos to train the model is novel, providing an alternative approach to solve the data scarcity issue when high-quality real-world multi-view videos are hard to obtain.",
    "candidate_a_text": "This paper presents SynCamMaster, a novel approach for generating synchronized multi-camera videos from diverse viewpoints, addressing a significant gap in the field of video generation. I find several aspects of this paper to be particularly strong, with the most significant being the introduction of SynCamMaster as the first method to address the multi-camera video generation problem—a novel contribution that fills a gap in the literature, as previous works have primarily focused on single-camera video generation or multi-view image generation for static scenes. The proposed multi-view synchronization module (MVS) is another strength, as it effectively integrates camera information into the generation process, and the use of a hybrid training scheme allows the model to learn from a variety of data sources, addressing the challenge of limited multi-camera training data. The empirical results are compelling, demonstrating that SynCamMaster significantly outperforms existing methods in terms of view synchronization and visual quality, and the extension of the method to novel view synthesis highlights its versatility. The release of a multi-view synchronized video dataset is also a valuable contribution. Overall, the paper is well-motivated and makes a substantial contribution by addressing the limitations of previous methods and introducing a novel approach to multi-view video generation.",
    "candidate_b_text": "This paper introduces a plug-and-play module that enhances a pre-trained text-to-video model for multi-camera video generation, ensuring consistent content across different viewpoints. To address the scarcity of high-quality training data, this paper designs a hybrid training scheme that leverages multi-camera images and monocular videos to supplement Unreal Engine-rendered multi-camera videos. The proposed method enables intriguing extensions, such as re-rendering a video from novel viewpoints.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 4,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {
        "consensus_review": 4,
        "consensus_score": 0.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          6
        ],
        "agreement_matrix": {
          "4": {
            "4": true,
            "6": false
          },
          "6": {
            "4": false,
            "6": true
          }
        },
        "agreement_scores": {
          "4": 0.0,
          "6": 0.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 34
  },
  {
    "id": 84,
    "paper_id": "9aIlDR7hjq",
    "reference_text": "The authors propose a frozen alternative to increase the diversity of synthetic training images generated by diffusion models, which conditions the diffusion model not only on few-shot images (done previously) but augmentations (novelty). The method itself is quite simplistic from a novelty perspective (simply adding augmentations to the conditioning). I would consider this a strength if the results were consistent and strong with a clear storyline for effective use-cases; however, I do not see this as being the case. I do not find the CFG scale experiments as adding significant value, as they are consistent with previous work and do not provide more interesting, surprising, or novel results, which in my opinion waters down the impact of the experimental section.",
    "candidate_a_text": "This paper presents an approach of augmentation-conditioning, which leverages real images with data augmentations to create synthetic images that are both realistic and diverse, aiming to bridge the domain gap between synthetic and real data and enhance downstream classification performance without requiring extensive fine-tuning of the diffusion model. However, the technical novelty of the proposed method seems limited, as it mainly combines existing data augmentations, like Mixup, before inputting images into an existing diffusion model. More discussion of the method’s novelty is necessary, and additional experiments comparing the superiority of the proposed method with current works, such as recent tuning-free approaches for diffusion models, would strengthen the paper. While the method focuses on tuning-free augmentation approaches, there are low-cost tuning methods, such as using LoRA, that also deliver strong performance, raising the question of why tuning-free methods are prioritized when low-cost tuning options might achieve better results with only a minor increase in computational cost. The application scope of the proposed method appears limited, primarily for long-tail or few-shot classification, and the experimental validation seems insufficient to fully establish its contribution.",
    "candidate_b_text": "This paper proposes a new method of generating synthetic training images by conditioning the generation process on an augmented real image and a text prompt. The idea of using augmented real images as a reference to guide the generation process is interesting. However, the paper lacks a comparison with a strong baseline that fine-tunes the diffusion model on the target dataset using a method like textual inversion, which makes it difficult to assess the true effectiveness of the proposed method.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          2,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "2": true,
            "3": true
          },
          "2": {
            "0": true,
            "2": true,
            "3": true
          },
          "3": {
            "0": true,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 35
  },
  {
    "id": 35,
    "paper_id": "HyN9POiYhN",
    "reference_text": "This paper investigates gender bias in autoregressive large language models (LLMs) by examining bias transfer—the correlation between models’ intrinsic bias (as measured by next-token prediction) and their bias in zero- or few-shot prompting—across a range of experimental settings and open-source models, with a particular focus on the effectiveness of prompt engineering techniques for mitigating such bias. However, the novelty of the work is limited, as there is already a substantial body of literature addressing similar questions regarding zero- and few-shot evaluation of causal LLM bias (see, for example, Ganguli et al., 2023; Bai et al., 2024; Lin et al., 2024; Huang et al., 2023; and the comprehensive survey by Ranjan et al., 2024). The paper does not cite this closely related prior work and is written as if previous studies of LLM bias were restricted to masked language models or fine-tuned autoregressive models, which artificially inflates its contribution. While the specific focus on bias transfer as a metric and the detailed experimental findings may offer some incremental novelty—particularly since prior work such as Ladhak et al. (2023) has only considered bias transfer in the context of summarization tasks and fine-tuning—the overall research area is already crowded. Thus, the main contribution appears to be the application of the bias transfer metric to a broader set of tasks and models, but this advance is modest in light of the extensive existing literature. The paper would benefit from a more thorough engagement with prior work and a clearer articulation of how its findings differ from or build upon established research in the field.",
    "candidate_a_text": "This paper investigates the transfer of gender bias from pre-trained causal language models to prompted models using the WinoBias dataset, focusing on prompt-adapted causal models, which is timely given the increasing use of prompting in deploying language models. However, the paper's scope is limited to gender bias and the WinoBias dataset, which may not fully capture the complexity of bias transfer in language models, and it does not extensively compare its findings with previous work on bias transfer in masked language models (MLMs). Given the conflicting findings in prior research, a more detailed discussion of how the results relate to and differ from previous work is needed to establish the paper's originality. The study's contribution lies in highlighting the strong correlation between intrinsic biases in pre-trained models and those in prompted models, but the lack of a thorough comparison with existing literature and exploration of a wider range of biases and datasets somewhat limits the significance and uniqueness of the contribution.",
    "candidate_b_text": "This paper investigates the bias transfer hypothesis (BTH), i.e. whether social biases in LLMs, internalized during pre-training, transfer into harmful task-specific behavior after model adaptations are applied, with a focus on causal models under prompt adaptations. The paper's contribution is limited; the authors only study the correlation between intrinsic and extrinsic biases in causal models under prompting and do not provide any mitigation strategies.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {}
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 36
  },
  {
    "id": 96,
    "paper_id": "CKYsXi0dOV",
    "reference_text": "This paper presents BLIP-3-Video, a novel multimodal language model for video understanding that introduces an innovative temporal encoder to significantly reduce the number of visual tokens needed to represent videos, offering a new approach to efficiency in video understanding models. While the integration of the temporal encoder is highlighted as a key innovation, I question the core novelty of this paper compared to prior works such as LLaMA-VID, Video-LLaVA, LLaVA-VID, and LLaVA-NEXT, as similar architectures have already been explored in these works. Although the experimental results show that 32 tokens achieved better performance on four short video benchmarks, this standard may change with different video lengths, scenarios, and question complexity, raising concerns about the scalability and generalizability of the method. Perhaps a more effective mechanism for accommodating more frames and selecting key information for video question answering from a large number of visual tokens is worth exploring, rather than focusing on a specific numerical value of visually overfitted tokens on a few benchmarks.",
    "candidate_a_text": "This paper introduces BLIP-3-Video, a novel multimodal language model designed for efficient video understanding, with the core innovation lying in its temporal encoder, which significantly reduces the number of visual tokens required to represent a video and directly addresses the computational challenges of processing long videos in large vision-language models. I find that the core idea of using a temporal encoder to reduce the number of visual tokens is a significant contribution, and the exploration of two distinct temporal encoders—spatio-temporal attentional pooling and Token Turing Machines (TTM)—is another strength that allows for a comparison of different approaches to temporal modeling within the same framework. However, the paper's novelty is somewhat limited by its reliance on existing techniques, as the core idea of using a temporal encoder to reduce the number of visual tokens is not entirely new and similar approaches have been explored in prior works. The authors acknowledge this to some extent, but a more detailed comparison with existing methods, such as token pruning or token merging techniques, would help to better contextualize the contributions of the proposed model and highlight its unique aspects. Despite this, the fact that BLIP-3-Video can abstract a video into as few as 32 tokens while maintaining high accuracy, and achieves competitive performance compared to much larger state-of-the-art models, is a remarkable achievement and constitutes a significant step towards more efficient video understanding. Overall, while the approach offers practical and efficiency improvements, the lack of a thorough analysis of how the proposed temporal encoder differs from existing token reduction methods makes it difficult to fully assess the model's unique contributions.",
    "candidate_b_text": "This paper presents BLIP-3-Video, which introduces a \"temporal encoder\" alongside a conventional visual tokenizer, allowing it to significantly reduce visual tokens (32 tokens compared to thousands in other models). However, the primary weakness is the insufficient novelty. As detailed in Section 2.2, the only improvements to TTM include (1) time-stamped positional encodings and (2) a 'grouped' TTM temporal encoder. These minor changes do not substantiate a significant contribution.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "Human Review 3",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 3,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "3": true
          },
          "3": {
            "0": true,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 37
  },
  {
    "id": 59,
    "paper_id": "XTxdDEFR6D",
    "reference_text": "This paper introduces LLM4Solver, a framework that leverages large language models (LLMs) to assist in algorithm design for combinatorial optimization, specifically targeting Mixed Integer Linear Programs. The approach involves using an LLM to generate algorithmic components in a programming language, which are subsequently refined through evolutionary algorithms. The authors claim that this method surpasses state-of-the-art human-designed and learning-based heuristics in terms of solution quality and efficiency across various benchmarks. However, the novelty of the approach appears limited, as similar uses of LLMs for algorithm design and optimization have already been explored in recent literature. Notably, works such as \"Evolution of heuristics: Towards efficient automatic algorithm design using large language model,\" \"LLaMoCo: Instruction tuning of large language models for optimization code generation,\" \"LLaMEA: A large language model evolutionary algorithm for automatically generating metaheuristics,\" \"ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution,\" \"Autonomous multi-objective optimization using large language model,\" and \"OptiMUS: Optimization Modeling Using MIP Solvers and large language models\" have addressed related ideas. While LLM4Solver demonstrates promising results, the core concept of prompting LLMs to generate code for optimization tasks is not particularly novel, and the paper primarily compares itself to the work of Kuang et al. without sufficiently situating its contributions relative to the broader body of recent work. The main contribution seems to be the specific integration of LLM-generated code with evolutionary refinement, but this incremental advance does not substantially differentiate the approach from existing methods. To more convincingly establish its novelty, the paper would benefit from expanded experimental comparisons and a clearer articulation of how its framework advances beyond prior LLM-based optimization techniques.",
    "candidate_a_text": "LLM4Solver applies the established paradigm of combining large language models (LLMs) with evolutionary search to the new domain of *exact* mixed-integer programming (MIP) solver algorithm design, evaluating on the challenging MIPLIB 2017 benchmark. While the methodology—LLM-driven code/algorithm generation with derivative-free optimization—is very similar to recent works like AEL, ReEvo, and AutoSAT, LLM4Solver’s main contribution is its focus on full solver algorithms for exact MIP, rather than heuristics or metaheuristics for routing or packing. The authors’ claims of being “first” are accurate for this specific application domain, but not for the general approach, and the distinction between “heuristic” and “algorithm” is sometimes overstated. Some relevant related works (e.g., DeepACO, Sym-NCO) are omitted, and the empirical gains may be partly attributable to benchmark choice. Overall, the work represents an incremental but well-executed advance in a rapidly evolving field, with its primary novelty in application scope rather than fundamental methodology.",
    "candidate_b_text": "This paper introduces LLM4Solver, a novel framework that leverages large language models (LLMs) to automatically design high-quality algorithms for combinatorial optimization (CO) solvers, specifically focusing on diving heuristics within branch-and-bound (B&B) algorithms. I find the core idea of using LLMs to generate algorithmic components for CO solvers to be both novel and promising, representing a significant achievement in demonstrating that LLMs can effectively tackle the complex task of algorithm design, which has traditionally been challenging for machine learning methods. Unlike many neural network-based approaches, LLM4Solver produces interpretable heuristics with code comments, making it easier for domain experts to understand and improve the underlying logic and search patterns, which I consider another major strength. The multi-objective extension of the framework, allowing for heuristics that generalize across different problem types, addresses a key limitation of many existing approaches and stands out as a valuable contribution. The empirical results compellingly demonstrate that LLM4Solver outperforms both human-designed heuristics and a state-of-the-art learning-based method in terms of solution quality, solving efficiency, and cross-benchmark generalization, with ablation studies showing the necessity of combining LLMs and evolutionary algorithms for high performance. Overall, I believe that this paper makes a significant contribution to the field of automated algorithm design and has the potential for significant impact, as the framework’s ability to generate high-quality, interpretable heuristics with improved generalization capabilities represents a substantial step forward.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {}
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 8
  },
  {
    "id": 80,
    "paper_id": "p6eQRlaxGo",
    "reference_text": "This paper presents AtomFlow, a flow matching model for designing a protein structure to bind a small molecule ligand, which jointly denoises the structure of the protein and ligand and does not require knowledge of the ligand pose, unlike RFDiffusionAA. The methodology can be described as a flow-matching version of RFDiff-AA and does not score high on originality or novelty from a ML perspective. Further, the flow model architecture and noising process are based on AlphaFlow, with different justification but no difference in practice as far as I can tell. The overall significance of the contribution is unclear as it represents an incremental methodological advance over RFDiff-AA with more or less the same model capabilities. The authors argue that not needing to specify the ligand pose is a big plus, but no meaningful evidence or use case is provided for this distinction.",
    "candidate_a_text": "ATOMFLOW introduces a generative model for protein-ligand complex design that jointly generates both ligand and protein structures from 2D molecular graphs, addressing ligand flexibility and protein-ligand interactions in a unified, end-to-end framework. The main technical advance is the application of SE(3)-equivariant flow-matching at the atomic level, enabling iterative, joint updates of both ligand and protein, which distinguishes it from prior models that either require fixed ligand conformers (e.g., RFDiffusionAA) or are limited to pocket refinement (e.g., PocketGen, FlowSite). The authors’ claim of being the first to enable de novo, all-atom complex generation from 2D ligand graphs is accurate within the generative modeling context, though unified atomic representations have been explored in predictive models like Umol. While the unified \"biotoken\" framework and atomic flow-matching are meaningful extensions, the underlying concepts are increasingly standard in the field; the novelty lies in their integration for this specific generative task. Overall, the submission represents a substantive and well-substantiated advance, though some claims slightly overstate conceptual novelty, and additional comparisons to related predictive and sequence-structure co-design models could further clarify its scope.",
    "candidate_b_text": "This paper presents the ATOMFLOW architecture for the design of protein folds conditioned by the ligand SMILES description. It's a well written technical paper about a novel architecture that can be useful for the bioinformatics community, but there is little technical novelty in the architecture blocks, loss function, or flow matching process (but the architecture is novel and useful). When generating folds for a purine ligand, the results were extremely conservative, with zero novelty and extremely low diversity, preferring the fold that dominantly occurs in the PDB, which suggests a significant bias towards folds that are abundant in the PDB. The designs have scores > 0.5, meaning that the fold structure is the same as in the PDB and no new folds have been discovered. It seems AtomFlow has memorized the space of all protein folds in the PDB and simply selects the one it has seen during training, so I suggest the authors run more cross-validation tests hiding some fold classes from the training and trying to reproduce them in the test.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "Human Review 2",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 2,
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          2,
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "2": true,
            "3": true
          },
          "2": {
            "1": true,
            "2": true,
            "3": true
          },
          "3": {
            "1": true,
            "2": true,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "2": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 17
  },
  {
    "id": 57,
    "paper_id": "AAjCYWXC5I",
    "reference_text": "This paper introduces a zero-shot in-context adversarial learning framework for Large Language Models (LLMs) aimed at enhancing research ideation. The approach leverages a multi-agent system inspired by the academic peer review process, with distinct proposer, reviewer, and area chair roles, to iteratively refine research ideas along the axes of novelty and feasibility. This framework fills a notable gap in the field by conceptually adapting adversarial learning to the context of LLM-driven idea generation, an area that has seen limited exploration. The use of a peer review-inspired multi-agent setup to promote iterative improvement in idea generation represents a conceptually novel contribution, distinguishing the work from prior approaches that typically do not model such structured, adversarial interactions among LLM agents. Overall, the paper’s novelty lies in its creative adaptation of adversarial learning principles and academic peer review dynamics to the automated ideation process, offering a fresh perspective and a promising direction for advancing LLM-based research support systems.",
    "candidate_a_text": "This paper introduces a zero-shot in-context adversarial learning framework to enhance research ideation using multi-LLM-agent interactions, where a proposer agent, reviewer agent, and area chair agent iteratively refine research ideas along novelty and feasibility dimensions, mirroring the academic review process. The multi-agent framework is well-designed, highly engaging, and interactive. The authors also propose a relative quality ranking metric to evaluate idea generation quality, showing that this approach significantly improves the novelty and feasibility of generated research ideas.",
    "candidate_b_text": "This paper presents a novel approach to enhancing research ideation using large language models (LLMs) by introducing a \"zero-shot in-context adversarial learning\" framework specifically tailored for research idea generation. The method draws inspiration from generative adversarial networks (GANs) but adapts the adversarial paradigm to the unique challenges posed by LLMs and open-ended ideation tasks. This represents a creative combination of existing concepts applied to a new domain, as the adversarial learning framework is operationalized through a unique multi-agent system in which each agent (Proposer, Reviewer, Area Chair) plays a distinct role in refining research ideas. Additionally, the paper proposes a new evaluation metric, the \"relative quality ranking score,\" designed to assess the novelty and feasibility of LLM-generated research ideas relative to a benchmark of human-generated ideas. This metric addresses the challenge of evaluating open-ended text generation by moving beyond traditional metrics and offering a more nuanced assessment. While the paper positions both the adversarial learning framework and the relative quality ranking metric as novel contributions, the novelty primarily lies in the creative integration and adaptation of existing ideas rather than the introduction of entirely new theoretical constructs. The GAN-inspired framing, while innovative, is more metaphorical than rigorously mathematical, and the multi-agent system mimics established peer review dynamics in a new context. Furthermore, although the relative quality ranking metric is presented as a novel evaluation tool, a more thorough discussion of its relationship to existing metrics for open-ended text generation would strengthen the novelty claim, as it remains unclear whether similar approaches have been explored in prior literature. Overall, the paper offers a fresh and thoughtfully integrated approach to LLM-driven research ideation, but its novelty is best characterized as a creative synthesis and domain-specific adaptation of established ideas, rather than a fundamentally new paradigm.",
    "candidate_a_label": "AI System: openreviewer",
    "candidate_b_label": "Human Review 1",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "openreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 1,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          1
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": false
          },
          "1": {
            "0": false,
            "1": true
          }
        },
        "agreement_scores": {
          "0": 0.0,
          "1": 0.0
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 29
  },
  {
    "id": 18,
    "paper_id": "jCNRcHrfLo",
    "reference_text": "This paper introduces a novel approach, Hierarchical Prompts with Context-Aware Calibration (HiCA), to enhance Open Vocabulary Object Detection (OVD). HiCA improves generalization by utilizing hierarchical prompts that map object regions through both coarse- and fine-grained knowledge, capturing shared information across base and novel classes. Additionally, context-aware calibration refines detection by linking contextual information with object categories, reducing background interference. Despite the emphasis on enhancing detection for novel classes in OVD, the improvement shown in Tables 1 and 4 is relatively minor, and most modifications do not outperform simpler, template-based methods for novel classes, which raises concerns about the practicality and overall impact of the proposed approach for new class detection.",
    "candidate_a_text": "This paper proposes hierarchical prompts with context-aware calibration (HiCA) for open-vocabulary object detection, leveraging both coarse-grained superclass knowledge and fine-grained category knowledge to improve generalization to novel classes while utilizing the visual context of the image to enhance these effects. However, I find that the proposed hierarchical prompts method is quite similar to the learnable multi-modal prompts method in [1], where multiple learnable text tokens are also used to prompt the text encoder for generating text embeddings. While the authors do not use learnable visual prompts as in [1], I still think the proposed hierarchical prompts method is an incremental work based on [1]. The originality of the method appears limited by this similarity, and a comparison experiment with [1] is necessary to better establish the specific contribution.",
    "candidate_b_text": "The submission introduces HiCA, a method for open-vocabulary object detection that combines explicit hierarchical prompts (superclass/class) with context-aware calibration via unsupervised visual context clustering and a distribution generation layer, all within a prompt learning framework. While the technical implementation—particularly the use of hierarchical prompts and unsupervised context clustering for calibration—is novel, the underlying conceptual ideas of leveraging semantic hierarchies and contextual information are well-established in recent literature (e.g., HierKD, DetCLIPv3, HTRPN). The authors tend to overstate their novelty, underrepresenting the sophistication and relevance of prior work that already integrates hierarchy and context, albeit through different mechanisms such as distillation or generative modeling. The main delta lies in the specific combination and technical realization of these ideas within prompt learning, rather than in a fundamentally new conceptual advance. Reviewers should note that, in this mature and rapidly evolving field, most recent contributions—including this one—are incremental and technical, and direct comparison to the most relevant recent works is needed to fairly assess the true contribution.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 0.5,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          1,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": false,
            "3": true
          },
          "1": {
            "0": false,
            "1": true,
            "3": false
          },
          "3": {
            "0": true,
            "1": false,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 0.5,
          "1": 0.0,
          "3": 0.5
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 5
  },
  {
    "id": 30,
    "paper_id": "YCdag94iZs",
    "reference_text": "This paper introduces a technique called MILCA, designed to perform counting and summing of features, where feature weights are predicted using a fully connected network (FCN) with a projection replacing the softmax layer to produce coefficients within a specified range. In my view, this paper lacks sufficient novelty. The counting-based approach appears to be a straightforward extension within the MIL space, and it does not introduce any new theoretical contributions either.",
    "candidate_a_text": "This paper introduces a novel approach to Multiple Instance Learning (MIL) by incorporating counting and attention mechanisms, where instead of relying on a single aggregated bag embedding for classification, the method identifies representative features across the dataset and encodes each bag as a count of these features, with informative features identified using statistical methods like the Mann Whitney test. While the concepts and issues presented in the paper have been previously discussed, the methods introduced offer a fresh perspective, and the method proposes a seemingly novel approach for encoding bags as a count of dataset-wide features. However, there is limited technical novelty, as the idea of counting in MIL is not novel and was already explored in other works, and the model reuses a lot of existing modules such as the Mann-Whitney test, thus novelty is also limited. The proposed approach seems to have better performance than the baseline on the disease classification using T cells repertoires benchmark, and the paper introduces a new dataset, named Wiki dataset, as an additional contribution. The Table 1 should compare with recent or state-of-the-art methods, otherwise the second novelty claim is not valid.",
    "candidate_b_text": "MILCA is a simple, efficient Multiple Instance Learning (MIL) method that extends counting/summing approaches by introducing learned feature weights and a projection step, aiming to improve interpretability and efficiency over attention-based models. The submission positions itself as an alternative to attention-based and graph-based MIL, but the technical novelty is incremental, mainly involving a different normalization (projection vs. softmax) and learned weighting, both of which have been explored in related motif-based and aggregation methods. Empirical results show modest accuracy improvements (about 3%) and efficiency gains, particularly in high-dimensional, low-sample regimes, though these may be context-dependent and not unique to MILCA. The authors somewhat overstate the conceptual distinction between \"counting\" and \"attention,\" as both are forms of weighted aggregation, and the practical impact of their technical variations may be limited. Overall, MILCA offers a practical, interpretable, and efficient extension of existing counting-based MIL methods, with its main contribution being empirical performance rather than a fundamentally new paradigm.",
    "candidate_a_label": "Human Review 2",
    "candidate_b_label": "AI System: ours",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": 2,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "ours",
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 4,
        "consensus_type": "majority",
        "other_reviews": [
          0,
          2,
          3
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": true,
            "2": false,
            "3": true
          },
          "1": {
            "0": true,
            "1": true,
            "2": true,
            "3": true
          },
          "2": {
            "0": false,
            "1": true,
            "2": true,
            "3": false
          },
          "3": {
            "0": true,
            "1": true,
            "2": false,
            "3": true
          }
        },
        "agreement_scores": {
          "0": 0.6666666666666666,
          "1": 1.0,
          "2": 0.3333333333333333,
          "3": 0.6666666666666666
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 38
  },
  {
    "id": 74,
    "paper_id": "9Orm76dUuT",
    "reference_text": "This paper presents an approach that extends visual universal adversarial attacks, originally designed for image classification, to large multi-modal models (MLLMs) by optimizing adversarial perturbations in conjunction with a predefined trigger token to elicit specific harmful responses. First, to the best of my knowledge, no prior work has explored attacks in this particular setting, combining adversarial perturbations on images with a triggering token. However, given that existing approaches, such as Dong et al. (2023b), have already demonstrated that visual universal adversarial perturbations (UAPs) can be applied to MLLMs (potentially in a black-box setting as well), the technical novelty of this work appears somewhat limited, especially so given the limited transferability. From a practical perspective, the lack of cross-model transferability significantly reduces the attack's relevance.",
    "candidate_a_text": "This submission addresses test-time backdoor attacks on multimodal large language models (MLLMs) in black-box settings, claiming novelty in decomposing the attack across visual (setup) and textual (activation) modalities using universal adversarial perturbations. While the authors position their work as the first to demonstrate such attacks without access to training data or model parameters, several recent works (e.g., Jailbreak in Pieces, Visual Role-Play) have already explored similar test-time, black-box, and multimodal attack paradigms. The main technical refinement lies in the explicit separation of setup and activation across modalities, which offers operational flexibility but is conceptually similar to compositional or adaptive triggers in prior work. The submission tends to overstate its novelty and omits direct comparison with the most relevant recent literature, sometimes mischaracterizing prior work as limited to unimodal or training-time settings. Overall, the contribution is best viewed as an incremental technical extension rather than a paradigm shift, and claims of being \"first\" should be moderated in light of the rapidly evolving field.",
    "candidate_b_text": "This paper presents a test-time backdoor attack method called AnyDoor, which employs adversarial noise on images alongside static triggers at the text level. The exploration of backdoor attacks on multimodal large language models represents a novel research area. However, the proposed AnyDoor attack is more accurately classified as an adversarial attack rather than a backdoor attack, as it primarily relies on optimizing adversarial noise in images. The core technique combines standard adversarial perturbations (for images) with specific string triggers (for text), but universal adversarial perturbations and token-level triggers have already been extensively studied in existing literature. As a result, the proposed attack does not introduce new insights or techniques for the backdoor research community, and the technical novelty is limited.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "Human Review 2",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 2,
      "consensus_info": {
        "consensus_review": 0,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          2
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "2": true
          },
          "2": {
            "0": true,
            "2": true
          }
        },
        "agreement_scores": {
          "0": 1.0,
          "2": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 43
  },
  {
    "id": 73,
    "paper_id": "E1Tr7wTlIt",
    "reference_text": "The paper introduces a novel approach called Partial Vector Freezing (PVF), designed to reduce computation in Secure Aggregation Protocols (SAPs) without increasing communication overhead, and further proposes the disrupting variable extension to PVF to support enhanced privacy. While I appreciate the clarity and straightforwardness presented in your methodology, I am concerned about the apparent simplicity of the proposed solution. The approach, as described, seems to lack the level of innovation. Consider expanding on the theoretical background, comparing your method with others in detail, and emphasizing any novel insights or improvements that your solution offers.",
    "candidate_a_text": "This assessment reviews the λ-SecAgg with Partial Vector Freezing (PVF) submission, which proposes a modular approach to reduce computation and communication in secure aggregation for federated learning by freezing most of the update vector and processing only a fraction, with mechanisms for full aggregate recovery and privacy preservation. The submission is well-situated within the active field of secure aggregation, closely related to prior work on sparsification, partial encryption, and modular protocol innovations, though it sometimes overstates its distinctness and omits discussion of several relevant works. The main claimed advance is the ability to achieve efficiency gains without information loss or extra communication, but this claim requires stronger empirical and conceptual comparison to advanced sparsification and partial encryption methods, which may offer similar benefits. Integrating PVF with existing secure aggregation protocols and introducing a privacy-preserving element appear to be logical extensions rather than fundamentally new contributions. Overall, while the approach is timely and potentially useful, the novelty and significance of the technical contributions depend on clearer differentiation from prior art and more comprehensive evaluation.",
    "candidate_b_text": "This paper introduces a novel method called λ-SecAgg, which integrates a module named Partial Vector Freezing (PVF) into Secure Aggregation Protocols (SAPs) for federated learning. The concept of freezing and unfreezing vector entries in the context of secure aggregation is very novel, effectively reducing the computational burden on SAP, which has been a significant bottleneck in real-world federated learning applications, especially for large-scale models such as Large Language Models (LLMs). Additionally, the paper proposes a Disrupting Variables Extension (DVE) that enhances privacy by adding noise to the frozen entries using Differential Privacy (DP). The strengths of the method lie in its innovative design, theoretical rigor, and comprehensive evaluation, showing significant performance improvements.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "Human Review 3",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 2,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 3,
      "consensus_info": {
        "consensus_review": 2,
        "consensus_score": 0.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "2": {
            "2": true,
            "3": false
          },
          "3": {
            "2": false,
            "3": true
          }
        },
        "agreement_scores": {
          "2": 0.0,
          "3": 0.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 44
  },
  {
    "id": 86,
    "paper_id": "QFaj7InstQ",
    "reference_text": "This paper introduces the Item Language Model (ILM), a framework designed to bridge behavioral embeddings from recommendation systems with language understanding in Large Language Models (LLMs). The core contribution lies in adapting a Querying Transformer (QFormer) architecture with a novel item-item contrastive loss, enabling unified and interleaved processing of both behavioral and textual information for language generation tasks. The approach demonstrates innovation in its use of QFormer with item-item contrastive learning to address the modality gap between recommendation signals and language understanding. While this represents a creative step toward integrating behavioral and semantic information, the novelty is somewhat limited by the existence of related work, such as LC-Rec and BinLLM, which also aim to bridge collaborative and semantic signals within large language models. The main distinction of this paper is the specific adaptation of QFormer and the introduction of the item-item contrastive loss, which is well-justified and empirically validated. However, a more thorough comparison with these recent LLM-based recommendation systems would help to better establish the unique contributions and boundaries of novelty. Overall, the paper offers an interesting and incremental advance in the integration of recommendation and language modeling, but its novelty would be strengthened by a clearer articulation of differences from closely related approaches.",
    "candidate_a_text": "This assessment finds that the \"Item Language Model\" (ILM) submission presents incremental advances in the integration of behavioral embeddings into large language models for recommendation, primarily through the use of a Querying Transformer and an item-item contrastive loss. While these architectural and loss function choices are technically distinct, they represent variations on well-established approaches in a rapidly evolving and competitive field. The conceptual framework and problem space are shared with several recent works (e.g., CoLLM, User-LLM, CALRec), some of which are not cited or directly compared against, limiting the clarity of the submission's true novelty. The evaluation methods and claimed improvements are standard for the area, and the submission does not convincingly demonstrate unique capabilities or tasks beyond prior art. Overall, the work is a routine extension of existing research, and reviewers should recognize its contributions as incremental rather than groundbreaking, with a need for more thorough comparison to closely related methods.",
    "candidate_b_text": "This paper proposes a novel approach by adapting the Querying Transformer model to align behavioral and textual data in recommendation systems, integrating collaborative filtering embeddings with large language models. Although these methods are mentioned in the related work, a more detailed comparison with other methods that integrate collaborative filtering with LLMs, such as CoLLM and OpenP5, would strengthen the paper.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "AI System: openreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 0,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "openreviewer",
      "consensus_info": {}
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 22
  },
  {
    "id": 94,
    "paper_id": "ursX3k1rTO",
    "reference_text": "This paper presents a transformer-based architecture to generate symmetric crystals conditioned on space groups in a two-stage process, and proposes a novel representation of crystal symmetry that could facilitate learning of crystal symmetry with deep learning approaches. The paper also proposes metrics to assess the symmetry of the generated crystals and highlights further gains over baseline approaches. While several works are cited in the related works section, neither described nor highlighted the difference from their approach, making it difficult to assess the uniqueness of the contribution. It is also important to add how many new tokens the method generates or if it just predicts the fixed set of tokens in different combinations (and these combinations result in more template novelty than just sampling existing templates from training data). Finally, in Table 1a, I would like to see the number of novel templates as absolute numbers instead of percentages, and the percentage of novel but structurally invalid generations from your method.",
    "candidate_a_text": "The submission presents a permutation-invariant, symmetry-conditioned autoregressive transformer for crystal structure generation, with its main technical novelty being the omission of positional encoding to enforce permutation invariance. This approach is an incremental refinement over closely related prior work, particularly CrystalFormer, which already uses Wyckoff positions and space group conditioning in a similar transformer architecture. The authors’ claims of being the “first” to combine these elements are overstated, as similar representations and conditioning have been explored in recent literature, and some relevant works (e.g., PARD) are not cited. While the empirical results are competitive, the performance gains may stem from implementation details rather than fundamental conceptual advances. Overall, the submission fits within a rapidly evolving field characterized by incremental improvements, and reviewers should interpret claims of novelty and superiority with caution, seeking more direct comparisons and a fuller discussion of related work.",
    "candidate_b_text": "This paper proposes the Wyckoff Transformer, a generative model designed for creating highly symmetric crystal structures by explicitly leveraging space group symmetry through the use of Wyckoff positions as a discrete, permutation-invariant representation of atomic locations. The Wyckoff Transformer introduces a novel approach to crystal generation by utilizing Wyckoff positions to encode symmetries explicitly, making it unique among generative models. Unlike traditional methods, it avoids positional encoding and uses permutation-invariant tokenization tailored to space group symmetries, a creative and effective innovation for materials science. This approach addresses limitations in prior methods, which struggled to produce symmetry-compliant structures, and demonstrates superior performance in symmetry-conditioned generation, creating a diverse set of stable crystal structures that respect the underlying physical symmetries. The model’s potential for symmetry-conditioned generation highlights a promising direction for future research in material informatics and generative modeling.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "Human Review 0",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 0,
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 0.5,
        "num_reviews": 3,
        "consensus_type": "majority",
        "other_reviews": [
          0,
          4
        ],
        "agreement_matrix": {
          "0": {
            "0": true,
            "1": false,
            "4": false
          },
          "1": {
            "0": false,
            "1": true,
            "4": true
          },
          "4": {
            "0": false,
            "1": true,
            "4": true
          }
        },
        "agreement_scores": {
          "0": 0.0,
          "1": 0.5,
          "4": 0.5
        }
      }
    },
    "assignment_type": "overlap",
    "evaluator_sample_id": 33
  },
  {
    "id": 2,
    "paper_id": "m8Rk3HLGFx",
    "reference_text": "This paper presents a multi-view video generation approach based on an input textual prompt or monocular video, incorporating a multi-view synchronization module into a pre-trained text-to-video model to maintain appearance and geometry consistency across multiple viewpoints. The multi-view video generation is a novel task, and this paper shows some promising results in this direction, where the temporal consistency within each single view and spatial consistency across different views are maintained reasonably well. The proposed way of using multi-view images, synthetic multi-view videos, and real-world monocular videos to train the model is novel, providing an alternative approach to solve the data scarcity issue when high-quality real-world multi-view videos are hard to obtain.",
    "candidate_a_text": "This paper presents SynCamMaster, a novel approach for generating synchronized multi-camera videos from diverse viewpoints, addressing a significant gap in the field of video generation. I find several aspects of this paper to be particularly strong, with the most significant being the introduction of SynCamMaster as the first method to address the multi-camera video generation problem—a novel contribution that fills a gap in the literature, as previous works have primarily focused on single-camera video generation or multi-view image generation for static scenes. The proposed multi-view synchronization module (MVS) is another strength, as it effectively integrates camera information into the generation process, and the use of a hybrid training scheme allows the model to learn from a variety of data sources, addressing the challenge of limited multi-camera training data. The empirical results are compelling, demonstrating that SynCamMaster significantly outperforms existing methods in terms of view synchronization and visual quality, and the extension of the method to novel view synthesis highlights its versatility. The release of a multi-view synchronized video dataset is also a valuable contribution. Overall, the paper is well-motivated and makes a substantial contribution by addressing the limitations of previous methods and introducing a novel approach to multi-view video generation.",
    "candidate_b_text": "This paper proposes a plug-and-play module that enhances a pretrained text-to-video model for multi-camera video generation, with a core design centered on an inter-view synchronization module to ensure appearance and geometry consistency across different viewpoints. However, in terms of pioneering work, it has come to my attention that Sora possesses multi-camera video generation capabilities, so the claim that \"SynCamMaster pioneered multi-camera real-world video generation\" warrants reconsideration. Similar modules to the inter-view synchronization module are commonly found in other generation models employing cross-view attention, which suggests that the novelty is incremental. It would be beneficial to emphasize how SynCamMaster distinguishes itself from existing methods.",
    "candidate_a_label": "AI System: deepreviewer",
    "candidate_b_label": "Human Review 6",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 4,
      "candidate_a_system": "deepreviewer",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 6,
      "consensus_info": {
        "consensus_review": 4,
        "consensus_score": 0.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          6
        ],
        "agreement_matrix": {
          "4": {
            "4": true,
            "6": false
          },
          "6": {
            "4": false,
            "6": true
          }
        },
        "agreement_scores": {
          "4": 0.0,
          "6": 0.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 47
  },
  {
    "id": 89,
    "paper_id": "rWIrdAo2xC",
    "reference_text": "This paper proposes to train a conditional diffusion model for generating novel views of humans from given single-view images, directly supervised by proxy ground truth 3D Gaussian attributes. The method outperforms other generalizable novel view synthesis techniques like LGM, and the idea of using a neural network to constrain the distribution of target 3D Gaussian attributes makes sense and is effective. However, although the method demonstrates improvement compared to previous methods like LGM, the image quality remains limited since only one single input image is used, and another potential direction in this field is incorporating a 2D diffusion prior to enhance information, as demonstrated in Human 3Diffusion. A comparison to these baselines is needed.",
    "candidate_a_text": "This submission introduces a novel direct attribute-level supervision paradigm for 3D Gaussian Splatting (3DGS) in generalizable monocular 3D human rendering, diverging from the prevalent pixel-level (indirect) supervision used in prior works. The approach leverages a two-stage proxy attribute construction and a conditional diffusion model to directly supervise 3DGS attributes, which is a substantive technical difference from existing methods like GPS-Gaussian, Zou et al., LGM, and FreeSplat. While the use of diffusion models for 3D attribute generation is not new, its application to 3DGS attributes in this context is novel, though the underlying methodology follows established trends in the field. The authors’ claims of inefficiency and suboptimality in prior pixel-level supervision are sometimes overstated and not always empirically substantiated, and some relevant recent works are not cited or compared. Overall, the main contribution is the new supervision strategy for 3DGS, but the practical impact should be carefully evaluated with empirical results and broader comparisons to recent literature.",
    "candidate_b_text": "This paper introduces a method that renders novel views of human avatars, given only a single image, by proposing a pipeline of multiple stages that first learns a prior and then renders a human under novel views, highlighting their paradigm as \"direct\" and using extracted attributes for supervision in contrast with other \"indirect\" approaches. Even though the authors show good results, the overall contribution and novelty of the paper is not well supported. Components like transformers, stable diffusion, etc. have been used by other papers as well. Learning pixel-aligned features from different views has been proposed by previous works, like the generalizable NeRF ActorsNeRF (ICCV 2023) and MonoHuman (CVPR 2023). Stable diffusion has been used by works for humans, like DreamHuman (NeurIPS 2024), DreamAvatar (CVPR 2024), AvatarPopUp (ECCV 2024), and other works for 3D objects (e.g. DreamFusion). It is not clear to this reviewer what the additional challenges of monocular reconstruction are in the particular set up of this paper and how the particular method addresses them.",
    "candidate_a_label": "AI System: ours",
    "candidate_b_label": "Human Review 4",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": "ours",
      "candidate_a_type": "ai",
      "candidate_b_type": "human",
      "candidate_b_system_or_id": 4,
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 0.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          4
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "4": false
          },
          "4": {
            "1": false,
            "4": true
          }
        },
        "agreement_scores": {
          "1": 0.0,
          "4": 0.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 48
  },
  {
    "id": 77,
    "paper_id": "wE8wJXgI9T",
    "reference_text": "This paper investigates the phenomenon known as the \"modality gap\" in multi-modal contrastive learning models like CLIP and proposes renaming it to \"contrastive gap,\" arguing that this gap emerges as a consequence of contrastive training rather than modality differences. The paper provides a comprehensive empirical analysis of how dimensionality and batch size affect the contrastive gap, offering insights into why this phenomenon occurs in multi-modal models. The proposed solution of adding uniformity and alignment terms to the CLIP loss is relatively simple to implement and shows some improvements in certain tasks. However, the paper incorrectly attributes CLIP's loss function to SimCLR's NT-Xent loss, when CLIP actually builds upon multi-class N-pair loss, which undermines the paper's theoretical foundation and technical credibility. There is also insufficient justification for renaming \"modality gap\" to \"contrastive gap,\" and the paper lacks a comparison with previous modality gap solutions (e.g., Liang et al. 2022).",
    "candidate_a_text": "An empirical study on the impact of several loss functions on the CLIP embedding space has been presented in this work. The key contribution of this work is relatively marginal. The proposed contrastive gap lacks insightful theoretical evidence and guarantees, and the proposed mitigation strategies all build on top of existing works. While the new proposed fine-tuning loss shows some improvements, the overall technical novelty is lacking.",
    "candidate_b_text": "This paper studies the representations of multi-modal contrastive learning and the contrastive gap, proposing to add uniformity and alignment terms to the CLIP loss to reduce this gap. While the authors mention that their method is similar to existing methods, they do not clearly articulate what makes their approach unique, and specifically, the differences in the uniformity and alignment terms compared to existing methods need to be more clearly delineated. The paper lacks a clear explanation of how the proposed method differs from previous approaches and should provide a more detailed comparison of the proposed loss function with existing loss functions in multi-modal contrastive learning.",
    "candidate_a_label": "Human Review 3",
    "candidate_b_label": "AI System: deepreviewer",
    "metadata": {
      "reference_type": "human_consensus",
      "reference_review_id": 1,
      "candidate_a_system": 3,
      "candidate_a_type": "human",
      "candidate_b_type": "ai",
      "candidate_b_system_or_id": "deepreviewer",
      "consensus_info": {
        "consensus_review": 1,
        "consensus_score": 1.0,
        "num_reviews": 2,
        "consensus_type": "majority",
        "other_reviews": [
          3
        ],
        "agreement_matrix": {
          "1": {
            "1": true,
            "3": true
          },
          "3": {
            "1": true,
            "3": true
          }
        },
        "agreement_scores": {
          "1": 1.0,
          "3": 1.0
        }
      }
    },
    "assignment_type": "unique",
    "evaluator_sample_id": 49
  }
]