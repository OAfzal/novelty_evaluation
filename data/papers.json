{
  "RoN6NnHjn4": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper introduces a novel Vec2Face generative framework for generating identity-specific synthetic face images suitable for training deep recognition models, improving on the state-of-the-art with a unique fMAE architecture that differentiates itself from existing diffusion and GAN-based solutions. The approach also proposes to utilize guided perturbation during inference, not only to achieve higher intra-identity diversity of samples but also to allow for control over the pose and quality of images, which is crucial for generating large-scale datasets suitable for training recognition models. Importantly, Vec2Face enables better scalability than existing approaches, allowing the generation of diverse datasets of up to 20 million images compared to existing datasets of 500 thousand images, and the recognition model trained on the generated data even achieves better results on the Cross-Age LFW benchmark than when training with the real CASIA-WebFace dataset, which represents a tremendous achievement as existing approaches typically achieve worse performance. However, despite these strengths, the novelty introduced with this paper remains slightly unclear, as the overall paper gives the impression that the fMAE is novel, but it is not explicitly pointed out whether it shares similarities or draws inspiration from previous works, such as Hu et al. (2023), or if the novelty lies in the projection and expansion of input features to 2D feature maps. Additionally, the observation that a too large inter-class separation does not benefit performance might not be a unique finding, as similar results have been reported in prior work (Boutros et al., 2023). Overall, the paper is suitable for the conference and will likely highly influence future research on the generation of synthetic biometric datasets."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes VEC2FACE to synthesize facial images from a sampled identity vector for synthetic face recognition, utilizing a GAN solution to reconstruct the image at the training stage and adopting the decoder for inference. While the method can generate as many novel identities as possible and achieves good performance compared to the listed literature, I believe the novelty of this paper is incremental, and improvement is good but does not achieve state-of-the-art. The contribution of generating as many novel identities as possible is benefited from the previous work Arc2Face, and adopting pure identity embedding to generate a facial dataset is demonstrated in the related literature, ID3. The attribute operation is also introduced in ID3. The authors state they achieve state-of-the-art results; however, the SOTA result is achieved by CemiFace. Using GAN to achieve such results is inspiring, but overall, the originality is limited by reliance on prior work and the incremental nature of the contribution."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "The paper introduces a novel method called Vec2Face to synthesize large-scale, high-quality face images for training face recognition models. The use of a feature-masked autoencoder is somewhat novel. However, certain claims of novelty in the paper are potentially misleading, and it is crucial to ensure that all claims are accurate and supported by evidence. The number of identities generated (300K-400K) is comparable to existing real-world datasets used to train facial recognition models, which raises concerns about the practical benefits of generating synthetic data at this scale. Additionally, the paper omits a key citation: \"Vec2face: Unveil human faces from their black box features in face recognition\" (CVPR 2020), which appears relevant to the paper's methodology and should be properly acknowledged."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper presents a method and its analysis, with clarifications provided in the rebuttal that were useful for assessing the paper's contribution. This work is considered valuable to the ICLR community."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "Vec2Face is a holistic, vector-driven generative model for synthetic face dataset creation, notable for its explicit control over both inter-class and intra-class variation and its ability to scale to 300K identities within a single model—surpassing previous GAN/diffusion-based methods, though not rendering-based datasets like DigiFace-1M. Its main technical novelty lies in direct vector sampling/perturbation for identity and attribute control, including a gradient descent-based mechanism for attribute manipulation, rather than relying on external attribute models or complex latent space analysis. While the authors claim significant advances over prior work, some of these claims (e.g., on scalability and control) are somewhat overstated, as certain GAN/diffusion and rendering-based methods offer similar capabilities via different mechanisms. The empirical result that Vec2Face-trained models can outperform real data on several benchmarks is notable, though the underlying causes (method vs. data/model/training choices) are not fully disentangled. Overall, Vec2Face represents a substantive incremental advance in controllable, scalable synthetic face generation, but the paper would benefit from a more balanced discussion of related work and a clearer delineation of its true technical and empirical deltas."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a method for synthesizing large-scale face datasets for training face recognition models using a feature masked autoencoder (fMAE) and an image decoder, taking a sampled vector as input to generate and control the identity of face images and their attributes. The authors propose a method to synthesize a large number of face images with well-separated identities and controlled variations in attributes, addressing the limitations of existing methods. The synthesized dataset, HSFace, achieves state-of-the-art accuracy on multiple real-world test sets, demonstrating the effectiveness of the proposed method in training face recognition models."
      }
    ]
  },
  "yYQLvofQ1k": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The paper introduces VIRSCI, a multi-agent, LLM-based system designed to simulate teamwork-driven scientific research, organizing agents to mimic collaborative processes such as selecting collaborators, generating research ideas, assessing novelty, and drafting abstracts. The multi-agent approach proposed in this paper has the potential to greatly enhance the quality and breadth of scientific research, with discussion-oriented idea generation that closely mirrors real scientific processes. The 5-step approach—comprising Collaborator Selection, Topic Selection, Idea Generation, Idea Novelty Assessment, and Abstract Generation—presents a promising and robust framework for idea generation. VIRSCI, as a multi-agent system for scientific collaboration, shows clear advantages over single-agent methods. While VIRSCI may generate highly unique or novel ideas, these are less valuable if experimental designs cannot support them within practical constraints."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes a new multi-agent system VIRSCI, designed to mimic the teamwork inherent in scientific research, and constructs the entire pipeline from team organization to final idea formation. This paper is the first to apply the LLM-based multi-agent system to the problem of scientific discovery, realizing the generation of research ideas in autonomous scientific discovery. The contributions are mainly in pioneering the use of multi-agent technology in a completely new field and demonstrating that integrating collaborative agents can lead to more innovative scientific outputs. The simulation results are consistent with key findings in Science of Science, such as that new teams tend to produce more innovative research, demonstrating VIRSCI's potential as a powerful tool for future research in this field. This is the first attempt to apply the LLM-based multi-agent system to the field of scientific exploration, which allows us to see greater possibilities of AI for science and shows that in the future, multi-agent technology may really be able to make new and valuable scientific discoveries."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "The paper introduces VIRSCI, a Large Language Model (LLM)-based multi-agent system designed to simulate the collaborative nature of scientific research teams, with virtual scientists engaging in discussions to generate, evaluate, and refine research ideas. The paper provides an insightful exploration into how team size affects the novelty of generated ideas, demonstrating that there is an optimal team size that maximizes creativity, which is interesting and contributes to the understanding of team dynamics in scientific idea generation. However, the proposed framework seems to be an application of existing LLM-based multi-agent systems to the specific domain of scientific idea generation, and while the application is interesting, the methodological innovation appears limited, as the framework primarily extends basic LLM multi-agent methodologies without introducing significant novel approaches or mechanisms specific to the challenges of research idea generation. There are also concerns that the use of a dataset overlapping with LLM training data may compromise the ability to accurately assess the novelty of generated ideas, as models might reproduce existing concepts rather than generate genuinely novel ones, and the lack of human evaluation further raises concerns about the robustness of the conclusions regarding novelty."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces VIRSCI, an LLM-based multi-agent system, a framework specifically designed to model the teamwork collaboration style in scientific research. The authors propose a novel research generation framework with LLMs collaborating on this task through a 'team discussion' mechanism to simulate discussion scenarios in real life. With abstracts as the novel ideas output of the model, the authors also construct a benchmark dataset and evaluation metrics to measure the performances, focusing mainly on the novelty of the idea. The paper evaluates the model's performance mainly through a novelty metric derived from embedding distances and based on citation counts. While novelty is important, it should not be the only focus of research idea generation, and the whole paper puts a lot of emphasis on measuring the novelty of the ideas but does not mention if they manually review if the outputs make sense, so there is a chance that the idea is novel but is due to the model's hallucinations."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment reviews a submission introducing VIRSCI, a multi-agent LLM system for scientific idea generation that uniquely integrates real scientist role-play and collaboration network data for team formation, aiming for objective, data-driven evaluation of idea novelty and impact. The work is methodologically closest to recent multi-agent LLM frameworks but distinguishes itself by grounding agent behavior in real-world data and employing rigorous, scalable evaluation metrics, rather than relying solely on subjective human review. While the authors claim to be the first to deliver an end-to-end, multi-agent scientific ideation pipeline, this novelty is somewhat overstated, as similar frameworks exist, though not with this exact integration of social network data and agent role-play. The most substantive contribution is the use of real scientist data for agent instantiation and team selection, which is a genuine methodological advance, whereas the multi-agent and end-to-end aspects are more incremental given current field trends. Overall, the submission advances the field by combining established multi-agent LLM techniques with social science-informed team modeling and objective evaluation, but reviewers should note that some claims of novelty are exaggerated and the precise source of empirical improvements is not fully disentangled from other system components."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper presents a novel approach to scientific idea generation using a multi-agent system, addressing the collaborative nature of scientific research. VIRSCI consists of a team of agents that collaborate to generate, evaluate, and refine research ideas, and is evaluated using real-world data with improved performance over single-agent systems. The findings suggest that multi-agent collaboration can enhance the novelty and impact of generated scientific ideas. The use of real-world data and the comparison with single-agent systems demonstrate the practical relevance and effectiveness of the proposed approach."
      }
    ]
  },
  "uy9oR0nYCW": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper analyses existing explainable methods, such as Occlusion and Attention visualization, for deepfake audio detection tasks using three baseline models and two existing datasets. The paper is merely an analysis paper of existing explainable methods, and there are no significant novel contributions to this paper. Although the authors mention in line 052 that the contributions are \"Empirical evaluations of novel explainability methods for audio transformers,\" I cannot judge what novelty is in attention visualization for transformers, as it has happened a lot in literature. More than 50% of the paper is just explaining trivial and non-paper contributions only."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes a novel explainability framework for audio transformers in audio deepfake classification by utilizing image occlusion to detect feature importance and attention roll-out to better understand features, and also open-sources a novel benchmark for detecting audio deepfakes in real-world cases by training on the ASVspoof5 dataset and testing on the FakeAVCeleb dataset. As the authors themselves have pointed out, attention roll-out and image occlusion-based analysis have been in existence for quite some time, but the novelty of the proposed work lies in applying them in spectrograms to aid in the explainability of audio deepfake analysis. However, how these attention roll-out and image occlusion-based analyses are aiding explainability specific to the audio deepfake analysis is not adequately explained, and how their contribution differs from already existing contributions of attention roll-out and image occlusion-based analysis methods in image feature explainability remains unclear. Using attention visualization to attribute where a transformer model is putting importance is not novel, and their analysis does not show enough contribution specific to explaining the decision process in audio deepfake classification in transformers. The idea of testing on a new dataset itself is not particularly novel, and they have not provided adequate explanations as to how their novel benchmark would be more helpful in audio deepfake classification."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes a conceptual explainability framework for audio deepfake detection, emphasizing sample-specific, time-specific, and feature-specific explanations interpretable by humans, and provides empirical evaluations of explainability methods for transformer-based detection models using occlusion and attention visualization techniques. However, both the proposed methods for audio explainability, occlusion and attention visualization, are already existing methods that are very common in literature, especially for vision and language tasks, and the paper did not make any modality-specific changes or edit the methods in any way for the audio domain. The roll-out attention method was introduced for natural language tokens, and while the paper claims to adapt the method for audio tokens, it is unclear what kind of modifications were made except just replacing the tokens, making the novelty introduced by the paper in these methods questionable. The paper also claims to introduce a novel benchmark to evaluate the generalization capabilities of deepfake audio classifiers by training on one dataset and evaluating on another, but this mechanism is already well-known and well-practiced, with no new contribution to the dataset, evaluation metric, or other changes, so it is questionable to consider this benchmark as a contribution. Thus, the novelty and originality of the paper’s contributions are limited."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper provides an open discussion and analysis of explainable and interpretable algorithms for audio deepfake detection. As Reviewer nNp8 pointed out, this paper is merely an analysis paper of existing explainable methods and there are no significant novel contributions to this paper. Reviewer 3hX6 questioned the contribution of this paper, noting that occlusion and attention visualization are commonly used techniques in computer vision and natural language processing. Reviewer Ee6m has clear concerns about the contribution, token attention plots, and the significance of training. Reviewer xF2E raised concerns about the technical novelty contribution of the paper's usage of existing explainability methods."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission addresses two active themes in audio deepfake detection—explainability for transformer-based models and generalization to real-world, cross-dataset scenarios—by adapting known attention roll-out mechanisms and proposing a tailored conceptual framework. While the authors claim novelty in these areas, the assessment concludes that the main contributions are incremental adaptations of established XAI techniques, with the primary innovation being their application to audio deepfake detection rather than the development of fundamentally new methods. The introduction of a new cross-dataset benchmark (ASVspoof 5 → FakeAVCeleb) is a practical contribution, but cross-dataset evaluation is already standard in the field. The empirical evaluation is thorough, yet similar comparative studies exist, and the submission omits discussion of several highly relevant recent works, sometimes overstating the limitations of prior art. Overall, the submission’s value lies in its comprehensive integration of existing methods and resources, but reviewers should be aware that its conceptual advances are modest and its claims of novelty may be overstated."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes to use explainable methods for state-of-the-art transformer-based audio deepfake detectors and open-sources a novel benchmark for real-world generalizability. The authors also compare and contrast various explainability methods, offering a conceptual contribution that defines key requirements for explainability in deepfake audio detection."
      }
    ]
  },
  "gRmWtOnTLK": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "In this paper the authors propose a new model called RFWave (Rectified Flow Wave) for achieving audio reconstruction from Mel Spectrograms or Encodec tokens. While methodologically the authors borrow many ideas from previous papers, e.g., the model architecture ConvNeXtV2 is not a novelty of the paper, and training in the STFT domain is a classic choice in audio deep learning, it is very interesting how the combination of those ideas can bring us closer on surpassing the fundamental limitations in efficiency of diffusion models from a practical point of view. The results in the experimental section are very good for the proposed model, putting it as a novel strong baseline in the audio reconstruction setting. The proposed losses and the idea of equal straightness intervals are valuable not only in the audio reconstruction context but also more generally when dealing with other types of STFT based models or other generative settings for rectified flow models."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper considers a method for reconstructing the waveform from Mel-spectrogram or discrete acoustic tokens by generating complex spectrograms and performing reconstruction on a frame level, achieving audio generation much faster than in real time. The paper's novelty is in question, as it mainly combines existing components (multiband rectified flow and ConvNeXt V2) rather than proposing a fundamentally new approach. While this engineering solution may have merit in specific applications, the lack of novel contributions limits the paper's impact. Strengthening the theoretical foundation or providing a unique methodological contribution would be beneficial in highlighting the algorithm’s distinct value."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "An efficient diffusion-based vocoder is proposed that generates the complex spectrogram of STFT at the frame level and can be generated in 10 denoising steps. The strength of this paper appears to lie in the application of the latest technique, rectified flow, which enables efficient sampling, and this aspect of the work stands out as a significant contribution. This work introduces three additional loss functions to improve performance, and the additional loss functions appear to consistently improve performance. However, the task addressed in this paper is rather conventional, using standard datasets and common evaluation methods, so the work comes across as a minor variation of existing research and lacks a clear element of originality. In a field where many similar approaches already exist, this work appears to be another study that only slightly improves existing benchmarks. Moreover, the approach is not theoretically groundbreaking either; at first glance, the proposed method appears to be a combination of existing ideas, making its originality and novelty less immediately clear."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper proposes a generative model for audio signals named RFWave, a diffusion-type solution. Inspired by rectified flow studies, the authors introduce an enhanced sampling strategy and novel loss functions designed to generate high-quality audio signals, addressing challenges such as ensuring that nearly-silent portions of the signal do not produce noisy outputs. Achieving these results with only ten sampling steps makes it a particularly remarkable solution."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "RFWave is an integrative approach that combines rectified flow, multi-band processing, and frame-level modeling for efficient and high-fidelity audio waveform reconstruction from Mel-spectrograms. While this specific combination is new for the task, each individual component—rectified flow (VoiceFlow), multi-band processing (Multi-Band Diffusion), and frame-level modeling (APNet2/Vocos)—has been explored in prior work. The authors' claims of being \"first\" in several aspects are somewhat overstated, as relevant works (notably VoiceFlow and PeRFlow) are omitted from citations, which may misrepresent the maturity of rectified flow in audio. The main technical contributions, such as a new sampling time point selection and overlap loss, appear to be incremental enhancements rather than conceptual breakthroughs. Overall, RFWave should be viewed as a strong compositional advance that synthesizes recent trends, and reviewers are encouraged to request direct comparisons to omitted related works for a more complete assessment of novelty."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents RFWave, a novel approach for audio waveform reconstruction using a multi-band Rectified Flow model. The integration of Rectified Flow and the use of STFT frames for parallel processing significantly improve the sampling speed, making RFWave highly efficient compared to existing diffusion-based methods. The paper includes extensive experiments comparing RFWave with existing diffusion and GAN-based models, demonstrating its superior performance in terms of quality and computational efficiency."
      }
    ]
  },
  "gDZd8UGaxS": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents a method for incremental learning that comprises two key components: (1) multiple experts and (2) prototype compensation. Though significant performance improvements are demonstrated, ProCEED shows limited novelty as it simply combines the approaches of Rypeść et al. (2023) (multiple experts) with Zhou et al. (2024b) (prototype compensation), and there is no fundamental innovation in the respective components compared to previous work. In addition, there are some same sentences as in previous literature, such as the description of the model architecture and the formulation of the prototype realignment problem, which closely mirror those in Rypeść et al. (2023) and Zhou et al. (2024b)."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "The paper introduces ProCEED, a method designed to address challenges in exemplar-free class-incremental learning (EFCIL) by realigning representations of previous classes in evolving subspaces and leveraging a mixture-of-experts (MoE) architecture, Gaussian prototypes, and angular drift compensation to preserve decision boundaries of past tasks. However, the paper lacks novelty, as it appears to be an ensemble of SEED [1] and EASE [2], with Section 3.3 being very similar to EASE. The motivation for using MoE is not clearly stated, and it would be beneficial to contrast how ProCEED differs from other MoE-based approaches upfront to contextualize its novelty."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "The paper presents a method called ProCEED for exemplar-free class incremental learning, which leverages an ensemble of mixture of experts (MoE). However, the proposed method, ProCEED, appears to be a blend of two existing incremental learning methods, SEED and EASE, published in recent conference papers with minimal changes. There is no methodology or textual section in the present paper that offers a genuinely novel contribution. While the addition of logit distillation slightly improves performance compared to using only feature distillation (as in SEED), it is a minor novelty and not substantial enough for publication, since logit distillation is already a well-established technique in the incremental learning literature. Simply incorporating it here does not introduce any significant novelty. Furthermore, the use of EASE’s methodology in incremental learning does not introduce novelty when applied within the SEED approach for exemplar-free cold start incremental learning with a fixed number of experts, as transferring a methodology from one incremental context (EASE) to another incremental context (SEED) without substantial innovation or modification does not constitute a meaningful novel contribution. Even the experimental methodology is not entirely novel, as the ablation study mirrors that of the EASE paper, just on different datasets. Finally, since no relevant contribution in this paper appears to be original, the paper lacks novelty."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "ProCEED is a hybrid method for exemplar-free class-incremental learning (EFCIL) that combines prototype realignment, ensemble-based inference, and mixture-of-experts (MoE) knowledge distillation, but its core components are all well-established in the literature. The submission’s main technical novelty is the specific combination and analytic implementation of these elements, rather than introducing fundamentally new concepts. Prior work—especially FeCAM, ADC, EFC, FeTrIL, EASE, SEED, and knowledge distillation methods like REAL—already address prototype drift, analytic classification, ensemble inference, and knowledge transfer, though the authors understate these overlaps and focus primarily on performance comparisons. The field is mature and highly incremental, with many recent works introducing similar combinations and minor technical tweaks, so empirical gains may reflect careful engineering rather than conceptual breakthroughs. Reviewers should note that while ProCEED is a strong and well-integrated system, its novelty is primarily in the integration and implementation details, not in new methodological advances."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a novel approach to address the exemplar-free class-incremental learning (EFCIL) problem by leveraging the statistics of deep features subspace to realign the representation of old tasks into the latest subspace. This approach is innovative and has the potential to improve the performance of EFCIL models. The authors provide a thorough literature review and compare their method with several state-of-the-art methods, demonstrating superior task-agnostic accuracy on challenging benchmark datasets with an equal class distribution across tasks."
      }
    ]
  },
  "m8Rk3HLGFx": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper presents a multi-view video generation approach based on an input textual prompt or monocular video, incorporating a multi-view synchronization module into a pre-trained text-to-video model to maintain appearance and geometry consistency across multiple viewpoints. The multi-view video generation is a novel task, and this paper shows some promising results in this direction, where the temporal consistency within each single view and spatial consistency across different views are maintained reasonably well. The proposed way of using multi-view images, synthetic multi-view videos, and real-world monocular videos to train the model is novel, providing an alternative approach to solve the data scarcity issue when high-quality real-world multi-view videos are hard to obtain."
      },
      {
        "id": "human_review_6",
        "type": "human",
        "label": "Human Review (review_6)",
        "content": "This paper proposes a plug-and-play module that enhances a pretrained text-to-video model for multi-camera video generation, with a core design centered on an inter-view synchronization module to ensure appearance and geometry consistency across different viewpoints. However, in terms of pioneering work, it has come to my attention that Sora possesses multi-camera video generation capabilities, so the claim that \"SynCamMaster pioneered multi-camera real-world video generation\" warrants reconsideration. Similar modules to the inter-view synchronization module are commonly found in other generation models employing cross-view attention, which suggests that the novelty is incremental. It would be beneficial to emphasize how SynCamMaster distinguishes itself from existing methods."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents a simple and efficient method with a thoughtful training strategy for multi-view video generation, which is identified as a novel task. The novelty of the method is highlighted, with clear motivations in the design of each component. The proposed approach uniquely addresses the data scarcity issue."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "SynCamMaster is an incremental advance in the rapidly evolving field of multi-view, 3D-consistent, camera-controllable video generation, building directly on recent works like CVD and CamCo. Its main technical contributions are the integration of a plug-and-play camera encoder and a hybrid-data training pipeline that combines multi-view images, single-view videos, and synthetic data to address data scarcity and improve generalization. While the authors claim to be the first to achieve multi-camera real-world video generation, this is not fully substantiated, as CVD and CamCo already address similar problems with comparable technical approaches. The submission’s novelty lies primarily in practical data engineering and module integration, rather than in introducing fundamentally new methodologies or problem settings. Overall, SynCamMaster offers useful implementation improvements and broader applicability, but its conceptual advances are incremental and its claims of major novelty are overstated relative to the current state of the field."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces a plug-and-play module that enhances a pre-trained text-to-video model for multi-camera video generation, ensuring consistent content across different viewpoints. To address the scarcity of high-quality training data, this paper designs a hybrid training scheme that leverages multi-camera images and monocular videos to supplement Unreal Engine-rendered multi-camera videos. The proposed method enables intriguing extensions, such as re-rendering a video from novel viewpoints."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper presents SynCamMaster, a novel approach for generating synchronized multi-camera videos from diverse viewpoints, addressing a significant gap in the field of video generation. I find several aspects of this paper to be particularly strong, with the most significant being the introduction of SynCamMaster as the first method to address the multi-camera video generation problem—a novel contribution that fills a gap in the literature, as previous works have primarily focused on single-camera video generation or multi-view image generation for static scenes. The proposed multi-view synchronization module (MVS) is another strength, as it effectively integrates camera information into the generation process, and the use of a hybrid training scheme allows the model to learn from a variety of data sources, addressing the challenge of limited multi-camera training data. The empirical results are compelling, demonstrating that SynCamMaster significantly outperforms existing methods in terms of view synchronization and visual quality, and the extension of the method to novel view synthesis highlights its versatility. The release of a multi-view synchronized video dataset is also a valuable contribution. Overall, the paper is well-motivated and makes a substantial contribution by addressing the limitations of previous methods and introducing a novel approach to multi-view video generation."
      }
    ]
  },
  "Mgf7qdUbX5": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The paper explores vulnerabilities in aligning LLMs using RLHF, specifically focusing on backdoor poisoning attacks, and proposes AdvBDGen, an adversarially fortified generative framework that creates prompt-specific, adaptable, and stealthy backdoor triggers resistant to common defensive measures. Unlike prior works that use fixed triggers, the paper leverages GAN-style training to explore context-adaptive and paraphrase-based backdoors that are harder to detect and neutralize. However, the novelty is limited, as the use of GAN-style methods for generating backdoor triggers is well-established and has been extensively explored by the NLP and computer vision (CV) research communities. The comparative analysis between naive LLM-generated paraphrases and those produced by AdvBDGen underscores the method's efficacy and robustness, but the main contribution appears to be the application of these established techniques to prompt-specific, stealthy triggers in the LLM alignment context."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This work introduces a GAN-based framework to produce text backdoors, where the generator is designed to fuse triggers into inputs and the discriminator is responsible for identifying the presence of triggers. The contribution is incremental, lacking novelty in both the problem definition and the technical solution. In terms of problem definition, evasive backdoors have been extensively studied, wherein novel approaches, such as \"style backdoors\" that treat semantic styles as backdoor triggers, have rigorous definitions. However, this work merely relies on a black-box model to produce trigger-embedded inputs, without providing intuitions about the nature of the backdoor. The technical solution is simply a combination of existing modules, without offering strong insights or a truly novel design."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper presents a new approach, AdvBDGen, to generating adaptable, transferable, and stealthy backdoor triggers for large language models (LLMs), with attacks that can automatically generate backdoor triggers during the alignment stage and are robust to perturbations. However, the key motivation of this work and the novelty of the proposed approach are not very clear to me. To the best of my knowledge, semantic-based backdoor attacks are extensively proposed in previous research, including but not limited to the following work: [1,2,3]. This paper only mentions \"constant triggers\" during the motivation and development of the method, which makes the overall contribution and novelty very confusing. Correspondingly, the relevant semantic-based backdoor attack methods are not considered and compared with in the experiments, making the results not solid enough."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper presents interesting ideas and makes some progress, but the limited scope of experimentation on only the PKU Beavertails dataset restricts the generalizability and scientific impact of the results, making it difficult to assess the robustness and practical utility of the proposed defense method. While the additional experiments on perplexity-based and round-trip based defense significantly improve the paper, the lack of integration of the RAP defense method leaves a gap in the comprehensiveness of the study and weakens the contribution. These unresolved issues, particularly the limited experimental validation, diminish the paper's suitability for publication at this time."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission, AdvBDGen, is positioned within the rapidly evolving field of adaptive, stealthy backdoor attacks on LLMs, specifically advancing prompt-specific, fuzzy trigger generation. Its main technical novelty lies in the use of adversarial generator-discriminator training with dual discriminators (strong/weak) to optimize both stealth and installability of backdoor triggers—a setup not present in prior work, though the general idea of adaptive or paraphrased triggers is well established. While the authors accurately distinguish their training-time attack from test-time jailbreaks and cite relevant literature, they tend to overstate their \"first to\" claims and underplay the sophistication of recent related works (e.g., NOTABLE, Mind the Style of Text, BadChain), omitting direct comparisons that would clarify their true contribution. The empirical improvements in stealth, robustness, and transferability are credible but appear to be incremental, with the main conceptual advance being the dual-discriminator adversarial training framework. Overall, the submission represents a substantive but evolutionary step in the field, and reviewers should be aware of both its technical strengths and the somewhat overstated novelty relative to recent literature."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a backdoor attack against LLMs leveraging a generator-discriminator architecture, where the generator produces trigger-encoded prompts classified by the discriminator and is optimized to evade the strong discriminator while being detectable by it. The proposed method is not very novel; the high-level idea of using a generator-discriminator architecture has been explored in the vision domain [1], and the authors simply adapt this idea to the LLM domain."
      }
    ]
  },
  "zOMa82W1HV": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "The paper proposes a sub-linear complexity speech summarization model by combining Q-former and pretrained Mamba, essentially replacing transformers in existing methods (especially arxiv:2407.02005) with Mamba, which leads me to doubt the technical novelty. While the model enables efficient speech summarization and empirical results are strong, the method appears more as a substitution of model components rather than a fundamentally new approach. The Q-former mechanism is similar to a kind of VQ but with continuous features, and there are many other approaches to compress speech signals into \"token-like\" embeddings, such as HuBERT units, speech tokens, and neural audio codec. Given these similarities and the lack of sufficient ablation studies to demonstrate the unique advantages of the proposed methods or to identify the impact of each component, I am not convinced of the originality or significance of the contribution."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper introduces an end-to-end abstractive summarization method that processes speech inputs directly, utilizing a querying-attention Mamba projector to condense extended acoustic features into compact semantic tokens, which are then fed into the Mamba Large Language Model (LLM), and further employs Direct Preference Optimization (DPO) fine-tuning to produce coherent summaries. The Mamba-based approach has not yet been utilized for speech summarization. However, I feel that Mamba-based approaches have been shown to be useful for various speech processing tasks and have been shown to be particularly efficient for long-form speech processing. The paper has limited novelty since it additionally verifies an expected conclusion that Mamba-based architecture is also useful for another speech processing task, namely speech summarization. Further, the lack of clarity in setup of baseline models make me question the improvements claimed in the work."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper describes an approach to summarizing 6-minute-long audio recordings by combining the Whisper speech encoder with the Mamba LLM through a cross-attention based Mamba querying projector. I have serious concerns about the novelty of the proposed approach. From a modeling standpoint, the work is very similar to Shang et al. (2024). From a training method standpoint, the 2-stage fine-tuning approach involving speech recognition and speech summarization is well established in the field since Sharma et al. (2021), leaving only two differences: (a) having an ASR training stage over both short and long audio as opposed to just short audio, and (b) using DPO post hoc, another well established technique to improve ROUGE and METEOR numbers. All in all, it appears that there is little technical novelty in the paper."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper introduces an end-to-end abstractive speech summarization method that utilizes Whisper for speech encoding, a querying-attention Mamba projector to condense extended acoustic features into compact semantic tokens, and Direct Preference Optimization (DPO) fine-tuning to produce coherent summaries. The contribution is somewhat limited, as Mamba-based multimodal LLMs and DPO have both been explored in speech instruction tuning. This work primarily combines these two methods and tests them on the speech summarization task. A full new review and results on more datasets would be needed to make this paper a clear accept given the somewhat limited novel (but good results and relevant task)."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission presents SQUBA, an end-to-end speech summarization model that combines a Mamba-based LLM with a query-attention projector for efficient handling of long speech inputs. While the integration of Mamba layers in both the LLM and the projector yields faster inference and competitive summarization quality, the core approach closely mirrors existing Q-Former-based modality bridging, with the main difference being the architectural substitution of Mamba for Transformer layers. The use of DPO for alignment is a straightforward adaptation of established techniques, and the claimed novelty of the Q-Mamba projector is likely overstated, as it does not introduce a fundamentally new compression paradigm. The empirical improvements are attributable to known properties of Mamba and token compression, rather than a novel algorithmic insight. The submission would benefit from more thorough ablation studies, direct comparisons to recent projector designs (e.g., DeCo), and a clearer articulation of its technical contributions relative to closely related work."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents SQuBa (Speech Querying Mamba Architecture), an end-to-end model for efficient speech summarization that leverages a querying-attention Mamba projector to condense extended acoustic features into compact semantic tokens, subsequently summarized by the Mamba Large Language Model (LLM). The proposed method is novel, using the Mamba architecture and querying attention projector to improve the efficiency of speech summarization."
      }
    ]
  },
  "ueeqGvQozB": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper focuses on developing a benchmark ML4MILP to evaluate machine learning (ML) based methods for solving mixed-integer linear programs of MILPs. I have questions about how this benchmark compares and contrasts against the benchmark created for the ML4CO Neurips competition (Gasse et al., 2022), and how this competition benchmark (and the existence of MIPlib) affect the \"first open-source benchmark dataset\" claim in line 74."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper introduces ML4MILP, an original benchmark collection targeted to the MILP domain, aiming to fill the gap of having a large scale, standardized benchmark for evaluating mixed integer linear program solvers. The benchmark is relevant for the ML and DL community, especially as ML-based approaches are becoming popular in this field. The introduction and related work provide a well-written, self-contained overview of existing solvers, techniques, and related benchmarks, highlighting the general lack of a larger standardized benchmark for MILPs. Based on structural and embedding distance between instances, the authors show that existing benchmarks are often more heterogeneous than the MILP benchmark they propose, which allows for some insight into the structure of the proposed and existing benchmarks. However, the overall contribution—introduction of the benchmark and showing the lack of homogeneity of existing classes of instances based on similarity evaluation metrics with respect to structure and neural embedding distances—is somewhat limited in scientific insight and currently results in me having a hard time voting for acceptance at a main ML and DL conference such as ICLR; maybe submitting the paper to a Benchmark and Dataset track of a suitable conference might be more targeted."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission introduces ML4MILP, a large-scale, open-source MILP benchmark featuring over 100,000 instances and a novel, embedding-based instance classification scheme that combines graph statistics and GNN-derived embeddings. Its main methodological advance is the use of GNN-based embeddings for fine-grained instance categorization, which, while new for MILP datasets, is an incremental extension of existing clustering and representation learning techniques. The dataset’s scale and the embedding-based classification provide more nuanced groupings than prior benchmarks (e.g., Distributional MIPLIB, ML4CO), though the underlying problem diversity remains similar. Some claims of “first” or uniquely fine-grained classification are somewhat overstated, as prior datasets do offer domain and hardness labels, and the provision of high-quality solutions and benchmarking protocols largely follow established practice. Overall, the submission’s primary contribution is infrastructural—advancing benchmark scale and classification methodology—rather than introducing fundamentally new algorithmic concepts."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a new benchmarking framework for solving mixed integer linear programs (MILPs) using machine learning (ML). The proposed ML4MILP framework is a significant contribution that can advance the development of ML-based methods for solving MILPs. The proposed framework is very comprehensive, including a large-scale dataset, appropriate similarity evaluation metrics, a reclassification algorithm, and a baseline library, ensuring that the framework can be used to evaluate a wide range of methods, from traditional optimization solvers to the latest ML-based approaches. The dataset contains instances from both real-world problems and synthetic instances, ensuring that the methods evaluated on it are robust and can be applied to a variety of practical scenarios."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper proposes a benchmark for evaluating machine learning-based mixed-integer linear programming solvers, consisting of a dataset of 100,000 MILP instances classified into 70 classes, and provides a baseline of different algorithms on the dataset. While the dataset is significantly larger than previous ones, I find that the proposed dataset is not significantly different from the existing ones, and the authors do not provide a clear motivation for why the existing benchmarks are not sufficient. The authors claim that existing benchmarks are not homogeneous, but they do not provide a clear definition of homogeneity in the context of MILP instances, nor sufficient evidence that the proposed dataset is indeed more homogeneous than the existing ones. The argument that the similarity between instances in the same class is higher than in existing benchmarks is not convincing without a clear explanation of how similarity is measured and why it matters. Although the proposed dataset’s larger size is emphasized, the authors do not provide a clear justification for why a larger dataset is necessarily better. Overall, the novelty and significance of the contribution are unclear due to a lack of rigorous comparisons to prior benchmarks, definitions of claimed improvements, and justifications for the benefits claimed."
      }
    ]
  },
  "Ipe4fMCBXk": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents a flow matching framework called Recombination Flow Matching (RFM) which performs flow matching over protein residue frames to achieve recombination of natural protein segments to form novel proteins. The novelty of this algorithm is limited, as the architecture of the framework is largely borrowed from previous protein flow matching papers, including protein representations and the setup of training. The addition of recombination loss is marginally novel. As evaluated with scTM and novelty, the authors claim that RFM achieves SOTA performance on novel protein generation."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper presents a new method for protein design, Recombination Flow Matching (RFM), which is claimed to be the first to leverage the critical biological phenomenon of recombination for protein design by transforming larger fragments of protein backbones with rotations and translations to design novel protein structures. However, I find that the novelty claim falls short: while recombination is indeed important in evolution at the genome level, there is little evidence that it is a major driver of diversification at the protein level, and the authors do not convincingly establish that recombination at the protein level is a significant evolutionary mechanism. Furthermore, the manuscript conflates the definition of \"recombination\" used by protein engineers—using fragments from existing proteins to design new ones—with the evolutionary biology definition, and the method aligns more with the former, which is a well-established strategy with numerous prior works such as Bedbrook et al. 2017/2019, Romero et al. 2012, and Voigt et al. 2002. Therefore, the claim that RFM is the first model to leverage recombination in protein design is not accurate from a protein engineering perspective, as recombination-based design methods have been used for decades. Additionally, the evaluation metrics used to support claims of novelty are insufficient, as recombination-based methods inherently recycle domains from existing proteins, making it unlikely that substantially different functions will emerge, and the metrics employed may not adequately demonstrate true novelty in the generated protein structures."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper proposes a method whose novelty is in question, as one reviewer raised concerns about the novelty of this work. Additionally, the 6th reviewer, who had a positive recommendation, indicated very low confidence in their review due to a lack of context in the prior literature, further highlighting uncertainty regarding the originality and contribution of the paper."
      },
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper introduces the Recombination Flow Matching (RFM) model, which introduces recombinations of protein segments to the traditional flow matching model with the aim of generating more novel structures by incorporating recombination. The idea of applying biological recombination principles in protein design is innovative, drawing parallels with natural evolution and differing from other generative model methods that emphasize individual residues or entire protein backbones. The experiments indicate a performance advantage over existing methods in novelty, and the approach has potential for great impact as it addresses the problem of protein structure discovery with downstream applications in fields like drug discovery and biotechnology."
      },
      {
        "id": "human_review_5",
        "type": "human",
        "label": "Human Review (review_5)",
        "content": "This paper proposes a novel generative model known as the Recombination Flow Matching (RFM) Model for novel protein design. RFM uses the concept of recombination from biology to create novel proteins by combining selected segments from other proteins, and utilizes rigid body dynamics to optimize the positions of segments within the resultant protein structure. As evaluations, the authors use a dataset of protein structures to experiment with the effectiveness of RFM in comparison to prior literature, and RFM outperforms baseline algorithms on both the recombination success rate as well as novelty metrics."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission introduces Recombination Flow Matching (RFM), a generative model for protein structure design that explicitly incorporates the recombination of large protein segments, distinguishing itself from prior flow/diffusion-based models that focus on motif insertion or inpainting. RFM’s main claimed contributions are: (1) being the first to operationalize recombination as a generative operation in flow/diffusion models, (2) preserving the structural integrity of recombined segments with automated spatial arrangement, and (3) generalizing to multi-protein recombination. While the model’s explicit recombination framing and multi-segment capability represent a substantive extension within the flow/diffusion paradigm, similar capabilities (e.g., multi-motif scaffolding, automated arrangement) are already present in recent models like FADiff and FrameFlow, making the technical novelty more incremental than transformative. The authors’ characterization of prior work is generally accurate but sometimes overstates the novelty of recombination and omits relevant ML-guided recombination and latent space evolution approaches from the broader literature. Overall, RFM’s main advance is the explicit modeling of segment-level recombination in a generative framework, but reviewers should be aware that the distinction from existing motif/segment scaffolding models is partly terminological and that the broader context of evolutionary-inspired computational methods is not fully addressed."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces the recombination flow matching model (RFM), a generative model that combines protein segments from different proteins to create novel structures. This paper proposes a new generative model that combines protein segments to create novel structures and explores the generalizability of the proposed model to recombining different numbers of proteins and generating proteins of varying lengths. However, the paper does not compare the proposed model with existing methods for generating novel protein structures, such as those based on language models or other generative models, making it difficult to assess the originality or significance of the contribution."
      }
    ]
  },
  "zXCnIyX9MG": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper proposes an autoencoder-based framework for finding shared and private subspaces of multimodal neural and behavioral data, aiming to separate the shared subspace from the modality-specific subspaces by adding Cauchy-Schwarz divergence constraints on the distribution of subspaces. Methodological novelty seems minimal; the authors note “a novel regularization term designed to identify features common to both behavior and neural activity” as their main methodological novelty, but a very similar regularization scheme has been previously proposed by Yi et al. (2022), and the difference between Shared-AE and this work is not adequately discussed, making the methodological novelty unclear. The idea of using CS-divergence instead of standard VAE with KL-divergence is not novel either, as previously proposed by Tran et al. (2021). There are numerous methods on neural-behavioral modeling and finding shared vs. private subspaces, none of which are compared to and many which are not discussed, including closely related works such as Gondur et al. (2024), Hurwitz et al. (2021), Sani et al. (2024), and Zhou and Wei (2020), making it difficult to assess the uniqueness or advantage of the proposed method. The effect of the novel terms in the loss, i.e., the CS-divergence and their inverses, are not assessed, even though this is the main addition to a standard multimodal AE architecture in this work, and it is crucial to evaluate whether the presence of each term contributes to the results or not. The authors claim that their framework is better for more complex/social behavior types than Schneider et al. (2023), but this claim does not seem convincing without further comparisons to other neural-behavioral models."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper introduces a latent-space disentangling autoencoder to identify shared and private latent features from multi-modal neural and behavioral data, with disentanglement based on a Cauchy-Schwarz divergence based regularizer applied between different components of the latent representations. The overall idea of disentangling the latent representation space using inter- and intra-modality loss regularizers has been previously explored in several works, and there are actually works proposing a similar autoencoder regularization framework in other settings [Tran et al. \"Cauchy–Schwarz Regularized Autoencoder\", JMLR 2022]. Perhaps one question that the authors should clarify with a clear statement is their methodological ML novelty (i.e., if the proposed regularized training scheme is completely novel, or if the paper only contains a strong empirical novelty). It has a unique empirical strength with a focus on neural data analysis from complex multi-modal social behavior experiments."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper contributes to a growing literature on learning shared representations for multimodal data in neuroscience by further partitioning each modality's latent space into private and shared latent spaces, with the shared spaces linearly mixed across modalities and separated using a Cauchy-Schwarz Divergence. However, it is somewhat unclear what the technical innovation in this paper is beyond the Yi et al. preprint cited, as well as a similar paper by Yi and Saxena at EMBC in 2022, since both of those works use the same CS divergence setup as here, and I am struggling to see where the technical innovation is (though the application is somewhat different). The authors use a latent space partition that is distinct from the Whiteway et al. paper but somewhat related to the Sani et al. work they cite, and while the PSID paper is linear, the Shanechi group also has work on nonlinear methods that preserve this kind of partition. I don't see the strength of the experimental results here being novel or interesting enough on their own to justify acceptance without an additional technical advance, and the technical contribution of the paper is perhaps small when considered in light of other work cited. In all, this is a solid paper that is, in my view, below the bar for acceptance without a more substantial technical contribution."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper introduces a novel \"disentangling autoencoder\" to identify shared and private latent features from multi-modal neural and behavioral data. I am ultimately convinced that the novelty of the theoretical contribution (in particular, the orthogonality of its latent representations and the fact that it can find shared representations between 3 or more modalities), as well as the rigour of the comparisons to previous methods place it above the bar for acceptance."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "Shared-AE is an incremental contribution to the well-established field of multi-modal representation learning, specifically within the cluster of methods that explicitly partition latent spaces into shared and private components. Its primary technical novelty lies in the use of CS divergence (and its inverse) as a regularization strategy for disentangling shared and private subspaces, which is a variation on existing regularization approaches such as orthogonality and mutual information minimization. While the authors claim superior interpretability, disentanglement, and robustness to missing modalities, these problems and solutions have been addressed in several prior works, some of which are not cited or fully acknowledged in the submission. The empirical improvements reported may be attributable to implementation details rather than fundamental conceptual advances, and the characterization of prior work sometimes understates their sophistication. Overall, Shared-AE is a technically sound but incremental advance, and a more balanced comparison to all relevant literature would strengthen the submission."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a method for disentangling the shared and private latent features of multi-modal data. While the method is simple and the author provides a clear motivation for proposing it, the method is not compared with state-of-the-art methods in the field, and I found a few recent methods that are highly related to this paper (e.g., [1,2]). Including these in the comparisons would help clarify the contribution. The method is relatively new, but further evaluation and comparison are needed to better establish its originality and significance."
      }
    ]
  },
  "EUAxxrxOM8": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "The paper introduces a novel application of MPC to the RMAB problem which achieves a suboptimal gap $O(1/\\sqrt{N})$ with a minimal set of assumptions and exponential convergence rate under local stability conditions. This paper presents an interesting framework based on dissipativity and provides theoretical analysis. However, I suggest highlighting the novelty and primary contributions of the theoretical analysis more clearly, and clarifying what are the main technical contributions compared to existing works."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper presents a novel use of dissipativity for analyzing RMABs, which offers fresh insights into this field, and proposes a model predictive control (MPC) policy using a rolling computational horizon of length $\\tau$ that achieves a suboptimality gap of order $\\mathcal{O}(1/\\sqrt{N})$. However, my main concern is that the order of $\\mathcal{O}(1/\\sqrt{N})$ gap has been a well-known result for a long time, which various types of algorithms, including LP-based algorithms in many related works cited by this submission and without an additional MPC layer, can also achieve. Though controlling $\\tau$ is new, resolving the LP at each time step is not really a novel idea and can be found in multiple works in related work cited by this submission. Furthermore, a new work (https://arxiv.org/pdf/2410.15003) has recently pushed the gap to the order of $\\mathcal{O}(1/N)$ by using a diffusion approximation technique, so the proposed MPC-based algorithm does not improve the well-known optimality gap, which is claimed as a main contribution in this work."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents an approach with somewhat incremental novelty and faces computational scalability issues."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper studies the discrete time infinite horizon average reward restless Markovian bandit (RMAB) problem and proposes a model predictive control (MPC) based non-stationary policy with a rolling computational horizon, whose sub-optimality gap is analyzed and performance evaluated via simulations. However, it is hard to identify the technical novelty from the perspectives of algorithm design and technical proofs and results, as this paper heavily relies on previous works such as Gast et al. 2023a,b. The LP based relaxation and randomized rounding procedure are almost the same as those in Gast et al. 2023a, and a finite-horizon MPC algorithm (LP-update policy) was already proposed in Gast et al. 2023a,b, making this work more like a straightforward extension. The first result in Section 4.1 is not surprising and is commonly known in the RMAB literature, with LP-based methods for RMAB being provably asymptotically optimal as shown in prior work, e.g., Verloop 2016. The second result in Section 4.2 has a proof that is directly from Gast et al. 2023a and Hong et al. 2024a. Overall, despite leveraging the MPC idea, the paper's contributions appear incremental and lack clear technical novelty compared to existing literature."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission addresses the average-reward Restless Markovian Bandit (RMAB) problem with resource constraints, proposing a Model Predictive Control (MPC)/LP-update policy analyzed using dissipativity theory—a novel proof technique in this context. While the rolling horizon/LP-update policy structure is not new and closely follows prior work (e.g., Gast et al. 2023a, Ghosh et al. 2022), the use of dissipativity for establishing asymptotic optimality under weak, verifiable assumptions is a substantive technical innovation. The authors claim to avoid hard-to-verify assumptions like indexability or UGAP, aligning with a broader field trend toward scalable, assumption-light methods, though the ease of verifying dissipativity may be somewhat overstated. Empirical results and claims of practical superiority are credible but not uniquely attributable to the conceptual advance, as similar policies have shown strong performance in prior work. Overall, the main contribution is a new analytical approach (dissipativity) for a well-studied policy class, with some overstatement of novelty regarding assumptions and policy structure; reviewers should focus on the technical depth and generality of the dissipativity-based analysis."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper considers a new angle to study the RMAB problem, i.e., the dissipativity framework. However, the motivation for using the dissipativity framework is unclear, and it is not evident what the advantages are compared to other frameworks in studying the RMAB problem. The main contributions are not clear, and I am unsure about the main challenges and the main novelty in the proof technique in this paper. The literature review is not comprehensive, and relevant prior works are missing, making it difficult to assess the originality and significance of the proposed approach."
      }
    ]
  },
  "W5S1DEjN8x": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes Practical ε-Exploring Thompson Sampling (PETS), which applies Thompson Sampling to continuous control tasks by leveraging Langevin Monte Carlo for approximate posterior sampling and maintaining ensembles of parallel Markov chains to reduce sample correlation and improve exploration diversity. However, the method closely mirrors ensemble sampling techniques, essentially amounting to maintaining an ensemble of models and uniformly sampling one at each decision point, which is conceptually similar to existing ensemble sampling methods used to approximate Thompson Sampling. The approach shows strong parallels to ensemble-based methods such as \"Ensemble Sampling\" by Lu & Van Roy and \"Ensemble Langevin DQN\" by Dwaracherla & Van Roy, where ensembles are used with Langevin dynamics for exploration, but the paper does not acknowledge or discuss these similarities, missing an opportunity to position PETS within the broader context of ensemble-based RL methods. The omission of relevant literature on ensemble methods and randomized value functions, including works like Bootstrapped DQN and randomized value functions via multiplicative normalizing flows, further limits the paper's positioning within the existing body of research and undermines its claim of novelty. Without proper acknowledgment or differentiation from these established approaches, the paper's originality is diminished, and its impact is limited by the lack of clear articulation of novel contributions compared to prior work."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper discusses how to apply Langevin Monte Carlo (LMC) based Thompson sampling algorithm for continuous action space RL tasks. The work is nice in the sense that it extends LMC-LSVI from Ishfaq et al 2024a to continuous action setting, which is an important problem setting, and it also shows the efficacy of the method through some thorough experiment. The proposal of parallel posterior sampling is a promising way to mitigate the issue of correlated sampling in vanilla LMC. However, my main complaint for this work is that it doesn’t situate the contribution of the paper relative to other highly related papers such as Ishfaq et al 2024a and Ishfaq et al 2024b. In line 70-74, it introduces the idea of using LMC for TS as if it’s the first work to do so. Proposition C.1, Definition C.2, Lemma C.5, Lemma C.6, Lemma C.7, Lemma C.8, Lemma C.9 all are essentially verbatim to different lemmas used in the proof of Ishfaq et al 2024a. With this respect the novelty of the algorithmic contribution and theoretical analysis are very limited in this paper. Practical $\\epsilon$ Exploring Thompson Sampling (PETS) directly borrows ideas/methods from two existing works $\\epsilon$-TS [Jin et al 2023] and LMC-LSVI of Ishfaq et al 2024a. As such the novelty in terms of algorithmic design seems limited. Despite borrowing significant chunk of Ishfaq 2024a for algorithm design, the paper fails to properly acknowledge it in their method section, especially in Section 3.1."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The authors propose a TS-based exploration technique for RL with continuous action spaces, using Langevin Monte Carlo (LMC) to sample from the posterior of the action-value function and approximate the optimal action through both gradient-based and gradient-free methods. My main concern is the technical novelty. The proof of Theorem 1 seems very similar to the analysis in Ishfaq et al., 2024. What is the main technical challenge or novelty in this work compared to Ishfaq et al., 2024, particularly in proving Theorem 1? A detailed discussion of these technical challenges in the main paper would be very helpful. Since the proposed method uses parallel models, it relates closely to ensemble-based algorithms, such as Bootstrapped DQN (Osband et al., 2016) and recent work on linear ensembles for bandits (Janz et al., 2024), and comparing your approach with these ensemble-based algorithms would enhance the paper. I believe this type of work will make a significant contribution to the community."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission presents a technically solid and practical extension of Langevin Monte Carlo Thompson Sampling (LMC-TS) from discrete to continuous control reinforcement learning, with the main innovations being the use of parallel Markov chains for posterior sampling and empirical integration with standard RL algorithms. The theoretical regret analysis for continuous controls is a meaningful, though incremental, adaptation of prior work (notably Ishfaq et al., 2024), closely following existing proof structures. While the practical implementation details (e.g., parallel chains) may improve exploration diversity and performance, these are engineering adaptations rather than conceptual breakthroughs, and similar diversity-promoting strategies exist in the literature (e.g., ensembles, bootstrapping). The submission somewhat overstates its novelty by not citing or comparing to recent neural/ensemble-based TS methods that also address high-dimensional and continuous RL settings. Overall, the work is a valuable and well-executed incremental advance, but its claims of being \"first\" and uniquely novel should be moderated to reflect the broader context of related research."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a practical approach for implementing Thompson sampling in continuous control problems by using Langevin Monte Carlo (LMC) for sampling to address the intractability of the posterior over the parameters of the action-value function and maintaining $n$ parallel Markov chains to mitigate issues with naive LMC application. However, the paper's novelty is limited. The primary contribution is the use of Langevin dynamics to sample parameters, which is a straightforward application of existing methods. Additionally, maintaining multiple parallel Markov chains to reduce correlation is a common technique in posterior sampling. Furthermore, using gradient-based or gradient-free methods to approximate the optimal action is also not a novel contribution, as it is a standard approach in many fields."
      }
    ]
  },
  "2ofVtMvRil": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "The authors investigate how a temporally-dependent version of predictive coding can extract compact latent spaces in the form of periodic grid activity from temporally structured place cell input. Overall, the study seems like an incremental follow on of the tPCN paper applied to a new domain, but which does not require fundamental changes to the original algorithm. Several existing studies, largely cited in the paper, already investigate the formation of such successor representations by predictive coding. My primary concerns center around the novelty of the algorithm beyond tPCN itself; simply applying a non-negative constraint and applying to a new task does not seem like a sufficiently novel contribution for ICLR."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This study demonstrates that predictive coding can effectively train neural networks to develop hexagonal grid representations from spatial inputs, providing a biologically plausible explanation for the emergence of grid cells in the medial entorhinal cortex. However, my major concern is that the work may lack novelty. The use of non-negative and sparse network designs to produce grid cell-like patterns has been extensively discussed, with prior work reporting that non-negative and sparse properties can generate grid cell-like patterns and theoretically demonstrating why non-negativity is the main driver of grid cell formation, which the author's paper does not address, instead of sparsity. Similar findings have also been reported elsewhere, and earlier work proves that a nonnegativity constraint on firing rates induces a symmetry-breaking mechanism favoring hexagonal firing fields, with further studies exploring the necessary conditions for generating grid cells. Prediction tasks, including path integration, that produce grid cell-like patterns have also been widely reported, especially when the input data takes a place cell-like form, and other studies have used place cell-like input and path integration tasks to train networks and generate grid cells, while some have theoretically analyzed the role of predictive learning in forming low-dimensional representations. In my understanding, tPCN is very similar to a one-step RNN (apart from the difference in local learning rules), so the fact that its training process resembles that of one-step tBPTT is not surprising; as previously noted, the key to forming grid cells lies in the predictive task, not the RNN network itself, and therefore, the similarity between tPCN and RNN does not offer significant insight into the generation of grid cells. For these reasons, I believe this paper does not offer substantial novelty or make a clear contribution to the field."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission extends predictive coding networks (PCNs), previously used for sensory and memory tasks, to the spatial navigation domain, demonstrating for the first time that grid cells can emerge in PCNs/tPCNs with local, Hebbian learning rules. The main novelty lies in combining predictive coding, spatial navigation, and biologically plausible learning, and in analytically linking tPCN Hebbian learning to truncated BPTT—a result not previously shown for temporal/spatial tasks. While the emergence of grid cells from place cell inputs with non-negativity is well established in other frameworks (e.g., sparse coding, RNNs), the predictive coding implementation and its robustness to architecture/input are incremental but useful advances. The authors’ claims of novelty are accurate for predictive coding networks specifically, but somewhat overstate the lack of biological plausibility in prior grid cell models. Overall, the work represents a substantive but evolutionary step, extending predictive coding theory to a new domain and providing a new normative link to established learning algorithms."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper makes a novel connection between predictive coding networks and grid cells by showing that predictive coding networks can extract grid cell representations from place cell inputs and that temporal predictive coding networks can learn to integrate velocity information to learn grid cell representations. However, there are many models that can learn grid cells, including attractor network models and other predictive models, so it is not clear why predictive coding networks are particularly suited for this task, and the paper does not provide a strong motivation for this connection. While the paper briefly mentions some existing models, it does not provide a comprehensive comparison to them, and a more detailed comparison—including an analysis of the computational requirements and biological plausibility of different models—would help in evaluating the originality and significance of the contribution."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper introduces a novel approach to modeling grid cell formation in the medial entorhinal cortex using predictive coding networks (PCNs). I find the use of predictive coding to model grid cell emergence to be a novel and compelling approach, offering a biologically plausible alternative to models that rely on backpropagation, which is unlikely to be implemented in the brain. The paper’s main contribution lies in its novel application of predictive coding to model grid cell emergence, providing a unified learning algorithm for diverse cortical representations. The authors successfully demonstrate that grid cells can arise as latent representations learned through predictive coding in both static and dynamic environments—a significant contribution to the field—and show that the learning rule of temporal predictive coding networks (tPCNs) implicitly approximates truncated backpropagation through time (BPTT). They compare their model with recurrent neural networks trained with BPTT, showing that both models can achieve similar performance in terms of path integration and grid score, while emphasizing the biological plausibility of their approach. However, I note several weaknesses that limit the clarity of the paper’s originality: the work lacks a detailed comparison with prior models, particularly recurrent neural networks and attractor networks, making it difficult to fully appreciate the unique contributions of the proposed model. While the authors mention advantages such as not requiring biologically implausible backpropagation and addressing the need for local learning rules, they do not provide an in-depth analysis of how their model mechanistically diverges from or improves upon existing approaches. This leaves the reader with an incomplete understanding of how the PCN model advances the field beyond existing models, and undermines the assessment of its true novelty and significance. Overall, I believe the paper makes a significant contribution by presenting a novel and biologically plausible model for grid cell formation, but the originality would be better established with a more thorough and mechanism-level comparison to prior work."
      }
    ]
  },
  "Yq8At31hLi": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper introduces SMI-TED, an encoder-decoder Transformer-based model for learning representations of molecules that can be used for downstream tasks such as reaction yield prediction. The approach seems similar to MolFormer (Ross et al., 2022): pre-training of a Transformer model using a BERT-like approach for downstream adaptation for various molecular regression/classification tasks. The main differences seem to be the additional decoder (and reconstruction loss) and the ensembling of different representations; however, the ensembling does not actually seem to be used for many of the tasks."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes SMI-TED, a transformer-based foundation model for property predictions and reaction yields, pretrained on a large dataset of 91 million molecules using SMILES representations. My major concern about the paper is the lack of novelty. The core of the paper is a standard transformer architecture with very limited modifications for molecular data, and this choice of a transformer-based encoder-decoder setup for SMILES has been extensively applied to similar tasks. I did not notice clear statements or claims on substantial changes or improvements in the network architecture or the input representation. Since the technical novelty is minor, I believe most performance improvement comes from using a large dataset, and I did not spot any significant challenge in curating such a dataset since all data is publicly available at PubChem. The experimental results only show limited improvements over baseline methods on standard benchmarks, and for MoleculeNet, the performance is very close to and sometimes even worse than state-of-the-art methods such as UniMol, whose results are missing for regression tasks. Additionally, since these methods are not trained on such a large dataset, I think they will surpass the proposed method if they were trained on the same data scale."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces SMI-TED (SMILE Transformer Encoder Decoder), a foundation model for molecular property prediction and generation, which is an encoder-decoder architecture designed to create unified molecular representations by mapping SMILES strings into a fixed-dimensional latent space for both property prediction and molecule generation. The proposed methodology of this paper is extremely limited in its novelty. Many of the components of the model, such as a transformer trained on SMILES strings, BERT-style pre-training, utilization of sparsely-gated mixture-of-experts to scale the model, and the general encoder-decoder architecture, have been explored in previous works. The only methodological novelties, to the best of my knowledge, are the specific details behind the encoder-decoder training methodology used to extract a fixed-dimensional latent space for the entire SMILES token sequence, and the two-phase pre-training strategy combining masked language modeling and encoder-decoder training. These methodological novelties are quite minor and do not significantly advance the state-of-the-art in molecular property prediction or generation. For the two methodological novelties mentioned above, the authors provide limited description and discussion, making it difficult to assess the significance of these contributions, and as it stands, these design choices seem arbitrary."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "**Summary for Reviewer:**\n\nThis assessment finds that SMI-TED introduces the first large-scale encoder-decoder transformer foundation model for chemistry and materials, leveraging Mixture-of-Experts (MoE) scaling and pretraining on 91M SMILES, which distinguishes it architecturally and in scale from prior encoder-only or decoder-only models. While the authors accurately situate their work within the foundation model paradigm and cite relevant literature, they do not critically compare SMI-TED to the most similar recent models (e.g., UniMAP, UAM, Egret), particularly regarding multimodality and uncertainty-aware approaches. The main novelty is architectural (encoder-decoder + MoE at scale), but this is an incremental advance, as both techniques are well-established in NLP and have been applied at smaller scale in chemistry. Claims of state-of-the-art performance, latent space compositionality, and openness are now common in the field, and the empirical improvements may be attributable to increased scale or implementation details rather than conceptual innovation. The submission would be strengthened by more direct comparisons with omitted related work and by clarifying whether observed gains stem from methodological advances or routine scaling."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces SMI-TED, a large-scale foundation model for materials and chemistry, trained on a massive dataset of 91 million SMILES samples (4 billion molecular tokens) from PubChem using self-supervised learning. The encoder-decoder architecture enables a wide range of complex tasks, including the prediction of quantum chemical properties and reaction yields, and the authors also introduce a mixture of experts model to improve performance. The model achieves state-of-the-art results across multiple benchmark datasets, demonstrating its versatility and effectiveness. Notably, the model’s latent space exhibits compositionality and separability, essential properties for higher-level reasoning tasks and few-shot learning capabilities. However, the main weakness of this paper is the lack of comparison with other similar models, as there are a number of large-scale foundation models for molecules that are not cited or compared with in this paper, including MolT5 and MolFormer, which has also been trained on similar data."
      }
    ]
  },
  "vwOq7twk7L": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents a new method for identifying whether an image was part of the diffusion model training set, without access to the prompt used for generation. As far as the authors know, they are the first to identify this task. The authors propose for the first time to experiment with inference with perturbed prompts for the task at hand, and they introduce Inversion-based Inference Perturbation (IIP) as a novel method that out-performs competitors on an extensive test suite. However, regarding the novelty of the task, I note that [1] and [2] engage in membership inference without using prompts, as far as I know, and I ask the authors to clarify how their proposed task of image-level memorization detection differs from membership inference, particularly in light of the cited works that perform membership inference without prompts."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper investigates the memorization limitation of the Stable Diffusion model to help protect the training data’s privacy and proposes a new task: image-level memorization detection, which is different from existing works that detect memorization during inference. This paper further proposes a correspondingly designed method for this new task. However, the motivation for the proposed new image-level memorization detection task is based on a misunderstanding of the related work, as the baseline methods do not need an organized prompt list and can detect potential memorized images during the inference process with good accuracy, making such lines of method actually more practical than the proposed image-level memorization detection task; thus, the proposed task is of limited practical significance. Additionally, the finding that large TCNP correlates with memorization is not novel, as it is actually the identical finding of [1] that a noise can present large TCNP even after the first step of DDIM denoising, although the paper differs in performing DDIM inversion to the clean image."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "The paper introduces a method for image-level memorization detection, motivated by an analysis showing two key characteristics of memorized images under perturbed inference: similarity discrepancy and a large magnitude of text-conditional noise prediction. The key idea of the method seems to be using an unconditional DDIM inversion to derive latent codes and optimizing non-memorized prompt embeddings for effective perturbation."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents a new setting of image-level memorization detection given a user-provided reference image. The authors are encouraged to provide clearer distinctions from related work and a more compelling discussion of the practical significance of the task. Although some margins need to be improved, this paper still stands with its merits."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission addresses the important and challenging problem of image-level memorization detection in text-to-image diffusion models without requiring access to prompts or metadata, situating it as a prompt-free, inversion-based detection method. The work is most closely related to recent prompt-level detection approaches (e.g., Wen et al., Ren et al.), but extends these ideas to the more realistic image-only setting, leveraging unconditional DDIM inversion and inference perturbation. While the technical components—such as inversion, perturbation, and detection metrics—are largely adapted from prior work, the systematic combination and empirical validation for image-level detection represent a meaningful advance. The authors’ claims of being \"first\" are accurate in a narrow, systematic sense, but the conceptual gap from prior work is smaller than implied, as similar metrics and techniques are reused. Overall, the submission’s main contribution is a robust, prompt-free detection framework, with novelty arising from its integration and application of existing methods rather than from fundamentally new concepts."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a method to detect if an image has been memorized by a diffusion model, even without knowing the prompt used to generate it. The problem of detecting memorized images without knowing the prompt is a novel and important problem. The proposed method is simple and effective, and the experiments show that it outperforms the baselines in all settings. The paper lacks a comparison with the method proposed by Wen et al. (2024) for detecting memorized prompts; it would be helpful to include a comparison with this method, both in terms of effectiveness and efficiency, to provide a more comprehensive evaluation of the proposed approach."
      }
    ]
  },
  "QoDDNkx4fP": {
    "human": [
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes a novel framework, ETA (Evaluating Then Aligning), which addresses a critical gap in the safety of Vision Language Models (VLMs) by focusing on inference-time alignment. Unlike existing approaches that primarily rely on extensive fine-tuning or are limited to specific types of inputs, ETA offers a fresh perspective by combining multimodal evaluation and bi-level alignment without requiring additional training. This plug-and-play nature makes it a highly original contribution, providing a more flexible and scalable solution for enhancing VLM safety. The significance of this work is substantial, as it introduces a method that does not require fine-tuning and can be easily integrated into existing systems, making it practical for widespread use. By improving safety while maintaining model efficiency and general capabilities, ETA could encourage broader adoption of VLMs."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes a new safeguard mechanism for the Vision Language Model (VLM) during the inference phase. However, although the simplicity of the proposed methods, the novelty seems significantly limited, as the proposed methods are based on the simple application of existing models such as CLIP and Reward Models."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper investigates the safety aspect of VLMs and presents a two-phase inference-time alignment framework, with the key contribution being a multi-level alignment strategy following an initial safety assessment of visual content and output responses. While the reviewers appreciated the simplicity and effectiveness of the proposed method, they noted that the novelty is somewhat limited. Reviewer tnBM acknowledged that the only remaining concern is the novelty (a point also raised by Reviewer JJRX). After reviewing the authors' rebuttal and clarification on novelty, the AC believes its contribution is sufficiently different from existing works and found its novelty acceptable (though not substantial)."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "ETA proposes a modular, plug-and-play, inference-time safety alignment framework for vision-language models (VLMs), combining explicit multimodal evaluation (visual and textual) with bi-level alignment (interference prefix and best-of-N search). Its main contribution is the systematic integration and empirical validation of these established techniques, rather than introducing fundamentally new algorithms. While ETA demonstrates improved safety and utility over prior methods, its novelty is incremental, as similar modular inference-time defenses (e.g., ECSO, AdaShield, MLLM-Protector) exist, and several relevant recent works are omitted from the comparison. The authors’ claims of unique novelty and prior work limitations are somewhat overstated, though their engineering advance is meaningful. Reviewers should weigh ETA’s practical integration and robust evaluation against the incremental nature of its contribution and the need for more comprehensive contextualization within the rapidly evolving field."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper introduces a novel two-phase framework named ETA to enhance the safety of Vision Language Models (VLMs) at inference time. The proposed method is well-motivated, providing a clear explanation of why the continuous nature of visual token embeddings can bypass LLM safety mechanisms."
      }
    ]
  },
  "2L1OxhQCwS": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "The paper explores the use of Transformer and LSTM-based models for financial time series forecasting tasks using high-frequency limit order book (LOB) data, proposing a new LSTM-based model, DLSTM, alongside a modified Transformer architecture tailored for financial predictions. However, the paper lacks substantial novelty, as the DLSTM model is essentially a combination of existing methods, such as time series decomposition and LSTM layers, without a clear innovation. Similarly, the Transformer modifications are incremental and do not provide a compelling improvement. As a result, the contributions seem incremental and insufficiently distinct from existing work in financial time series forecasting, and the study demonstrates only marginal improvements from the proposed Transformer modifications over traditional LSTMs, which does not convincingly justify their adoption for practical trading applications."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper compares the effectiveness of Transformer and LSTM architectures for financial forecasting using high-frequency trading data, introducing a new LSTM-based model called DLSTM and a finance-specific Transformer. However, there is limited novelty in the proposed approach, as the decomposition strategy is only applied to the LSTM model and several state-of-the-art Transformer-based models are not included in the comparison."
      },
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper conducts a comparative study between LSTM-based and Transformer-based models for financial time series forecasting in electronic trading using high-frequency limit order book data, and introduces a new LSTM-based model, DLSTM, which creatively combines LSTM with a time series decomposition approach inspired by the Autoformer architecture. The study offers a novel perspective on the application of these models in this context, and the innovative integration of existing ideas in DLSTM allows it to outperform other models in the mid-price movement prediction task. The paper's adaptation of existing Transformer-based models' architecture to better suit the demands of the movement prediction task also showcases the potential for further improvements in this domain. Overall, the paper's originality and the creative approach to model design make it a valuable contribution to the field of financial time series forecasting using deep learning models, offering new insights and directions for future research."
      },
      {
        "id": "human_review_5",
        "type": "human",
        "label": "Human Review (review_5)",
        "content": "This research compares the effectiveness of Transformer and LSTM architectures in financial forecasting and introduces DLSTM and a finance-specific Transformer. The novelty of the proposed approach is limited. While the authors introduce a DLSTM model to improve performance, the idea of decomposition was previously explored in models like DLinear [1], diminishing the originality of the contribution. Beyond the comparative analysis, additional innovation is also limited."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission presents an incremental advance in the comparative evaluation of LSTM and Transformer models for high-frequency financial time series forecasting using limit order book (LOB) data. Its main technical contribution is the introduction of a DLSTM model that integrates time series decomposition directly into the LSTM architecture, though similar ideas have appeared in hybrid models and the novelty is moderate. The modified Transformer proposed for financial movement prediction lacks sufficient detail to assess its distinctiveness, and similar adaptations are already common in the literature. The comparative analysis is thorough and uses multiple metrics, but such studies are routine in this field and the work does not benchmark against the most recent or directly relevant LOB-specific models. Overall, while the submission is well-executed and methodologically sound, its claims of novelty are somewhat overstated and it would benefit from deeper engagement with the latest related work and a more measured framing of its contributions."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper compares the performance of Transformer and LSTM-based models on financial time series forecasting tasks using high-frequency limit order book data, introducing a new LSTM-based model (DLSTM) and a modified Transformer architecture for financial predictions. However, the paper lacks novelty as it does not introduce a new dataset, and the proposed model is a simple combination of existing methods. The authors should provide more details on how their approach differs from previous work and what novel insights it adds to the field."
      }
    ]
  },
  "Zdl2i7RKmz": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This article presents a method for solving MARL tasks by inserting vectorized observations from the environment into an LLM prompt template and obtaining decisions from LLM mediators in two different ways (RB and NL), aiming to leverage the reasoning and planning abilities of LLMs to assist in completing the fire extinguishing task in Aerial Wildfire Suppression. This article claims that the novelty lies in using LLM as a mediator to intervene between the action policy and the environment, dynamically, especially in the Natural-language intervention method where prompt-encoded environment features are fed into a Strategy LLM and then the natural language outputs are fed into the LLM-Mediator for real-time action policy. However, what this article claims as novelty is not quite robust. The novelty from my side is: translating prompt to natural language strategy and then translating it back to formatted expression. Embedding the environment information into LLM prompt has been studied by a lot of works in different ways, and simply applying this idea to multi-agent environment, as one of the \"centralized training and execution (CTE)\" methods, does not contribute to this area."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents an empirical study investigating how LLMs can be used as a controller to intervene in the learning process of MARL, with the main contributions being the evaluation of two representations of prompts: a rule-based controller and a natural language controller. From the perspective of an empirical and engineering work, this paper seems like an original work, as I cannot find duplicated counterparts online. Although the paradigm of intervening in the MARL learning process is not a completely new idea, the leverage of LLMs to produce the interventions should be novel. The trial made in this paper well shedded light on the possibility of this combination, which is thrilled to MARL researchers such as myself. In this sense, I believe this paper studied significant work."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "In this work, the authors propose introducing LLM-mediated interventions to support multi-agent reinforcement learning (MARL) systems, specifically through two separate strategies (rule based and natural language) designed to map linguistic specifications to agent instructions, and present a novel Aerial Wildfire Suppression (AWS) MARL environment to benchmark their approaches. My primary concern with this work is that the novelty of the proposed framework is unclear. The authors claim that no existing systems support the use of LLMs for MA learning systems; however, recent work has studied this problem (see Sun & Huang, 2024 and references therein for a recent overview). Additionally, while the authors claim that the proposed AWS environment has more complex environmental dynamics than prior work, it is unclear how the proposed benchmark differs substantively from existing simulation environments such as Minecraft (e.g., Wang et al., 2023). Further, the rule-based and natural language controller implementations, while useful points of comparison, appear to be engineering system implementation efforts as opposed to research contributions. The authors also mention this in the statement of contributions, explaining “We implement a novel *system*.”"
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents an interesting approach to incorporating LLM-mediated interventions in multi-agent reinforcement learning systems, with the primary technical contribution centering on using LLMs as mediators to guide MARL agents through rule-based and natural language controllers. While some reviewers acknowledged the potential value of this direction, particularly for future human-AI interaction scenarios, the core idea is interesting but the execution and validation need substantial strengthening. The work relies solely on PPO as the base MARL algorithm without comparisons to more sophisticated state-of-the-art CTDE-based approaches like QMIX, VDN, or MADDPG, making it difficult to assess the true value and originality of the proposed LLM-mediated guidance system. Furthermore, the evaluation is conducted exclusively on a new Aerial Wildfire Suppression environment, raising questions about the method's generalizability and significance compared to established MARL benchmarks. The divergence in reviewer opinions reflects the paper's mixed contributions, as the technical depth, experimental rigor, and comparative evaluation are not sufficient to clearly establish the uniqueness or significance of the approach."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment situates the submission within the emerging area of LLM-mediated interventions in multi-agent reinforcement learning (MARL), highlighting its main novelty as enabling real-time, adaptive, user-driven natural language interventions during training—an incremental but distinct advance over prior static or episodic LLM-based reward shaping and planning. The authors’ characterizations of related work are generally accurate, though they somewhat overstate the static nature of previous approaches to emphasize their own adaptivity; the true novelty lies in the temporal granularity and system integration of both rule-based and NL controllers for real-time intervention. The empirical validation in a complex, realistic AWS environment strengthens the demonstration, though this is an application-level rather than methodological contribution. The assessment notes that while the interface and system integration are new, the underlying principle of human-in-the-loop or adaptive intervention is well established, making the advance primarily one of interface and flexibility rather than core algorithmic innovation. Overall, the submission represents a meaningful but incremental step forward, with its strongest differentiation in real-time, user-driven NL mediation and system integration, and its weakest in fundamental learning principles or algorithmic novelty."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents an approach for guiding multi-agent reinforcement learning (MARL) systems using Large Language Models (LLMs) as a mediator, proposing an LLM-based intervention mechanism that temporarily overrides agents' learned policies to improve coordination and performance. The approach of using LLMs as a mediator for MARL is novel; however, the proposed method is not compared against any relevant baselines, specifically other methods that use LLMs for guiding RL agents, such as LLM-Tamer, LLM-Planner, LLM-GI, or recent surveys in the area."
      }
    ]
  },
  "6akuzEqP38": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper introduces an interesting method called \"Articulated Anything\" to address the problem of articulated object generation. While the method is reasonable, it essentially relies on the power of various large models and diffusion models, which may limit the novelty of the proposed framework. The performance of the proposed method is not particularly impressive, and it is difficult to observe a significant improvement compared to existing methods, such as CAGE."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes a novel pipeline that enables the creation of articulated objects from arbitrary input mesh, addressing a critical research gap in 3D generation for articulated objects and contributing to an increasingly important area. The main contribution is this pipeline that enables the generation of diverse articulated objects by taking arbitrary 3D mesh as input."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper proposes a pipeline to take a 3D mesh and produce an articulated version of the mesh through three stages: movable part segmentation, articulation estimation, and refinement, combining recent advances with prompting GPT4o. There is a lack of comparison with prior work and limited ablations, raising concerns about whether comparing articulation parameter estimation against generative models such as NAP and CAGE is appropriate, as it seems more appropriate to compare against methods that aim to predict articulation parameters given an input rather than generating different distributions of articulation parameters. The reviewers and AC share concerns about whether appropriate evaluation and comparisons were performed to understand the performance and limitations of the proposed approach, leading to doubts about the significance and readiness of the work for publication."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission \"ARTICULATE ANYTHING\" presents a novel pipeline that integrates open-vocabulary segmentation, LLM-based articulation estimation, and diffusion-based generative refinement to convert any rigid 3D mesh into an articulated object, distinguishing itself from prior works that address only subsets of this problem or are limited to closed-set categories. The main technical advances include the use of GPT-4o for direct joint parameter estimation from geometry and language, and a diffusion-based optimization strategy with random part transformations to preserve part semantics. While the integration of these components into a single, open-vocabulary, category-agnostic pipeline is a substantive contribution, the individual elements (segmentation, LLM reasoning, diffusion generation) are incremental extensions of existing methods, and some claims regarding the limitations of prior work are somewhat overstated. The authors' characterization of their novelty is generally accurate for the full pipeline, but less so for individual components, and the omission of some recent related works (e.g., OpenObj, Kinematic-aware Prompting) leaves the comparison incomplete. Overall, the work represents a significant step forward in open-vocabulary articulated object modeling, though its impact is primarily in the integration and scaling of recent advances rather than in fundamental algorithmic breakthroughs."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a pipeline to convert static meshes into articulated objects by first segmenting the object into parts using existing methods (Part123), estimating the joint parameters by prompting GPT-4o, and refining the model using a 2D diffusion model to recover the missing geometry. The main contribution of this work is the engineering effort to prompt GPT-4o for estimating joint parameters and recovering missing geometry using a 2D diffusion model. However, the proposed method heavily relies on the 3D segmentation method Part123 and the vision-language model GPT-4o, and the authors did not compare their method with other methods that predict joint parameters, such as ANCSH and OPD, or with other methods for recovering missing geometry, such as completing the contour, which is a common technique used in computer vision."
      }
    ]
  },
  "ayupWYA1qD": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes a novel Transformer-based forecasting model tailored to observability, with key contributions including a novel transformer block integrating one channel-mixing attention layer followed by multiple blocks of time-wise attention, and an output head employing a mixture of Student-t distributions. The design of proportional factorized space-time attention potentially offers an expressive and efficient backbone for multivariate time series modeling. However, the technical contributions of space-time attention and Student-t mixture heads are of limited novelty and provide marginal improvement. If domain-specific training data is a contribution, details are needed on the pre-training corpus construction, ablation studies, and novelty of data selection."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper introduces Toto, a foundation model designed for time series forecasting with a specific emphasis on observability metrics. The paper’s two primary contributions are the novel observability data and the newly designed foundation model. The technical novelty is limited; for instance, the probabilistic forecasting using the Student-T mixture model (SMM) is an extension, as the Student-T distribution as a prediction head has already been proposed [1]. The paper introduces a new dataset, a large dataset of proprietary observability metrics, which contains statistical characteristics absent from existing datasets."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "TOTO proposes a transformer-based foundation model for time series forecasting in observability settings, introducing a proportional factorized space-time attention mechanism and a Student-T mixture model head, with large-scale pretraining on proprietary and open datasets. While the combination of these techniques and the application to observability data at scale is timely, both the factorized attention and Student-T mixture head have clear antecedents in prior work (e.g., CaFA, MTS-Mixers, SsSMM, AutoMixer), making the technical contributions largely incremental. The authors overstate the novelty of their approach by not fully acknowledging or benchmarking against closely related models, particularly AutoMixer and Predictive Observability, which have already explored similar domains and methodologies. Empirical improvements may be attributable to data scale or implementation rather than conceptual advances, and the lack of direct comparison to relevant baselines weakens claims of state-of-the-art performance. Overall, TOTO is a solid entry in the rapidly evolving field of time series foundation models, but its main advances are in the integration and scale of existing ideas rather than in fundamentally new methods."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces Toto, a foundation model designed for time series forecasting with a focus on observability metrics. Toto features two main innovations: a proportional factorized attention mechanism and a Student-T mixture model head, which enable it to handle high-dimensional, sparse, and non-stationary data effectively. Trained on one trillion time series data points, including a large portion of proprietary observability data, Toto demonstrates state-of-the-art zero-shot performance on standard benchmarks like electricity and weather forecasting, and also outperforms existing models in observability-specific tasks. Toto’s architectural innovations make it a versatile tool for both general-purpose forecasting and domain-specific applications, setting a new benchmark for scalability and accuracy in time series analysis. The use of a Student-T mixture model for probabilistic forecasting is a robust approach that can handle the complex distributions often found in time series data, particularly in observability metrics."
      }
    ]
  },
  "RiQRUcjXBD": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper explores an emerging and interesting application of LLM in proposing scientific ideas and creates a new annotated database of 50k recent AI/ML papers, annotated with entities and embeddings for ideas, backgrounds, summary etc. To my knowledge, no comprehensive scientific literature database exists where each paper's ideas are extracted and annotated, and this specific ensemble literature retrieval strategy using entity retrieval, semantic retrieval, co-citation retrieval and clustering is new, to the best of my knowledge. However, the entire retrieval strategy is an ensemble of previously utilized techniques, such as entity matching, semantic embedding search, co-citation (this is implemented in many existing databases such as PubMed), and clustering, and the idea proposal pipeline is only an implementation but with limited originality. In my opinion the most original aspect of this paper is its introduction of the annotated database where each paper has its ideas extracted and processed, though the paper does not talk about quantitative quality control of the database creation process. I am fairly certain this system outperforms vanilla approach to ask LLM to generate new research ideas, at least in the field of NLP, but the lack of sufficiently innovative approaches and the limited scale and comprehensiveness of the work make me doubtful of its contribution to the research community."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces SciPIP, an automated system for literature retrieval and scientific idea generation that demonstrates notable improvements over previous baselines and introduces several techniques enhancing both the literature retrieval and idea generation modules. Although SciPIP extends SciMON's framework with additional techniques and demonstrates improved performance, the enhancements are generally minor and come with extra computational costs and complexity. In SciMON, a related paper on scientific idea discovery, semantic, entity, and citation signals are also leveraged during literature retrieval, so the novelty of SciPIP’s approach is incremental. Furthermore, the reviewer questions the quality and originality of the generated ideas, noting that, for example, SciPIP generates an idea for multimodal learning involving “minimizing a contrastive loss function to bring together similar instances of different modalities,” which is a widely accepted approach in the field (e.g., CLIP [1], published in 2021), suggesting that the generated ideas may not be particularly novel. Additionally, the reviewer expresses concern that scoring generated ideas solely based on a novelty score from an LLM may not be reliable, as it seems unlikely that LLMs alone can accurately determine novelty."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper introduces a system called SciPIP, designed to help researchers come up with new ideas more effectively by retrieving relevant studies from a research database and leveraging a model to brainstorm entirely new ideas, aiming to balance novelty and feasibility. While SciPIP generates promising, innovative concepts and combines literature retrieval with brainstorming, I have concerns about the true originality of the generated ideas. Large language models may rely on knowledge from their training data, potentially producing ideas that are essentially variations of existing research rather than truly novel contributions. The current evaluation mainly relies on comparing generated ideas to existing papers using similarity scores, but this approach has limitations in assessing true novelty and academic value, as high similarity scores might indicate copying or minor tweaks, and low scores do not necessarily signal a breakthrough. Since similarity scores are based on semantic overlap, they do not capture the underlying logic, originality, or technical feasibility, making it difficult to measure the actual contribution or innovation of the ideas. The reliability of using large language models to evaluate originality is questionable, and supplementing the evaluation with expert reviews or alternative methods could better ensure that SciPIP’s outputs demonstrate true innovation and relevance to the research community."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The paper introduces the Scientific Paper Idea Proposer (SciPIP), a tool designed to assist researchers in overcoming information overload and enhancing the novelty of interdisciplinary research by leveraging large language models to suggest novel research ideas based on a new literature retrieval method. The original part of the work is represented mainly by the multi-path proposal. The paper's approach to aid researchers through enhanced literature retrieval and idea suggestion is noteworthy but not path-breaking. It represents a solid application of existing techniques like LLMs and embedding-based retrieval methods but lacks a transformative innovation to elevate its contribution to the research community. Overall, while useful, SciPIP as presented currently may be best suited for practical use in specific contexts rather than serving as a major research contribution."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "SCIPIP proposes a system that integrates advanced literature retrieval with LLM-based idea generation, featuring an explicit dual-path workflow (literature-inspired and brainstorming-based proposals) and a SEC-based (Semantics, Entities, Citation) retrieval method. While these design choices are well-structured, they represent incremental advances rather than conceptual breakthroughs, as similar multi-dimensional retrieval and iterative/agent-based idea generation approaches already exist in recent literature. The authors overstate the novelty of their contributions and underrepresent the sophistication of prior work, omitting direct comparisons to closely related systems such as RefAI, BioReader, ResearchAgent, and AI Scientist. The construction of a large, multi-dimensional literature database and reported empirical improvements are routine in the field and do not, by themselves, establish significant novelty. Overall, SCIPIP reflects current trends in retrieval-augmented LLMs and automated idea generation, but its main contributions are best viewed as specific workflow and integration choices within an already mature and rapidly evolving research area."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces SciPIP, a framework that leverages large language models to generate scientific paper ideas by integrating literature retrieval and brainstorming. The dual-path idea proposal method is an interesting approach that combines inspiration from existing literature with pure LLM-based brainstorming, aiming to balance novelty and feasibility, which are critical in scientific idea generation. However, the paper does not provide a detailed comparison of SciPIP with existing tools or methodologies, making it difficult to assess its unique contributions and advantages. Additionally, the idea proposal section needs a clearer description of how novelty and feasibility are quantified and assessed, and the paper does not provide enough detail on the specific metrics or criteria used for evaluating the novelty of generated ideas."
      }
    ]
  },
  "dTGH9vUVdf": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper presents FreeVS, a Video Stable Diffusion-based generative view synthesis method for driving scenes that synthesizes high-quality camera views both on and beyond recorded trajectories. The key innovation is the clever use of pseudo-images obtained through colored point cloud projection as a unified representation for all view priors, which simplifies the learning objective for the generative model. As opposed to recent contenders that rely on gaussian splatting or nerfs to represent the scene, the authors train a diffusion model on colored LiDAR point clouds. While the method introduces two new challenging benchmarks and outperforms previous approaches, the novelty is somewhat limited as the contribution boils down to an addon for Video Stable Diffusion that has colored LiDAR point features concatenated. The method trades the gaussian and nerf artifacts with diffusion ones, and while FreeVS works better than previous attempts from novel views, for single front view, splatting still yields significantly better results."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper approaches the task of novel view synthesis of outside-trajectory viewpoints on driving videos by training a conditional video diffusion model on outside-trajectory views created through projection of existing 3D point clouds. Creative use of 3D and off-the-shelf models enables a non-conventional setup, as novel view synthesis is so often limited to input trajectories, making the task fairly straightforward and limited due to constraints on using positions connected to the car. Instead, this work approaches prediction several meters away from car trajectories by utilizing colored LiDAR across multiple views to create point clouds that can be projected into pseudo-images, which is a nontrivial trick to implement effectively. This work shows it can be useful for training a generative model, and I feel the paper should be accepted as it offers good contributions in 3D and video generation to yield an effective method."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "The paper introduces FreeVS, an approach to view synthesis for driving scenes that overcomes limitations of existing methods, which primarily focus on synthesizing camera views along pre-recorded vehicle trajectories. The authors proposed a \"psuedo LiDAR controlnet\" for SVD, which is easy yet effective. The benchmark of novel trajectory synthesis looks interesting to me, and the authors proposed two benchmarks for evaluating novel camera synthesis and novel trajectory synthesis. While the baseline methods are not specifically designed for the similar purpose of the paper, there are works that use virtual warping for improving the novel view quality such as [1] [2], that might be better for the baselines. This reminds me of the existing novel trajectory synthesis benchmark [3], and the authors should test their methods on such a dataset and demonstrate the absolute performance gain using the metrics of PSNR, SSIM, etc. I personally like the idea of the paper, but I still have many concerns and would provide a final rating based on the authors' responses."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents a video diffusion model conditioned on projected colored LIDAR point clouds, and shows that it enables high-fidelity novel view synthesis in autonomous driving scenarios. In particular, compared to prior methods like StreetGaussians which show results from viewpoints limited to driving trajectories, this approach allows rendering from different viewpoints."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "FREEVS is a generative novel view synthesis (NVS) method for driving scenes that introduces a pseudo-image representation to enable pose control and 3D consistency, evaluated on challenging out-of-trajectory benchmarks with geometry consistency assessed via 3D detectors. While the pseudo-image approach is a novel technical variant, the overall paradigm—generative, pose-controllable NVS for driving scenes—has been addressed by recent works such as MagicDrive3D, WoVoGen, and MapNeRF, making the claim of being \"first\" overstated. The authors’ characterizations of prior work sometimes exaggerate their limitations, particularly regarding out-of-trajectory synthesis and pose control precision, and omit some relevant recent methods. The main technical delta lies in the specific representation and training pipeline, while the use of new benchmarks and geometry consistency metrics represents an incremental rather than fundamental advance. Reviewers should note that FREEVS’s contributions are best understood as a novel variant within a rapidly evolving field, rather than a wholly new paradigm."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces a novel method for synthesizing camera views on free driving trajectories in real driving scenes. The proposed method is a fully generative approach that can synthesize camera views on novel trajectories without the need for reconstruction, which is a novel and practical contribution to the field. The paper also introduces two new benchmarks for evaluating driving scene novel view synthesis: novel camera synthesis and novel trajectory synthesis, which are more challenging and practical than the traditional evaluation on recorded trajectories."
      }
    ]
  },
  "FR2WQcwjG4": {
    "human": [
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper focuses on the problem of novelty detection under style shifts and proposes a novelty detection method that crafts an auxiliary OOD set with style features similar to the ID set but with different core features, utilizing a task-based knowledge distillation strategy to distinguish core features from style features. In essence, the performance of the proposed method mainly relies on the quality of the generated data, and this paper only utilizes some commonly-used operations and does not propose any inspired ideas, which lacks novelty. There exist some methods that aim to leverage large-scale models, e.g., CLIP, to solve this challenge, and the authors should introduce these methods and make an analysis. In essence, these operations are commonly used methods and this paper does not propose any inspired ideas, which lacks novelty."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "The manuscript proposes a novel novelty detection method that combines data augmentation with a knowledge distillation strategy, integrating a saliency detection task to improve detection accuracy and demonstrating effectiveness across various datasets, particularly in medical imaging, which adds significant research value. However, task-based knowledge distillation strategies have already been applied in several OOD detection tasks, and the method does not clearly demonstrate how it differs from other approaches that utilize knowledge distillation, so the innovation of the proposed method is somewhat limited. The core idea remains focused on mitigating the impact of different styles on OOD detection, and while the research value lies in the model's ability to accurately identify OOD categories across various stylistic contexts, the manuscript may need to further elaborate on the innovative aspects of this approach."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents a work for novelty detection under style shifts, where the core idea is to create OOD samples by distorting content and train a student network via contrastive learning to align ID features with a frozen teacher network while separating OOD features. However, the technical contributions of this paper are weak and should be improved, which puts the work below the acceptance line."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission sits at the intersection of contrastive learning, synthetic OOD crafting, and teacher-student frameworks, with its main novelty being the use of Grad-CAM to distort core features for OOD sample synthesis and a new distillation objective for student-teacher output alignment/divergence. While the combination of these elements is new, each component has precedents in recent literature, and the field already contains meta-data free, style-robust approaches (e.g., Normalization Perturbation), making some novelty claims less pronounced. The authors’ characterizations of prior work are generally accurate but sometimes overstate limitations or underplay the capabilities of related methods, particularly regarding style robustness and meta-data requirements. Empirical improvements are notable but may be influenced by implementation details, and the practical impact of the proposed OOD crafting mechanism versus existing augmentations should be further isolated. Overall, the submission offers a substantive but incremental advance, with its strongest differentiation in the specific mechanism for saliency-guided OOD crafting and integration with a novel distillation objective, but less so in its broader framework or meta-data free claims."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a method for novelty detection in the presence of style shifts using a teacher-student framework that leverages saliency maps from a pretrained teacher network to identify core features of in-distribution samples. The proposed method is well-motivated and theoretically justified. I find the proposed method to be novel and it achieves state-of-the-art results on a wide range of datasets when compared to previous methods."
      }
    ]
  },
  "a6U41REOa5": {
    "human": [
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes a method to improve upon inverse folding methods by mining the PDB for similar structural motifs and using an MSATransformer to incorporate them when predicting loop sequences, alongside the usual structure-conditioned diffusion model as used in DiffAb. Inclusion of the data-mined motifs from PDB with similar loop structures is a quite nice and intuitive improvement that helps the performance of the model. While in general I like the idea of data mining the PDB for similar loops and using them as reference, I find that the paper is a marginal improvement on existing literature and is of somewhat limited scope for an ICLR paper (only doing inverse folding, while say DiffAb can also do complete loop sequence + structure redesign, and having only two experiments, one of which is not fully convincing to me). The difference to DiffAb was marginal and there can be a multitude of reasons for that difference."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces a retrieval-augmented diffusion framework for antibody design and optimization, with its key contribution being the retrieval of structurally-similar protein segments to the CDR for use as conditions in diffusion generation. Although the paper is the first to introduce retrieval-augmented diffusion in antibody design, there has been retrieval-augmented diffusion model proposed in other applications, such as in [1] and [2] (protein-specific 3D molecule generation), so the method novelty is fair due to the prior works. The paper needs to also cite previous retrieval-augmented generative models, and clearly state the contribution in terms of the method."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper introduces a retrieval-augmented diffusion model, RADAb, for antibody design using structural homologous motifs to guide the generative process. The approach features innovative integration of retrieval mechanisms with diffusion models and presents potential for enhancing biomolecular design. My decision to accept is based on the paper's innovative approach in antibody generation."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission introduces a retrieval-augmented diffusion model for structure-informed antibody design, combining motif retrieval with a generative diffusion backbone—an integration that is new for antibodies, though both components are individually well-established. The main technical advance is a dual-branch denoising module that incorporates both structural and evolutionary information, representing an incremental but meaningful extension of prior multi-modal conditioning approaches. While the authors claim \"first-of-its-kind\" status, the true novelty lies in the specific combination of retrieval and diffusion for antibody design, rather than in fundamentally new mechanisms. Reported empirical improvements over state-of-the-art baselines are notable, but may partially reflect implementation details or data curation rather than conceptual breakthroughs. Overall, the work exemplifies the field’s current trend of rapid, incremental integration of established ideas, and reviewers should weigh the substantive value of the architectural integration against the routine nature of such adaptations in the evolving protein design landscape."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces a novel approach to antibody design using a retrieval-augmented diffusion model. The idea of using retrieval-augmented diffusion for antibody design is novel, and the method outperforms existing approaches in antibody inverse folding and optimization tasks."
      }
    ]
  },
  "EKCubxFdOs": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper proposes a data generation and instruction tuning method for optimization-problem-solving LLMs, introducing the first complete framework for training LLMs to solve optimization problems, including instruction-tuning dataset construction and detailed methods for training. Although this paper introduces the first instruction-tuning framework for optimization tasks and makes a significant contribution to the optimization community, there is a lack of sufficient novelty, as several key components of the method follow prior work [1-3], particularly the instruction-tuning approach (Section 3.2), which reduces its originality. The method primarily applies standard training techniques, and the authors should emphasize their main innovations more clearly in the paper. Figure 1 does not effectively highlight the main differences between LLaMoCo and previous methods, which is overly simplified."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper introduces LLaMoCo, a new framework for fine-tuning LLMs to solve optimization problems, with two main contributions: a novel fine-tuning dataset and a new training warm-up strategy leveraging contrastive learning. The authors have developed and plan to release a novel dataset designed to teach language models to solve optimization problems, which represents a significant contribution to both researchers and practitioners. However, this paper lacks novelty, as it primarily focuses on fine-tuning OSS LLMs for a specific domain, with the main approach being straightforward: adjusting prompts (specifically, framing problem descriptions in Python or Latex) and developing a new dataset. The contrastive warm-up technique does not appear to be specifically tailored to optimization problems and could potentially be beneficial for fine-tuning in other domains as well; I would suggest separating this novel technique into a dedicated paper or clarifying how it suits the domain under discussion."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents LLaMoCo, a novel instruction-tuning framework for adapting large language models to generate optimization code from problem descriptions in Python or LaTeX. While LLaMoCo offers a structured approach to instruction tuning in the optimization domain and features a curated dataset and a two-phase training strategy, many components—including instruction tuning, dataset construction, and contrastive learning—leverage existing techniques. The primary contributions are seen as applications of existing methods rather than groundbreaking domain-specific innovations, and the justification for the contrastive warm-up phase as a domain-specific necessity was unconvincing, with suggestions that it could apply broadly across domains. As a result, the core novelty remains a concern, and the submission falls marginally below the acceptance threshold due to skepticism about the necessity and specialization of the proposed techniques."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "LLaMoCo extends instruction tuning and code generation methods to the emerging domain of optimization code, introducing a two-phase learning strategy (contrastive warm-up plus seq2seq tuning) and a formalized, domain-specific code-to-code instruction set. While these are meaningful methodological contributions, the claim of being the *first* instruction-tuning framework for optimization code generation is overstated, as prior works like OptiMUS have explored similar ideas, albeit with different rigor and methodology. The empirical results—showing small, instruction-tuned models outperforming larger general models—are consistent with trends in other domains and likely stem from improved data curation and tuning rather than conceptual breakthroughs. The distinction between prompt-based and instruction-tuned approaches is less clear-cut than the authors suggest, and the boundary is often blurred in practice. Overall, LLaMoCo represents an incremental but well-executed advance; its impact would be better substantiated by broader comparisons and ablation studies to isolate the effects of its novel components."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces LLaMoCo, a novel framework for adapting LLMs to generate expert-level optimizers, addressing a significant gap in the field. The approach provides a structured input-output format, uses a two-phase learning strategy, and incorporates contrasting representations of similar prompts. While LLaMoCo outperforms GPT-4 and other baselines on synthetic and realistic optimization tasks, showcasing its robustness and efficiency, the paper does not include comparisons with other algorithm selection methods, which could provide a more comprehensive evaluation of LLaMoCo's performance."
      }
    ]
  },
  "kymuzakf7V": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper suggests generating individual tabular feature values with LLM (which they denote as Oracle feature generation) per class, then building a prototype, which is used to build a non-parametric classifier. The method may offer novelty from one perspective: generating features with LLMs already exists [1], but using them additionally for building prototypes is relatively new."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper presents ProtoLLM, a novel approach to leverage LLM for supervised classification on tabular data, with a focus on few-shot and zero-shot learning. Proposed approach has (to our knowledge) significant novelty, as it does not require any training and inference is performed only based on a distance measure between the sample of interest and the generated prototypes. While the proposed approach is novel and interesting, the contribution is relatively thin for a conference like ICLR. The approach is relevant for both zero-shot and few-shot learning set-ups, but it seems restrained to applications on which LLMs may have relevant knowledge, hence not too specialized or novel, and is also restrained to classification and cannot be directly adapted for regression tasks."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper proposes a new method for few-shot learning in tabular data using large language models (LLMs). Unlike existing approaches, this method avoids overfitting to the limited training examples by prompting the LLM without directly including them in the prompt. Instead, the few-shot samples are used to create \"prototypes\" that guide the LLM's predictions. The novel prompting strategy helps prevent overfitting to the small training sets. While the proposed method presents an interesting idea, the performance improvements over simpler baseline methods are minimal, despite the proposed method requiring significantly more computational resources for inference, and the practical relevance of few-shot learning in tabular data remains unclear."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "ProtoLLM introduces an example-free, prototype-based approach to LLM-driven tabular classification, generating feature-level prototypes from task and feature descriptions rather than in-context examples. This method addresses overfitting and knowledge disruption issues seen in prior example-based LLM tabular methods (e.g., FeatLLM, TabLLM), and is evaluated using standard tabular classification benchmarks. While the technical shift to example-free prompting is substantive, the conceptual novelty is incremental, aligning with a broader field trend toward instruction-based and zero-shot LLM approaches. The submission somewhat overstates its uniqueness, as similar directions have been explored and some relevant related work (e.g., memorization, example selection, transfer/meta-learning) is omitted or under-discussed. Overall, ProtoLLM is a logical and meaningful next step in the field, but reviewers should carefully weigh the degree of innovation against both recent literature and the empirical evidence presented."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper proposes a method to prompt LLMs to generate feature values for each class in a classification task, then use the generated feature values to construct a prototype for each class, with the predicted class determined by the Euclidean distance between the input sample and each prototype. The proposed method is similar to the FeatLLM, with the difference being that FeatLLM generates the feature values for a given sample, while the proposed method generates the feature values for each class. I find that the paper should include a more detailed discussion of the differences and compare the performance between the two methods."
      }
    ]
  },
  "N2sN3LESoW": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "In this work, the authors propose Gap-Aware Preference Optimization (GaPO), a novel approach that integrates the degree of semantic gaps into preference optimization. Results on different datasets show that GaPO could surpass existing methods, including DPO and SimPO. When GaPO is combined with the scale norm, a hyperparameter γ is introduced, which is considered the contribution from SimPO, making it difficult to evaluate the independent validity of EF."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes a novel approach in RLHF by introducing a gap-aware margin into the preference optimization process, moving beyond binary preference labels by leveraging estimated semantic gaps between responses. The proposed GaPO model is a novel approach that provides an alternative to binary preference optimization by enhancing the model’s ability to accurately capture and reflect the subtleties of human preference intensity. The GaPO model outperformed existing approaches like SimPO and DPO on multiple benchmarks, particularly AlpacaEval 2.0, showing real-world efficacy. The paper also explores different gap forms in GaPO by experimenting with different EF function forms and normalization techniques."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes a new approach for preference optimization named Gap-Aware Preference Optimization (GaPO), which leverages semantic gaps as a signal for adjusting the gradient during training. The GaPO builds upon the SimPO training objective by introducing an extra evaluation factor (EF) as regularization, specifically integrating the measurement of the semantic gap between chosen and rejected responses into the training objective for preference optimization. A semantic gap is added to SimPO as a regularization term to create a training objective that accounts for the semantic gaps in preference pairs. Consider that GaPO is an improved version of SimPO, GaPO does not outperform SimPO on Arena-Hard, and the improvement on MT-Bench is marginal. Although the win rate of GaPO on AlpacaEval 2.0 is better, the response of GaPO is lengthy, and I do not believe the results can prove its superiority against SimPO based solely on AlpacaEval 2.0."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper identifies that the binary format of RLHF data labels fails to reflect the actual pairwise difference of human preference and proposes to weight the pairwise samples with respect to the semantic gap in order to provide supervision signals beyond binary labels. The proposed method is simple and has a clear connection to related works. The data-dependent margin and beyond-binary motivation is a good direction for preference optimization, however what the paper presents doesn't fully exploit the potential, as there can be a much wider spectrum of margins that more faithfully match the motivation, such as length difference or response-level LM embedding distance. A more comprehensive study over these options may bring in further contribution in this direction and potentially address the non-improvement for Arena-Hard and MT-Bench."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission introduces GaPO, a preference optimization method for LLM alignment that replaces fixed or tunable loss margins with margins dynamically set by external semantic similarity metrics (e.g., ROUGE L, Jaccard, BERTScore), aiming to better capture the intensity of human preferences. While this is a clear technical extension of prior work such as SimPO (which uses a tunable margin) and WPO (which applies dynamic weighting), the main novelty lies in the explicit use of semantic gap metrics to modulate the loss, rather than in the overall optimization framework. The conceptual advance is modest, as the loss structure closely mirrors existing approaches, and the idea of using external metrics to adapt loss terms is common in NLP. Empirical results show a modest improvement over state-of-the-art baselines, but these gains may be influenced by implementation details or metric selection rather than the core conceptual innovation. Overall, the work represents an incremental but well-executed advance in a mature and competitive field, and reviewers should weigh the technical clarity of the extension against the limited conceptual novelty."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes Gap-Aware Preference Optimization (GaPO), which integrates the degree of semantic gaps into preference optimization by modifying the existing margin term in the loss function and replacing it with an estimated gap computed using general metrics. The method is trivial—it simply introduces a new regularization term based on existing metrics."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper introduces Gap-Aware Preference Optimization (GaPO), which I find to be a novel and well-motivated approach to preference optimization. The authors correctly identify a key limitation of traditional RLHF methods—their reliance on binary labels that fail to capture nuanced differences in human preferences—and address this by incorporating the degree of semantic gaps into the loss function. GaPO provides a more granular supervisory signal, allowing the model to better understand and reflect the subtleties of human perception, which is a significant contribution that addresses a critical challenge in the field. The method is differentiated from prior work by explicitly quantifying the semantic gap using metrics such as Jaccard Score, ROUGE, and BERTScore, and using this as a basis for loss margin adjustment rather than binary labels. My assessment is that the introduction of GaPO represents a strong and original contribution, particularly in its empirical demonstration of surpassing existing state-of-the-art methods on widely used benchmarks such as AlpacaEval 2.0, with GaPO-ROUGE_L achieving a notable win rate. The paper’s thorough exploration of different forms of the loss function and mapping functions, as well as its analysis of various evaluation metrics, further supports the distinctiveness and robustness of the proposed approach. Overall, the combination of a novel method, its strong empirical results against leading baselines, and its focus on addressing a key shortcoming in preference optimization underscores the significance of the contribution."
      }
    ]
  },
  "a8mKwRQQrP": {
    "human": [
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper presents GAPSI, an algorithm that integrates the GAPS method from [1] for inventory control problems, and explains how common industry constraints such as perishability, lead times, and warehouse capacity can be mathematically modeled within the GAPS framework. However, after meticulously reading this article, I feel that the biggest issue is that it merely provides a detailed implementation guide and an empirical simulation evaluation of how the GAPS in article [1] can be applied to real inventory replenishment management. This article is primarily inclined towards empirical evaluation and presenting some implementation details regarding how the GAPS framework in article [1] can be applied to real inventory replenishment management, and I feel that this article is more like a heuristic guidance manual for inventory management that could be posted on arXiv or SSRN. My concern is that this article resembles a case study, lacking comprehensive theoretical proof and large-scale real-world practice."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "The paper introduces the GAPSI algorithm, which combines online learning techniques with inventory control theory to address complex inventory management problems. The introduction of the GAPSI algorithm represents a significant advancement in applying online learning techniques to realistic inventory management problems. This work notably contributes to practical aspects, especially in dealing with multiple products, perishability, and warehouse capacity constraints. The focus on the challenges posed by non-differentiability and the proposed solutions demonstrate a deep understanding of the complexities involved in inventory optimization. This algorithm is adapted from GAPS (Lin et al., 2024) to take into account specific aspects of inventory problems, in particular, the fact that the functions we are dealing with are not differentiable."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents an implementation manual that offers practical value to field practitioners in inventory control, aiding their day-to-day work and leveraging their existing knowledge and field insight. However, it falls short of the technical rigor and novelty needed for ICLR, and is not considered a novel academic research paper."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission presents GAPSI, an online learning algorithm for inventory control that adapts the OPS/GAPS framework by integrating feature-enhanced base-stock policies and AdaGrad learning rates, targeting complex settings with perishability, lead times, and nonstationary demand. The main technical contributions are incremental, involving routine adaptations of existing online learning and optimization techniques (e.g., feature parameterization, adaptive learning rates, handling non-differentiability) rather than introducing fundamentally new algorithms or theory. While the formalization of complex inventory problems within the OPS framework is a useful unification, similar constraints and features have been addressed in prior work, sometimes with more specialized algorithms. The empirical results showing superiority over classical policies in nonstationary settings are consistent with trends in the literature, and may be attributable to careful feature engineering and tuning. Overall, the submission’s novelty is somewhat overstated; its primary value is in the thoughtful integration and engineering of established methods for realistic inventory control scenarios."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes an algorithm called GAPSI to solve online inventory problems. The contribution of this paper appears limited, as the proposed algorithm primarily adapts GAPS to inventory problems. Additionally, the approach to computing the gradient $\\nabla L_t (\\theta_t)$ has been extensively studied in the field of inventory management."
      }
    ]
  },
  "j1jtyGdD4O": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper proposes CoDiCast, a conditional diffusion model for probabilistic global weather forecasting, where a key feature is that the conditioning states are encoded using a pretrained autoencoder before being fed as inputs to the denoising network in the diffusion model. I don’t believe that this paper has sufficient novelty, as diffusion models have been used for probabilistic weather forecasting in very similar ways before, such as in the GenCast model, and diffusion models have also been applied to other spatio-temporal forecasting problems, including autoregressive diffusion models similar to CoDiCast. The main novelty in this paper, as far as I can tell, is the use of a pretrained autoencoder for encoding the conditioning states, which, to the best of my knowledge, has not been used in this application domain before and is shown to result in a performance boost. If this design choice indeed results in consistent improvements compared to alternative approaches, it could be a valuable contribution, but the effect is only evaluated in a short ablation study and it is not clear how general the conclusions are."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes CoDiCast, a conditional diffusion model for weather forecasting, motivated by the need for uncertainty quantification and the high computational cost of ensembles with NWP methods. I am concerned about the novelty since PreDiff also uses a conditional diffusion model, and there are also MLWP models that calculate uncertainty and are not deterministic, such as PreDiff and GenCast. It seems the main contribution of this work is applying PreDiff, which was tested on precipitation nowcasting problems, to weather forecasting on ECMWF, which I'm not sure is enough novelty for this conference. In particular, the sentence \"Moreover, a single deterministic NWP- and MLWP-based method cannot achieve uncertainty quantification.\" is too strong given the works of PreDiff and GenCast. I would like the authors to explain the clear methodology differences between the proposed methods and other probabilistic diffusion models for weather forecasting, e.g., PreDiff and GenCast."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "The paper proposes CoDiCast, an autoregressive, diffusion-based weather model that generates ensemble forecasts using an additional embedding and attention-based UNet as the denoiser. However, the use of attention-based UNet is not novel and has been used extensively in prior work, such as [1], and even cross-attention has been used extensively in similar tasks, for example, ClimaX. The application of diffusion-based approaches for weather modeling is likewise not novel, as is rightly pointed out by the authors in GenCast and other similar works that come after that, such as SEEDS [2], where the code is open-sourced. Although the claim about GenCast not being open-sourced is valid, the application of CoDiCast on much coarser spatial resolution, limited temporal extent, and a small number of variables is unconvincing, especially given the much longer inference time compared to existing methods. In all, the lack of novelty, overly-simplified problem setting, and weak/inappropriate baselines are serious issues that motivate my reject rating."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper proposes a conditional diffusion model, where the conditioning leverages latent representations from an autoencoder, for weather prediction, and incorporates uncertainty quantification into the predictions by repeatedly sampling a Gaussian for each initial weather state and using that as input for the denoising process. Reviewers are rightfully concerned about limited novelty. One important shortcoming of the paper, as highlighted by one of the reviewers, is that uncertainty quantification isn't really critical for the prediction range in which the model is applied; uncertainty quantification becomes much more critical for long-range prediction and the method in the paper is falling short in those ranges."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission, CoDiCast, applies conditional diffusion models with cross-attention for global, medium-range weather prediction and uncertainty quantification (UQ), positioning itself as an open alternative to GenCast, a closely related but closed-source model. While the authors claim several firsts—including the first global, conditional diffusion model with UQ and novel use of cross-attention—these claims are not fully substantiated, as GenCast and other recent works (e.g., SwinRDM) already address similar problem spaces with comparable architectures and UQ capabilities. The main independently verified contribution is the openness of CoDiCast’s implementation, with other differences (such as cross-attention conditioning) being routine or incremental adaptations rather than conceptual advances. The assessment notes that the authors somewhat overstate the limitations of prior work, particularly regarding UQ, and do not provide direct technical comparisons with the most relevant recent models. Overall, the delta over prior art is incremental, reflecting the rapid, parallel progress in this field, and reviewers should calibrate expectations for conceptual novelty accordingly."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The authors propose a diffusion-based weather model that is probabilistic, in contrast to most other machine-learning based weather models that are deterministic. The method is novel in the field of machine-learning based weather prediction, and the results are good. However, the accuracy of the proposed method is indeed better than the other machine-learning based methods, but it is not better than traditional NWP methods. The uncertainty quantification is also not better than traditional NWP methods. Therefore, the proposed method is not better than traditional NWP methods, which are also much cheaper to run than the proposed method."
      }
    ]
  },
  "6XUSDvBFkV": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper proposes a structured binary quantization method to accelerate LLM inference by combining n:m pruning and binary quantization, compressing model weights to an average of less than 1 bit. While the proposed method is interesting and performs better than BiLLM, its novelty is limited: the proposed SI method is very similar to Wanda, with the main difference being the introduction of additional data normalization, and the binary quantization method is quite similar to BiLLM, where the hessian matrix is used to divide weights into salient and non-salient parts, and residual approximation is employed to handle the salient part. The only difference is that STBLLM processes the non-salient weights into three parts instead of two as in BiLLM. Overall, the contribution is incremental."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This work presents a structural binarization method for LLMs by combining N:M sparsity, residual approximation, and block-wise error compensation. The proposed method is a combination of several existing techniques including N:M sparsity, residual approximation, block-wise error compensation, and Trisection search (for the non-salient part). This raises some novelty concerns. I suggest the authors to highlight the main novelty and contribution of the current submission."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper introduces STBLLM, a new method for compressing large language models (LLMs) to less than 1-bit precision, with its main strength lying in its structured binarisation method that employs N:M sparsity and fine-grained weight quantisation to achieve sub-1-bit compression, allowing for more efficient storage and computation compared to previous techniques like BiLLM and PB-LLM. One of the main contributions is the Standardised Importance (SI) metric, which estimates weight importance without the need for computationally expensive Hessian-based methods, and by considering weight magnitude and input feature norm, allows for more effective weight pruning and sparsification. Another strength is the layer-wise adaptive binarisation, enabling different layers of the LLM to have varying N:M sparsity ratios for a better balance between compression and model accuracy, with weights divided into sparse, intermediate, and dense regions, each undergoing a unique quantisation scheme to ensure critical weights are preserved while less important weights are aggressively compressed. The paper also demonstrates practical efficiency improvements through a specialised CUDA kernel for structured binarisation, optimised for NVIDIA's Ampere sparse tensor cores, resulting in a 17.85x speedup compared to existing 2-bit implementations, and empirical validation confirms that STBLLM consistently outperforms other methods like BiLLM and PB-LLM across zero-shot, perplexity, and efficiency metrics, achieving superior compression with minimal impact on model performance. While STBLLM claims to “break the 1-bit barrier,” the paper does not adequately stress the theoretical limits of this approach."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission, STBLLM, extends the line of 1-bit LLM quantization by introducing structured (N:M) binarization, a new Standardized Importance (SI) metric for weight selection, and a specialized CUDA kernel for hardware acceleration, making it the first to achieve sub-1-bit/weight compression with structured sparsity in LLMs. While the combination of structured binarization and hardware optimization is a notable advance for practical deployment, many of the methodological components—such as adaptive grouping, layerwise adaptation, and importance-based selection—are incremental extensions of established techniques from quantization and pruning literature. The SI metric is presented as a key innovation, but its empirical superiority over Hessian- or activation-based metrics is not fully substantiated, and the novelty of adaptive grouping is somewhat overstated. The authors' characterization of prior work is generally accurate but occasionally downplays the sophistication and effectiveness of existing methods, particularly regarding importance metrics and structured pruning. Overall, the main contribution is the integration of structured binarization and hardware-aware engineering for sub-1-bit LLMs, with the strongest differentiation in practical speedup and memory savings, while the conceptual advances in importance metrics and adaptation are more incremental."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces a method to compress LLMs to less than 1-bit precision, which is a very challenging task. The proposed method, STBLLM, is built upon BiLLM, which may limit its novelty. The authors first identify the redundancy in binarized LLMs through the flipping experiment, and then propose a structural binarization method to further compress the model, employing an N:M sparsity technique and a Standardized Importance (SI) metric. While the proposed approach is well-motivated and achieves great performance, it is not a pure quantization method, as it uses pruning to reduce the number of model parameters, and therefore, I think it is not very fair to only compare it with quantization methods."
      }
    ]
  },
  "ZDoaLbOFaP": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper builds upon covariance neural networks, which are constructed to process covariance matrices, and incrementally improves on them by pre-processing the covariances through sparsifying the covariance matrix with hard or soft thresholding before feeding it to the network. The novelty is minor, as this work builds upon covariance networks, which are seldom used in practice, and the main contribution is the incremental improvement achieved by this pre-processing step."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes several sparsification strategies to tackle the spurious correlation and computation cost issues for the VNN, including hard and soft thresholding strategies for sparse covariance matrices and two stochastic sparsification techniques—Absolute covariance values (ACV) and Ranked covariance values (RCV)—for dense covariance matrices, with a theoretical analysis of the stability of VNN. Although the theoretical analysis of the stability of VNN is good, the originality of the proposed solutions is limited. The novelty of the proposed strategies is limited, as similar strategies have been used in the study of neural networks like dropout or pruning."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper proposes Sparse coVariance Neural Networks (S-VNNs), built upon previous work on VNNs, which applies sparsification techniques on the sample covariance matrix before convolution. While the idea of sparsifying the covariance matrix before convolution is sound, I have concerns about the limited novelty compared to prior work."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission introduces Sparse coVariance Neural Networks (S-VNNs), which extend coVariance Neural Networks (VNNs) by integrating established sparsification techniques (hard/soft thresholding, stochastic dropping) into the covariance matrix processing pipeline. The main novelty lies in applying these sparsification methods—well-known in statistics and GNNs—to covariance matrices within neural networks, accompanied by a tailored (though incremental) stability analysis. While the empirical results show improved stability, efficiency, and performance over dense VNNs and related baselines, these gains are consistent with known benefits of sparsification and may not be unique to S-VNNs. The work is best characterized as a principled integration of existing methods rather than a fundamentally new algorithmic advance, with the primary contribution being the adaptation and analysis of sparsification in the VNN context. Reviewers should note that the authors somewhat overstate the novelty of their stability analysis and the distinctiveness of their approach, as the underlying techniques are routine adaptations from prior literature."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a sparsification approach to improve the efficiency of the covariance neural network (VNN). The authors should discuss the connections and differences between the proposed method and existing sparse PCA methods, stochastic sparsification methods, graph sparsification methods, covariance sparsification methods, and neural network sparsification methods in the literature, as well as the connections and differences between the theoretical analysis in this paper and that in Sihag et al. (2022). In the experimental results, the authors should compare the proposed method with the existing sparse PCA methods, graph sparsification methods, and neural network sparsification methods in the literature."
      }
    ]
  },
  "waIltEWDr8": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents WASUP, an inherently interpretable neural network for image classification that combines a B-cos network with a classification head learning support vectors, classifying images based on similarity in the latent space. The proposed method could be seen as an extension of the B-cos network; however, the paper is not novel. The support vectors are, in essence, prototypes as in a ProtoPNet, and the comparison between an input image's latent representations and support vectors is also similar to the comparison between latent representations and prototypes in a ProtoPNet. The proposed method is simply a combination of a B-cos network and a ProtoPNet. Since the main ideas behind the paper are mostly explored in prior work and there is no novelty, the paper lacks significance."
      },
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "The paper proposed an inherently interpretable neural network that allows case-by-case reasoning and provides faithful local & global explanations by combining the Nadaraya-Watson head with the B-cos techniques. The paper’s contribution is limited. The faithfulness of the proposed model relies heavily on existing B-cos networks, while its global interpretability comes from a Nadaraya-Watson head. The work largely made engineering efforts to combine these two established ideas without introducing new insights. The original design element in this paper—using k-means clustering to extract class-specific centroids for support vectors—is intuitive but lacks novelty. The methodology is similar to prototype learning, and a stronger case for the interpretability of the approach could be made by including both numerical and visual comparisons."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper proposes to combine B-cos networks as feature extractor and a relatively common version of few-shot learning. Masking limitations in novelty: lets be clear: The Nadaraya-Watson head is (a simplified version of) few-shot learning. It is a softmax over negative distances between support samples and the test image. While it is appreciated that Wang and Sabuncu, 2022, gave a name to their analysis in order to emphasize a predecessor of few-shot learning, using this term in this paper suggests a larger or different novelty than there actually is. It should be made prominently clear in the manuscript that the Nadaraya-Watson head is effectively few-shot learning (seemingly without sampling random subsets of classes). The evidence head is a standard few shot head. Taking the positive part is a ReLU applied on a feature map. Again, that is renaming common parts to sound uncommon / novel. Masking limitations in novelty in such a way is disliked by the reviewer. This results in a low score for presentation. By that one cannot distinguish whether the contributions are actually mostly from the B-cos network or whether the few-shot head plays any role in (a) predictive performance or (b) attribution map quality. In the worst case the B-cos network alone does all the heavy lifting."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "WASUP is a hybrid interpretable image classification method that combines B-cos-style weight-input alignment with class-discriminative support vectors, aiming to provide both faithful and case-based explanations in a theoretically grounded, architecture-agnostic framework. The assessment finds that WASUP’s main contributions are its theoretical guarantee of faithfulness and its reduction in explanation complexity compared to traditional prototype-based models, though the degree of improvement over recent streamlined models is not fully quantified. While the authors claim WASUP is the first to offer local, global, and faithful explanations simultaneously, this is somewhat overstated, as recent works (e.g., ProtoPFaith, Deformable ProtoPNet) have made similar advances, albeit sometimes with only empirical faithfulness. The assessment notes that some relevant prior work on weight-feature alignment and support-based interpretability is not cited, and that the limitations of previous models are occasionally exaggerated. Overall, WASUP represents a meaningful synthesis and incremental advance in the field, but reviewers should carefully evaluate the strength of its theoretical contributions and empirical comparisons to the latest related methods."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a novel method that combines two existing methods—Nadaraya-Watson head for global explanations and B-cos networks for faithful local explanations—to provide both global and local explanations for neural network-based classification models. However, the novelty of the paper is limited, as the proposed method is simply a combination of two existing methods. The paper does not compare the proposed method with other explanation methods, which makes it difficult to evaluate its effectiveness against prior work."
      }
    ]
  },
  "oYLayGfWcI": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "The authors proposed a method for the music editing task that adapts the ReNoise regularization approach to flow matching. Although the authors offer valuable insights into training audio FM models, the proposed editing method is primarily an adaptation of the ReNoise technique, which limits the work’s technical novelty. While the paper provides extensive experimental insights, the overall results and contributions of the paper remain questionable."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper introduces a novel single-stage flow matching model for text-to-music generation, capable of generating and editing audio samples at 48 kHz stereo quality. The authors exploit a regularized flow matching inversion method to facilitate text-based music editing. However, while the results achieved are commendable, the contribution lacks sufficient novelty, as the primary distinction from ReNoise lies in the alteration of noise prediction to velocity prediction. Although the paper claims to introduce the flow matching model for text-to-music generation, it is apparent that its performance on subjective metrics does not match that of Stable Audio, and important editing methods like DITTO or MEDIC are not included in the literature review, nor is there a comparison with MusicMagus in zero-shot editing."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper presents MelodyFlow, a novel Flow-Matching based TTM model capable of generating 30 seconds of 48 kHz audio, and in parallel introduces a regularized inversion method to allow for inference-time editing. However, in general, it is hard to tell which parts of the algorithm from ReNoise are being kept and which are novel, and as there is very little textual explanation of the method (and only consistent referrals back to the ReNoise paper), it is hard to tell where the novelty lies. As this is a claimed core contribution of the paper, more time should be spent to make the differences between the baseline and their method explicitly clear. The differences between the baseline ReNoise and the proposed method seem small, and the core contribution then rests solely on the FM inversion technique, in which its novelty is hard to assess. Optimization methods like Novack et al., 2024 are agnostic to the sampling process (and in fact, the flow-matching equivalent has already been explored), and guidance methods like Zhang et al., 2024 are also agnostic to sampler, so the claim that editing methods are exclusive to diffusion models is not wholly true. The strongest modeling results are only with respect to an internal non-TTM baseline and do not seem to get comparable results with current SOTA methods, and without the modeling contributions, the novelty of the FM inversion technique remains unclear."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "MELODYFLOW presents a single-stage, flow matching (FM)-based model for high-fidelity, text-guided music generation and editing, introducing a regularized FM inversion method for zero-shot editing. While the use of FM for both generation and editing in a single-stage, high-fidelity (48 kHz, 30s) setting is new, most prior advances—such as single-stage generation (MusicGen, JEN-1) and zero-shot editing via inversion (ZETA/ZEUS, MusicMagus)—are conceptually similar, with the main novelty here being the specific combination and technical implementation. The claimed improvements in editability, efficiency, and fidelity are supported by reported results, but the magnitude and generality of these gains over strong baselines are not fully established in the excerpt. Many contributions, such as enhanced latent representations and broad task coverage, are incremental and reflect standard engineering progress in the field. Overall, the submission’s primary advance is the integration of FM and regularized inversion for editing, but reviewers should be aware that the conceptual leap over recent work is modest, and some novelty claims are somewhat overstated."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces MELODYFLOW, a single-stage text-conditioned flow matching (FM) model designed for instrumental music generation and editing. The authors adapt the ReNoise latent inversion method to the flow matching (FM) framework and compare it with the original implementation and DDIM inversion. They also conduct a variety of music editing tasks."
      }
    ]
  },
  "WlKGZuolEk": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper presents Hierarchical Multimodal Knowledge Matching (HMKM), an innovative, training-free, hierarchical knowledge matching approach for open-vocabulary object detection. By combining object-level and attribute-level prototypes, it addresses a critical gap in novel category detection effectively. HMKM’s model-independent, plug-and-play framework offers a substantial improvement in detecting previously unseen categories and can be integrated into existing models without requiring additional training. However, HMKM lacks comparisons with a wider range of state-of-the-art OVOD methods, and while it claims multimodal knowledge independence, this is not rigorously analyzed."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper proposes a method called HMKM to match image region features with categories, representing categories at both the object level and attribute level, and serving as a plug-and-play module to improve detection performance of novel categories in Open-Vocabulary Object Detection models. However, the innovative contribution relative to existing studies appears limited. While this work has improved the performance of the existing OVOD architecture, the novelty of this study is somewhat difficult to identify. Numerous related works already utilize visual prototype knowledge to enhance model classification ability, and a more explicit discussion on how it differs significantly from related methods should be conducted."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "The paper introduces a novel method, HMKM, for open-vocabulary object detection (OVOD) that leverages pre-trained vision-language models. The paper presents a creative approach to OVOD by introducing a hierarchical multimodal knowledge matching method that combines object and attribute-level knowledge, which is a novel contribution to the field. HMKM addresses a critical challenge in OVOD by improving the detection of novel categories without additional training, which is significant for practical applications where labeled data for new categories may be scarce."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This work designs a prototype-based matching method, called HMKM, to help OVOD models distinguish novel categories. The proposed method is straight-forward and can be applied on almost any existing OVOD models."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper addresses open vocabulary object detection by utilizing category-specific images to create object and attribute prototype representations, which serve as a plug-and-play module for existing OVOD models, enabling category matching with region features without the need for extensive training. The author aims to address the OVD problem in a training-free manner, which is interesting and useful for the OVD community. However, at a high level, using a dual-path score during inference to modulate classification confidence score is not novel; for example, ViLD and F-VLM ensemble predictions of the detector and CLIP via the geometric mean. The proposed method uses the combination of attribute-level similarity and object-level similarity to get a matching score, but this approach is not original given prior work. In addition, the proposed method refers to object-level and attribute-level features as hierarchical knowledge, but in essence, these features are extracted from different sizes of image crops, and whether they can be referred to as “hierarchical” is dubious. In the broader context, compositional learning, which is closely related to the focus of this paper, has already been successfully applied to various computer vision tasks, and the author did not include a necessary summary of how compositional learning benefits tasks like object detection in general."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment reviews a submission proposing a Hierarchical Multimodal Knowledge Matching (HMKM) method for open-vocabulary object detection (OVOD), emphasizing a training-free, plug-and-play approach that combines object and attribute prototype knowledge for improved detection of novel categories. The submission is well-positioned within the multimodal fusion and prototype-based generalization literature, with its main novelty being the explicit hierarchical (object + attribute) matching, which is less common in OVOD but conceptually present in related few-shot/zero-shot learning work. While the authors accurately characterize most prior methods as training-based, they overstate the uniqueness of their training-free approach and do not cite several relevant recent works (e.g., OVMR, CPN, Prototype Completion) that share similar goals or techniques. Empirical improvements are reported, but the assessment notes that these may stem from better engineering or prototype selection rather than fundamentally new concepts. Overall, the submission offers an incremental advance—primarily through hierarchical prototype integration—within a rapidly evolving field, but would benefit from a more comprehensive engagement with related prototype/attribute-based literature."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a new method for open-vocabulary object detection using a training-free approach to construct object and attribute prototypes in a hierarchical manner to match with region features, which can be easily integrated into existing OVOD methods to improve performance on novel classes. However, the performance improvement of the proposed method is not very significant; in some cases, the improvement is less than 1 AP novel, which may not be considered significant given the additional complexity and hyperparameters introduced by the method. The proposed method is only compared with a limited number of existing methods, and there are many recent OVOD methods that were not considered in the comparison, such as [1, 2, 3, 4, 5, 6]."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper introduces a new approach called Hierarchical Multimodal Knowledge Matching (HMKM) for training-free open-vocabulary object detection. The idea of combining object-level and attribute-level knowledge for open-vocabulary object detection is interesting, and the proposed method is training-free and can be easily integrated into existing object detection models. However, the paper only compares HMKM with a few baseline methods, and it would be better if more recent and relevant methods, particularly those that also leverage multimodal knowledge for open-vocabulary object detection, could be included for comparison to more accurately assess the proposed method’s contributions and relative performance. A detailed comparison of the proposed method with these approaches would help to highlight its unique contributions and limitations."
      }
    ]
  },
  "9Orm76dUuT": {
    "human": [
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper presents an approach that extends visual universal adversarial attacks, originally designed for image classification, to large multi-modal models (MLLMs) by optimizing adversarial perturbations in conjunction with a predefined trigger token to elicit specific harmful responses. First, to the best of my knowledge, no prior work has explored attacks in this particular setting, combining adversarial perturbations on images with a triggering token. However, given that existing approaches, such as Dong et al. (2023b), have already demonstrated that visual universal adversarial perturbations (UAPs) can be applied to MLLMs (potentially in a black-box setting as well), the technical novelty of this work appears somewhat limited, especially so given the limited transferability. From a practical perspective, the lack of cross-model transferability significantly reduces the attack's relevance."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents a test-time backdoor attack method called AnyDoor, which employs adversarial noise on images alongside static triggers at the text level. The exploration of backdoor attacks on multimodal large language models represents a novel research area. However, the proposed AnyDoor attack is more accurately classified as an adversarial attack rather than a backdoor attack, as it primarily relies on optimizing adversarial noise in images. The core technique combines standard adversarial perturbations (for images) with specific string triggers (for text), but universal adversarial perturbations and token-level triggers have already been extensively studied in existing literature. As a result, the proposed attack does not introduce new insights or techniques for the backdoor research community, and the technical novelty is limited."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission addresses test-time backdoor attacks on multimodal large language models (MLLMs) in black-box settings, claiming novelty in decomposing the attack across visual (setup) and textual (activation) modalities using universal adversarial perturbations. While the authors position their work as the first to demonstrate such attacks without access to training data or model parameters, several recent works (e.g., Jailbreak in Pieces, Visual Role-Play) have already explored similar test-time, black-box, and multimodal attack paradigms. The main technical refinement lies in the explicit separation of setup and activation across modalities, which offers operational flexibility but is conceptually similar to compositional or adaptive triggers in prior work. The submission tends to overstate its novelty and omits direct comparison with the most relevant recent literature, sometimes mischaracterizing prior work as limited to unimodal or training-time settings. Overall, the contribution is best viewed as an incremental technical extension rather than a paradigm shift, and claims of being \"first\" should be moderated in light of the rapidly evolving field."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a test-time backdoor attack against multimodal large language models (MLLMs), without accessing training data or modifying parameters, where the attack is assigned to the visual modality to set up backdoors and the textual modality is responsible for activating them. The proposed method is straightforward and lacks technical contributions, and the authors do not compare the proposed method with existing methods."
      }
    ]
  },
  "ursX3k1rTO": {
    "human": [
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper presents a transformer-based architecture to generate symmetric crystals conditioned on space groups in a two-stage process, and proposes a novel representation of crystal symmetry that could facilitate learning of crystal symmetry with deep learning approaches. The paper also proposes metrics to assess the symmetry of the generated crystals and highlights further gains over baseline approaches. While several works are cited in the related works section, neither described nor highlighted the difference from their approach, making it difficult to assess the uniqueness of the contribution. It is also important to add how many new tokens the method generates or if it just predicts the fixed set of tokens in different combinations (and these combinations result in more template novelty than just sampling existing templates from training data). Finally, in Table 1a, I would like to see the number of novel templates as absolute numbers instead of percentages, and the percentage of novel but structurally invalid generations from your method."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes the Wyckoff Transformer, a generative model designed for creating highly symmetric crystal structures by explicitly leveraging space group symmetry through the use of Wyckoff positions as a discrete, permutation-invariant representation of atomic locations. The Wyckoff Transformer introduces a novel approach to crystal generation by utilizing Wyckoff positions to encode symmetries explicitly, making it unique among generative models. Unlike traditional methods, it avoids positional encoding and uses permutation-invariant tokenization tailored to space group symmetries, a creative and effective innovation for materials science. This approach addresses limitations in prior methods, which struggled to produce symmetry-compliant structures, and demonstrates superior performance in symmetry-conditioned generation, creating a diverse set of stable crystal structures that respect the underlying physical symmetries. The model’s potential for symmetry-conditioned generation highlights a promising direction for future research in material informatics and generative modeling."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The authors propose Wyckoff Transformer, a generative model for material crystals that obey space group symmetry. The method proposed in the paper is intuitive, and addresses an important problem in de novo crystal generation with current AI models (ie, the lack of symmetry). However, issues were raised regarding conceptual overlap/attribution of previous work."
      },
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper focuses on the tasks of de novo materials generation and materials property prediction, with the main contribution being a Wyckoff representation tokenization and model training strategy. The Wyckoff representation builds in crystal symmetries in a natural way, and the model is good at generating materials with the proper space group. There are good empirical results for template novelty, P1, and Space Group metrics, but there is little improvement in standard de novo generation metrics, with SUN actually going down compared to DiffCSP. The property prediction benchmark is not particularly compelling because there are better benchmarks out there with other more recent models as baselines (e.g. CHGNet), such as Matbench discovery."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission presents a permutation-invariant, symmetry-conditioned autoregressive transformer for crystal structure generation, with its main technical novelty being the omission of positional encoding to enforce permutation invariance. This approach is an incremental refinement over closely related prior work, particularly CrystalFormer, which already uses Wyckoff positions and space group conditioning in a similar transformer architecture. The authors’ claims of being the “first” to combine these elements are overstated, as similar representations and conditioning have been explored in recent literature, and some relevant works (e.g., PARD) are not cited. While the empirical results are competitive, the performance gains may stem from implementation details rather than fundamental conceptual advances. Overall, the submission fits within a rapidly evolving field characterized by incremental improvements, and reviewers should interpret claims of novelty and superiority with caution, seeking more direct comparisons and a fuller discussion of related work."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces the Wyckoff Transformer, a novel generative model designed for the generation of symmetric crystals, and proposes a unique tokenization strategy that leverages Wyckoff positions and site symmetry to capture the intrinsic symmetry of crystals. The introduction of Wyckoff positions and site symmetry in a transformer-based framework is novel and addresses a gap in crystal generation methods, which often struggle with symmetry constraints. The proposed method demonstrates competitive performance in generating symmetric crystals, which is essential for real-world applications in material science and crystallography, and is evaluated on the MP-20 dataset and compared with several state-of-the-art methods, demonstrating competitive performance in generating novel structures while maintaining symmetry constraints that are crucial for material properties."
      }
    ]
  },
  "ech9J3xl9X": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This work introduced the NT-Java-1.1B model, detailing its development process and evaluation results. However, this work’s novelty is limited, as it builds on the StarCoderBase-1.1B model using training data from StarCoderData and applies established methods such as Next Token Prediction and Fill-in-the-Middle. While it provides an application of these methods, it does not introduce new improvements or substantial contributions."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents NT-Java, a narrowly fine-tuned code language model based on StarCoder's 1B parameter model, specifically tailored for edge applications on smaller devices and fine-tuned on the Java portion of the Stack. However, I find the novelty limited, as a number of large language model releases have already evaluated their models at smaller scales, and projects like llamafile have extensively tested such models on edge devices; it is unclear what distinguishes the \"Narrow Transformer\" from these models aside from being fine-tuned on Java-only. I am unconvinced that a Java-only fine-tuned transformer is a noteworthy result, especially since many blog posts already showcase the fine-tuning of pretrained models on distilled corpora, and I do not see how this paper goes beyond the current state of the art. Finetuning of pretrained models is almost commoditized by now, and with frameworks like Axolotl, similar results could be produced quickly given sufficient compute resources; I am left questioning how the authors go beyond this and contribute to small language model development."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This work introduced the NT-Java-1.1B model, detailing its development process and evaluation results. NT-Java-1.1B is developed based on the StarCoderBase-1.1B model and trained on a subset of StarCoderData."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission presents NT-Java-1.1B, the first open-source, Java-specialized small language model (1.1B parameters) designed for desktop deployment, achieved via standard fine-tuning and quantization of StarCoderBase. The primary contribution is practical—addressing the lack of Java-specific, resource-efficient code models—rather than methodological, as the technical approach closely follows established practices seen in analogous models for other languages (e.g., MonoCoder for HPC, VeriGen for Verilog, phi-1 for Python). While the empirical results for Java are new, the underlying insight that language specialization improves performance is well-established, and the process is a routine extension of prior work. The authors’ claims of being “first” are accurate for Java at this scale, but the submission overstates its conceptual novelty and omits discussion of closely related specialized SLMs and efficiency literature. Overall, the work fills a clear practical gap, but reviewers should note that its technical advances are incremental and primarily in application rather than innovation."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents a new model, NT-Java-1.1B, a specialized code language model for Java, focusing on desktop deployment. It enhances StarCoderBase-1.1B with Java-specific training and creates quantized versions for efficient inference on developer desktops. However, the paper lacks substantial innovation, with the training process being a simple application of existing methods to a new language."
      }
    ]
  },
  "wE8wJXgI9T": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "An empirical study on the impact of several loss functions on the CLIP embedding space has been presented in this work. The key contribution of this work is relatively marginal. The proposed contrastive gap lacks insightful theoretical evidence and guarantees, and the proposed mitigation strategies all build on top of existing works. While the new proposed fine-tuning loss shows some improvements, the overall technical novelty is lacking."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper investigates the phenomenon known as the \"modality gap\" in multi-modal contrastive learning models like CLIP and proposes renaming it to \"contrastive gap,\" arguing that this gap emerges as a consequence of contrastive training rather than modality differences. The paper provides a comprehensive empirical analysis of how dimensionality and batch size affect the contrastive gap, offering insights into why this phenomenon occurs in multi-modal models. The proposed solution of adding uniformity and alignment terms to the CLIP loss is relatively simple to implement and shows some improvements in certain tasks. However, the paper incorrectly attributes CLIP's loss function to SimCLR's NT-Xent loss, when CLIP actually builds upon multi-class N-pair loss, which undermines the paper's theoretical foundation and technical credibility. There is also insufficient justification for renaming \"modality gap\" to \"contrastive gap,\" and the paper lacks a comparison with previous modality gap solutions (e.g., Liang et al. 2022)."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper investigates the \"modality gap\" in CLIP embedding space and proposes to rename it \"contrastive gap\" because the gap emerges by contrastive training rather than modality difference. To tackle the problem, this paper proposes to add uniformity and alignment losses and evaluates the fine-tuned models on various downstream tasks. However, there is a lack of technical novelty, as this submission is closely related to (Al-jaff, 2023), and alignment and uniformity losses are often used by CLIP training. I also agree with the reviewers' opinions that the current study is somewhat limited, and the main claim of this submission is built upon a misunderstanding of the CLIP loss function and Wang & Isola (2020). The paper still has large room for improvements, including insufficient justification for renaming modality gap to contrastive gap and the lack of from-scratch training experiments."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission provides a theoretically grounded reframing of the \"modality gap\" in multi-modal contrastive models (e.g., CLIP), arguing that it is fundamentally a \"contrastive gap\" induced by the geometry of the contrastive loss, rather than by modality, data, or architecture. The main technical contribution is the explicit application of alignment and uniformity regularization to the loss function in large-scale multi-modal models, building on prior theoretical work (Wang & Isola, 2020) but extending it to the multi-modal setting. While prior works (e.g., Liang et al., Oh et al.) have empirically analyzed and reduced the gap using other methods (projection, hard negatives), this submission's novelty lies in its formal theoretical framing and large-scale empirical validation of alignment/uniformity regularization. The empirical finding that reducing the gap improves downstream performance is consistent with previous literature, and the main advance is in the method and theoretical interpretation rather than in practical outcomes. Reviewers should note that while the submission's theoretical perspective and explicit loss design are substantive, the empirical improvements and the identification of the gap as loss-induced are incremental relative to prior work."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper provides a new perspective on the modality gap issue in the CLIP model, i.e., it is not caused by modality itself but by the contrastive loss. It provides a detailed analysis of the relationship between the contrastive gap and the batch size and the dimensionality of the embedding space, and proposes a method to minimize the contrastive gap, which can improve the performance of the CLIP model on downstream tasks."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper studies the representations of multi-modal contrastive learning and the contrastive gap, proposing to add uniformity and alignment terms to the CLIP loss to reduce this gap. While the authors mention that their method is similar to existing methods, they do not clearly articulate what makes their approach unique, and specifically, the differences in the uniformity and alignment terms compared to existing methods need to be more clearly delineated. The paper lacks a clear explanation of how the proposed method differs from previous approaches and should provide a more detailed comparison of the proposed loss function with existing loss functions in multi-modal contrastive learning."
      }
    ]
  },
  "otXB6odSG8": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents an intercomparison study of multiple neural network architectures to replace radiative transfer parametrizations inside WRF, ultimately finding an RNN to be best and coupling it to WRF for a 4x speed up at a loss in accuracy of ~1K in surface temperature. There is limited technical novelty in the contribution which makes the work less interesting to the broad readership of ICLR. More specifically, all neural network architectures studied in this work have been previously used, the dataset has been introduced in a different study (Yavich et al. 2024), the concept of emulating radiative transfer has been widely studied and the empirical results are not ground breaking. While the study compares many baselines, it does not compare to any previously published emulators of radiative transfer schemes, which makes it difficult to assess how good the reported metrics are."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper employs the Neuralode method to perform the Atmospheric Radiation Parameterization task. While it may perform relatively well in weather forecasting, it lacks novel insights for the ML community, as it primarily appears to apply standard ML modules to AI4Science tasks. The methods used are previously published, and although adapting an existing method from area A to area B could reach the level of a Nature/Science paper, it doesn't meet my standards for ICLR. Typically, the main contribution should lie in the methodological design, incorporating modifications based on the specific problem at hand, and the comparison should focus on SOTA methods to highlight its novelty. Comparing only with outdated work makes it difficult to assess the novelty of the proposed method, and simply applying an existing method from area A to area B should truly surprise people, as such transfers are generally not considered easy. I believe this work falls short in both aspects."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This study uses a broad range of ML models, including CatBoost, Neural ODEs, CNNs, GRUs, and their polynomial activation counterparts, to assess the performance of different architectures in emulating radiation and to plug the best scheme into a regional climate model for speedup assessment. Apart from speedup in radiation emulation, the novelty of the work is not clear to me, since the central idea behind the work has been present by past papers and the paper pretty much uses established neural networks in their out-of-box configuration to emulate radiation. The idea that ML can solve this computational deadlock has been known for a while and some works mentioned in the study have also shown some preliminary progress in this direction. Because I find the ML novelty lacking and because the ideas have already been proposed in past studies, in my opinion, the paper might better fit in a physical science journal which would allow the authors to focus on the scientific merits of their particular analysis."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents the use of neural ODEs as a surrogate model for radiation parameterization to reduce the runtime of weather forecasting models, specifically integrating the surrogate with the WRF model and demonstrating speedups. This work extends previous works in using surrogates for the radiation parameterization component of weather models to speed up forecasting, and their neural ODE formulation is an interesting contribution that is shown to be more performant than previous models. However, the use of a surrogate model in itself is not a new contribution, and the overall depth of the contributions is limited by broad evaluations on many architectures with minimal analysis."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission applies Neural ODEs to atmospheric radiative transfer (RT) parameterization, building on a well-established body of work using neural networks—particularly RNNs—for this task. While the explicit use of Neural ODEs is less common, the practical distinction from profile-wise RNNs (which are mathematically equivalent to discretized Neural ODEs) is not clearly demonstrated, and the claimed \"optimality\" of RNNs is not rigorously substantiated. The integration of the neural emulator into the WRF model with significant speedup and maintained accuracy is consistent with recent trends and prior work, and does not represent a unique operational advance. The submission omits citation of directly relevant Neural ODE-based climate modeling work (e.g., ClimODE) and does not compare against more advanced architectures or address emerging evaluation metrics such as uncertainty quantification. Overall, the work represents an incremental advance—primarily in formalism rather than in substantive capability—within a rapidly evolving and mature research area."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper presents a thorough evaluation of different neural network architectures for modeling atmospheric radiation, including profile-wise RNN, sequential models, and point-wise models, providing valuable insights into the performance of various approaches in this domain. While the study demonstrates the practical applicability of the proposed models by integrating them into the WRF operational weather forecast model and achieving significant speedup without sacrificing accuracy, the paper does not provide a detailed comparison with other state-of-the-art machine learning methods for atmospheric radiation modeling. Including such comparisons would better contextualize the performance and originality of the proposed models."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper explores the application of neural networks for radiative transfer parameterization within weather and climate models, focusing on the Arctic region. I found the authors' exploration of various neural network architectures, including point-wise, profile-wise, and sequential models, to be a significant strength, and their systematic approach demonstrates a clear understanding of the problem's intricacies. The use of a profile-wise recurrent neural network (RNN) to emulate radiative flux propagation is a key aspect of their approach, and the finding that profile-wise RNNs perform best highlights the importance of considering vertical propagation and sequential dependencies in atmospheric data. The core contribution of the paper lies in demonstrating the feasibility and effectiveness of using neural networks, particularly profile-wise RNNs, for radiative transfer parameterization, leading to substantial computational gains within a regional weather model, as evidenced by the 26.5-fold speedup in radiation step computation. While the paper presents a valuable contribution by showcasing the potential of neural networks for accelerating radiative transfer calculations in weather and climate models, it lacks a detailed comparison to other existing parameterization schemes and does not provide a clear justification for the choice of the profile-wise RNN architecture over alternatives. Additionally, limitations in generalizability, potential overfitting due to a high number of parameters, and a lack of rigorous evaluation of overall forecast accuracy restrict the significance and originality of the contribution. Despite these limitations, the systematic evaluation of different neural network architectures and the demonstrated speedup provide a good starting point for future research in this area."
      }
    ]
  },
  "aPTGvFqile": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper proposes AlignCLIP to reduce the modality gap in CLIP embedding space by leveraging a shared transformer to align image and text embeddings, and introducing an additional loss function to separate semantically distinct unimodal embeddings. The technical contribution appears limited, as prior work has also leveraged the shared transformer framework [1, 2]. The proposed method utilizes a single-encoder framework, which is completely different from CLIP’s two-tower architecture, raising questions about whether the obtained embedding space is relevant to the CLIP embedding space."
      },
      {
        "id": "human_review_5",
        "type": "human",
        "label": "Human Review (review_5)",
        "content": "The paper proposes a method to mitigate the modality gap in the representational space of CLIP by introducing two modifications (termed AlignCLIP): sharing the weights of the vision and text transformers in CLIP (SharedCLIP), and using a new Intra-modality separation loss that encourages separation between images in CLIP space while respecting some of the semantics of the similarities between the images. Understanding and mitigating the modality gap is an important direction in CLIP and its variants, which are ubiquitously used in many applications. The authors suggested two directions (SharedCLIP + IMSep loss) for attempting to reduce the modality gap, and the effects of both SharedCLIP and SharedCLIP + IMSep loss were tested. Further, the impact of enforcing intra-modality separation on images, texts and their combination was tested individually, and the effect of the rescaling mechanism to control separation of similar images in the batch was also tested individually."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper introduces AlignCLIP, a variant of the CLIP model aimed at reducing the modality gap between image and text embeddings in cross-modal learning. Both SharedCLIP and IMSep primarily extend existing contrastive learning techniques; sharing parameters between encoders is a known approach, and the IMSep loss largely repurposes InfoNCE without substantial modifications. The paper’s novelty, therefore, is limited. Empirical comparisons with relevant baselines are lacking, as AlignCLIP is only compared with the original CLIP (and SharedCLIP, also introduced in this paper), omitting several cited \"naive\" approaches for modality gap reduction. Table 7 also compares AlignCLIP with two other CLIP variations, but from what I understand, these variations were not designed to mitigate the modality gap."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposed a novel method named AlignCLIP to address the modality gap problem for the CLIP network, with improved components including a shared transformer and an intra-modality separation (IMSep) module. The proposed Intra-Modality Separation module is novel and is benefit to the community. However, the idea of a shared transformer is less novel; since the shared transformer for CLIP has been proposed by You et al., extending it to the projection layer is just an incremental improvement. The paper should discuss why the extension to the projection layer is necessary compared to existing methods. While the IMSep module is more related to the general multi-modality contrastive learning task and has better expansibility for various model structures, the main novelty lies in this component rather than the shared transformer."
      },
      {
        "id": "human_review_6",
        "type": "human",
        "label": "Human Review (review_6)",
        "content": "This paper proposes the modality gap problems existing in CLIP and shows the multi-modalities distribution characteristics. To handle this problem, it proposes a novel method named AlignCLIP, which mitigates the multi-modal gaps by sharing the parameters between all modalities and introduces an intra-modality separation objective function to better align text-image and image-image pairs. This paper shows the multi-modalities distribution characteristics and proposes two novel ideas to mitigate such modal gap. However, it seems this paper is not the first to propose modality gap problems, and the authors have mentioned that there are several works also studying the modality gaps; I would like to know what is the difference between your findings and these works."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces AlignCLIP, a novel training strategy that aims to improve the cross-modal alignment of CLIP-like models by addressing the modality gap between image and text in the shared embedding space. The authors propose to share the learnable parameters between the vision and language encoders to align the two modalities more closely, and to add an Intra-Modality Separation (IMSep) objective function that pushes semantically dissimilar image embeddings apart. The IMSep objective adds a novel approach to handling intra-modality representations, ensuring that semantically dissimilar image embeddings are spread apart without affecting semantically similar pairs. AlignCLIP demonstrates significant improvements to the original CLIP, with measurable gains in downstream tasks such as zero-shot classification and robustness to distribution shifts."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper researches the challenging problem of effectively handling the pronounced modality gap in CLIP from a geometrical point of view, first visualizing this phenomenon using the pre-trained ViT. The overall novelty of this work is the proposed discriminative criterion based on the cross-modal (diagonal elements of the similarity matrix) information alignment and intra-modal (off-diagonal entries of the similarity matrix) discriminative learning. The overall idea of this work is good but lower than the excellent bar of 'spotlight'."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission addresses the modality gap in CLIP by proposing parameter sharing between vision and language encoders and introducing a semantically-regularized intra-modality separation objective. While these approaches are timely and relevant, both ideas have close precedents in recent literature (e.g., SoftCLIP, UniCLIP, iCLIP), and the main novelty lies in the specific implementation rather than the high-level concepts. The authors somewhat overstate the originality of their contributions, particularly regarding parameter sharing for alignment, and omit several directly relevant recent works from their discussion and comparisons. Empirical improvements are reported, but without comprehensive benchmarking against the latest baselines, it is unclear if these gains are due to conceptual advances or implementation details. Overall, the work represents an incremental advance in a crowded and rapidly evolving field, and reviewers should calibrate their expectations for novelty and request more thorough comparisons to recent related methods."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents a novel approach to address the modality gap problem in Contrastive Language-Image Pre-training (CLIP) models by proposing AlignCLIP, which improves cross-modal alignment by sharing learnable parameters between the image and text encoders and introducing a semantically-regularized intra-modality separation objective function. However, the idea of sharing parameters between the image and text encoders is not entirely new and has been explored in previous works, such as [1]. The authors should provide a more detailed discussion on how their approach differs from these existing methods."
      }
    ]
  },
  "A72sZWB66Q": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents HyperDet, a novel and effective framework for detecting synthesized images with high generalization capabilities. While the proposed method combines low-level and semantic features and uses a mixture of experts (MoE) for selection, the paper lacks sufficient motivation and insight into why this combination is novel and significant. Simply combining existing techniques without providing a compelling rationale or analysis of the advantages over prior work may not meet the bar for novelty and significance required by ICLR."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper presents HyperDet, a novel and generalizable detection framework designed to effectively identify synthetic images by integrating shared knowledge from lightweight expert detectors and leveraging a large pretrained vision model. I find the approach novel, particularly in its incorporation of hypernetworks that generate optimized weights for specialized LoRA experts, which enhances the extraction of generalized discernible artifacts. The authors also propose an SRM filter grouping strategy to capture varying levels of pixel artifacts and introduce a novel objective function to balance pixel and semantic artifacts. However, I note that the claimed novel objective function is simply a weighted sum of the binary cross-entropy loss of the original image and the filtered image."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "HyperDet is an incremental advance in the field of AI-generated image detection, building most directly on the MoLE paradigm by introducing a hypernetwork to generate and merge LoRA weights for expert detectors—a technical extension rather than a conceptual leap. While the use of a hypernetwork for LoRA weight generation is new in this context, other contributions such as grouping SRM filters and balancing pixel/semantic objectives are routine adaptations of established ideas. The empirical results show strong performance gains, but these may be attributable to ensembling and large-scale training rather than the specific hypernetwork mechanism. The authors’ characterizations of prior work are generally accurate, though they somewhat overstate the novelty and omit several recent, relevant feature fusion and hierarchical detection methods. Reviewers should recognize HyperDet as a solid technical extension with notable empirical results, but not as a fundamentally new direction in the field."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents a novel approach to detecting synthetic images by introducing the HyperDet framework, which leverages a collection of expert detectors and a hypernetwork to generate optimized weights for LoRA models, enhancing the detection of artifacts in images generated by various generative models. The paper introduces a novel approach by combining SRM filters with a hypernetwork to generate optimized LoRA weights, which is a creative solution to enhance the detection of synthetic images. HyperDet achieves state-of-the-art performance on the UnivFD and Fake2M datasets, demonstrating its effectiveness in distinguishing synthetic images across different generative models."
      }
    ]
  },
  "u2QdCiOgwA": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This work proposes a dynamic pruning method with learned masking that adapts based on language and task contexts, introducing a context-aware pruning mechanism that uses language tags to inform mask generation. The approach is novel, particularly in its dynamic adaptation and the innovative use of language tags, which has strong potential for real-world impact. However, network pruning using learned masks is not particularly novel and has been studied in various domains, including self-supervised models for language and speech. Expanding on these methods in the related work section and clarifying the novelty of this specific approach would strengthen the paper’s positioning. The method’s main contribution lies in its context-aware, adaptive pruning strategy, which offers an efficient and adaptable solution for deploying large speech models in resource-limited environments and could inspire further research into resource-saving models in NLP, computer vision, and other domains."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This research paper introduces a context-aware dynamic pruning technique for speech foundation models, enabling the models to adjust their structure based on contextual factors like language and task during inference. However, the paper lacks major novelty. I think the paper simply extends the work of Peng et al. (2023b) by utilizing the model structure of a speech foundation model to address multilingual and multi-task scenarios and therefore lacks major novelty. I would like to see a more detailed comparison of their approach to previous work and their novel contributions beyond Peng et al. (2023b)."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes a method to prune (specifically structured pruning) Whisper-like speech foundation models, with a novel modification to the Context-Aware Gate Predictor: to handle multiple languages and tasks simultaneously, the authors created vectors representing the language and task, combined them with the speech features, and used them as input to the Gate Predictors. The analysis provided on sparse encoders and decoders is novel, and the extension to multilinguality and multi-task is new and promising. However, most methods applied are borrowed from Peng et al. (2023b), who also propose module-wise structured pruning, and this paper just applies it to multilingual and multi-task scenarios and explores how a large-scale speech foundation model adapts its structure based on context. I do not find anything much novel about the paper; specifically, an extension of an existing method to multilingual and multi-task scenarios is not very interesting to me, and I would appreciate if the authors can point out what is novel in their \"novel context-aware pruning technique\" (except the gating technique), which is difficult for me to understand."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This research paper introduces a context-aware dynamic pruning technique for speech foundation models, enabling the models to adjust their structure based on contextual factors like language and task during inference."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission presents an incremental but meaningful advance in the area of context-aware, dynamic pruning for speech foundation models (SFMs). The main novelty lies in leveraging a combination of task, language, and speaker context to dynamically predict pruning masks at inference time, enabling more fine-grained adaptation than most prior works, which typically focus on static or less granular context features. While the authors claim to be the first to achieve such context-aware, inference-time pruning without model alteration, this is somewhat overstated, as closely related works (e.g., S3-Router, I3D, Layer Pruning on Demand) already address dynamic and context-dependent pruning, albeit with different mechanisms or narrower context scopes. The interpretability analysis and demonstration of task context dominance are useful but represent routine extensions of prior mask-based pruning studies. Overall, the submission is well-positioned within a rapidly evolving field, offering a substantive, if incremental, contribution, but would benefit from a more balanced comparison with recent related work and a more nuanced articulation of its novelty."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper presents a novel dynamic pruning technique for speech foundation models that adapts to contextual factors like speaker characteristics, languages, and tasks during inference. This approach offers a practical solution to reduce inference time without compromising accuracy, which is a valuable contribution. While the study highlights the effectiveness of dynamic pruning and provides insights into the interpretability of pruned network structures, the current version does not fully explore existing research on dynamic pruning in speech foundation models, and it lacks a thorough comparison with other dynamic pruning techniques. Including such comparisons would strengthen the argument for the proposed method and provide a clearer picture of its advantages and limitations."
      }
    ]
  },
  "lBrLDC7qXF": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes Context Aware BERT for Knowledge Graph Completion (CAB-KGC), which introduces contextual information to entities and relationships in KG, eliminates the need for entity descriptions and negative sampling, and reduces computational complexity while improving performance. The language model-based method discussed in this paper exhibits a certain novelty compared to traditional structure-based approaches. In addition, the Evaluation based on Distance from Average Solution (EDAS) criteria used in the paper is a relatively novel evaluation metric that can better evaluate model performance in the presence of multiple metrics. However, this work has limited technical contributions, as the approach simply concatenates entities, relationships, and their contexts and inputs them into the language model, then calculates the probability of all available entities as tail entities; the idea lacks novelty."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces the CAB-KGC (Context-Aware BERT for Knowledge Graph Completion) model, which presents a novel approach by leveraging the contextual information of neighboring entities and relationships without relying on entity descriptions or negative triplet sampling, a common limitation in previous KGE and LLM-based methods. This removes the dependency on external textual information, making it applicable to a wider variety of KGs, especially those that lack entity descriptions, and leads to more efficient training and improved evaluation performance. However, the innovation in this work seems incremental, as it mainly builds on the SimKGC framework, with the only major difference in the CAB-KGC model being that it does not require head entity descriptions and employs a classification loss (cross-entropy) instead of contrastive loss for training. The introduction of the EDAS criterion also has the potential to influence future performance evaluation practices in the knowledge graph domain."
      },
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper introduces a novel approach for Knowledge Graph Completion (KGC) named CAB-KGC, which leverages context-aware information from both head entities and relationships, aiming to address limitations of existing embedding-based and LLM-based models. The introduction of the EDAS (Evaluation based on Distance from Average Solution) metric is a unique contribution, offering a more comprehensive assessment by incorporating both positive and negative deviations. However, CAB-KGC’s novelty is unclear, as the model combines context-aware techniques with BERT embeddings, which, while valuable, may not be a groundbreaking innovation within the KGC domain. The approach might appear as an incremental improvement over existing models rather than a fundamentally new technique, and it would be beneficial for the authors to clarify CAB-KGC’s unique contributions and differentiate it from similar methods."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "CAB-KGC is an incremental extension of recent PLM-based knowledge graph completion (KGC) models, primarily distinguished by its elimination of negative sampling and strict avoidance of entity descriptions, relying solely on structural context for tail prediction. While the authors claim novelty in context integration, several recent models (e.g., CSProm-KG, StAR, NNKGC) also incorporate KG structure/context, making CAB-KGC’s main difference one of implementation detail rather than conceptual advance. The introduction of the EDAS evaluation metric is a useful addition, but similar motivations for improved evaluation have been addressed in other recent work. The paper reports state-of-the-art results, but the attribution of improvements to specific innovations is not fully disentangled, and the field is characterized by frequent, incremental advances. Overall, CAB-KGC is a well-executed synthesis of current trends, but its novelty is moderate, and the paper would benefit from a more thorough and balanced comparison to recent structure-aware and prompt-based KGC models."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a novel KGE model, namely Context-Aware BERT for Knowledge Graph Completion (CAB-KGC), which utilizes contextual information from linked entities and relations within the graph to predict tail entities and eliminates the need for entity descriptions and negative triplet sampling. However, the novelty is limited, as the proposed method is quite similar to the well-known entity2vec, which also uses the surrounding entities and relationships of the head entity as contextual information to represent the entity; the primary difference is the use of BERT to encode the context rather than an encoder as in entity2vec. It is therefore necessary to explain the novelty of the proposed method. Additionally, the proposed EDAS criterion is a direct application of the EDAS method to KGE, and thus the contribution is limited."
      }
    ]
  },
  "381rZinzJE": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes Physics-Informed Autoencoders (PIAEs) that extend standard autoencoders with a stochastic differential equation framework to model Net Ecosystem Exchange (NEE) changes over time, specifically addressing nighttime measurement gaps in CO2 emission data from agricultural fields. Their main contribution is the integration of autoencoder architectures with physical NEE models, utilizing SDEs defined as Wiener processes to combine daytime and nighttime models with Gaussian noise. The method is shown to provide forecasting capabilities and enhance performance on NEE gap-filling by accurately learning the NEE distribution and associated parameters. The approach is evaluated on 8 years of flux tower data from East Anglia, demonstrating improvements over current state-of-the-art methods, particularly for nighttime predictions, where a 22% higher R2 score is achieved compared to Random Forest approaches."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper proposes Physics-Informed Autoencoders (PIAEs) to address gaps in carbon dioxide (CO2) emission measurements, specifically for Net Ecosystem Exchange (NEE) data from agricultural fields. According to the reviews, the paper has weaknesses including limited novelty, as the work mainly applies existing methods to a specific domain. There is a lack of broader applicability and justification for key design elements, such as the two-phase training process. Summing up, while the results are promising, the paper's complexity and missing details limit its impact on a general machine-learning audience."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The paper studies the application of autoencoders for the problem of imputing missing values in Co2 Net Ecosystem Exchange (NEE) measurements, where the autoencoder takes in several covariates at a given timestep and predicts the next-step NEE along with several variables of a Stochastic Differential Equation (SDE) that models changes in NEE. In the introduction, the first highlighted contribution is the introduction of a SDE for NEE measurements, and put that way, it sounds like the SDE is novel also in the physics; however, this point is not stressed again later on, so I wonder whether the SDE is known and the novelty is in its use as supervision for learning ML models. In section 4.4, it is mentioned that the integration of the SDE in the training of the autoencoder follows previous work [Raissi 2017], but it is not sufficiently described to make the paper self-contained. The related work is not sufficiently described, and it is not clear whether the reported baselines RFR and XgBoost variant based on the work of [Moffat 2007] are also physics-informed or only statistical. The presentation of the paper is convoluted, and requires a degree of familiarity with the NEE problem that is uncommon in the ICLR community, making it hard for me to judge the significance, originality and potential impact of the work."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission presents a physics-informed autoencoder (PIAE) that integrates stochastic differential equations (SDEs) and empirical day/night Net Ecosystem Exchange (NEE) models for gap-filling and forecasting from eddy covariance data, situating it at the intersection of physics-informed deep learning, SDE-based modeling, and environmental data processing. The main novelty lies in the domain-specific integration of established techniques—namely, combining SDEs and empirical NEE models within an autoencoder framework—rather than in the invention of fundamentally new algorithms. While the approach is the first to apply this unified framework to NEE gap-filling, the underlying components (physics-informed autoencoders, SDEs, empirical models) are well-established in other domains, and the submission does not fully acknowledge related work or recent advances in baseline methods like REddyProc. Reported empirical improvements, especially for challenging nighttime and long-gap scenarios, are notable but may be influenced by implementation choices rather than conceptual breakthroughs. Overall, the contribution is an incremental but meaningful adaptation of existing methods to a new application domain, with the strongest differentiation being the specific integration and evaluation approach for NEE gap-filling."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper introduces a Physics-Informed Autoencoder (PIAE) to fill gaps in Net Ecosystem Exchange (NEE) measurements, integrating physics-based NEE models with a stochastic component in a way that is a novel approach and could potentially improve the reliability of carbon dioxide emission measurements. The authors claim that PIAE outperforms existing methods like Random Forest Robust (RFR) in predicting NEE, especially at night, and provides improved data quality for climate change research. The method shows significant improvements in handling nighttime data gaps, which are challenging for existing techniques."
      }
    ]
  },
  "mEpqHvbD2h": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "The authors propose a framework for finetuning a pretrained diffusion policy using on-policy RL, specifically PPO, and show that their method improves performance on long-horizon sparse-reward tasks and works on real hardware. While the technical contribution is fairly good, the novelty of this approach is relatively low. The proposed method follows the more general and somewhat repetitive trend of \"take diffusion policy and try it on task X and potentially finetune with method Y\" style papers in the robot learning community. At this point there are a number of diffusion policy papers applied to various tasks as well as the original diffusion policy paper itself that show that diffusion models consistently outperform GMM and Gaussian policies at capturing the modes of a multi-modal distribution, and so this result is no longer surprising or novel. Despite the somewhat low novelty, I believe that the impressive results and the fact that this paper is one of the first, if not the first, diffusion policy finetuning methods capable of leveraging high-throughput simulators is an important contribution to the robotics and ML communities."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper studies how to use PPO to finetune the diffusion-model based policy in an offline-to-online setting. Utilizing RL method to finetune diffusion model is in great need, and the fruitful experimental results presented in this work are of great value and originality. However, based on my comprehension, the diffusion policy and PPO used in this work are of no novel changes and contributions. Combining these two together and successfully applying on real-world robotic systems is of great value and potential, but the advantage of diffusion policy to vanilla parameterization is already well-established and not the contribution of this paper, in my opinion. Sect. 6 of structural exploration is of much importance and novelty."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces Diffusion Policy Policy Optimization (DPPO), an algorithmic framework that fine-tunes diffusion-based policies using policy gradient optimization (PPO) from reinforcement learning, and presents a novel dual-layer MDP framework along with several best practices such as fine-tuning only the last few denoising steps and using DDIM sampling to improve efficiency and performance. However, compared to QSM, DPPO does not introduce particularly novel concepts but rather provides engineering or empirical adjustments, such as fine-tuning certain denoising steps and replacing value estimation with advantage estimation."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper introduces a technique for fine-tuning diffusion policies with a policy gradient method. Previously, policy gradient updates of diffusion policies were conjectured to have training instability. With the new technique proposed, the authors show that the resulting algorithm called DPPO can fine-tune to superior policies both in simulations and real robots. This paper has the potential to be adopted by many papers in the future."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "**Summary for Reviewer:**\n\nThis submission introduces DPPO, a framework for fine-tuning diffusion-based policies in reinforcement learning using policy gradient (PG) methods, with a focus on robotic control. While prior work has attempted PG-based optimization for diffusion policies—often finding it unstable or ineffective—DPPO demonstrates, through careful design choices (e.g., normalization, regularization, PPO-style updates), that PG can be made robust and effective, achieving strong empirical results in both simulation and real-world tasks. The main contribution is thus an empirically validated set of best practices for stable PG fine-tuning, rather than a fundamentally new algorithmic idea. The authors somewhat overstate their novelty, as similar PG approaches have been explored (though less successfully) in recent literature, and some closely related works are omitted from the discussion. Overall, DPPO represents an incremental but important advance in the field, with its primary delta being practical effectiveness and empirical robustness rather than conceptual innovation."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a framework for fine-tuning diffusion policy via on-policy reinforcement learning. The novelty of this paper is limited, as the idea of fine-tuning diffusion policy via RL has been explored in previous works, such as [1]. The authors claim that previous works conjectured that policy gradient (PG) methods are less efficient for diffusion policies, but there is no evidence to support this claim; to the best of my knowledge, there is no work suggesting that PG is less efficient for diffusion policies. On the contrary, [1] directly fine-tunes the diffusion policy via on-policy PG and achieves good performance. The authors should compare DPPO with other diffusion-based policies and other fine-tuning methods, such as [1], to better establish the contribution. Therefore, I do not believe that DPPO can be claimed as a state-of-the-art method."
      }
    ]
  },
  "skJLOae8ew": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents an application of diffusion models to the generation of architectural floor plan images. However, there is limited novelty in the application or use of diffusion models, as there is ample prior work using diffusion models for architectural floor plan generation or other kinds of layout generation, which reduces the novelty of this work. The author(s) also did not cite these other related works and/or discuss the relationship or difference between the presented work and prior work."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes a new pipeline for generating architectural floor plans by applying diffusion models, fine-tuning them to learn implicit design concepts in architectural design, and generating detailed and functional floor plans. However, the technical innovation in this paper is quite insufficient, and the introduction to the U-Net architecture is entirely superfluous. For ICLR, this paper clearly lacks innovation and systematic methodology, and it does not quite meet the threshold for current AI conferences."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This research introduces a novel application of diffusion models adapted for the generation of architectural floor plans. However, there are concerns over the technical contributions, novelty of the paper."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission adapts diffusion models—specifically a customized U-Net architecture with integrated upscaling—for efficient, high-resolution architectural floor plan generation, situating it within a well-established trend of domain adaptation and engineering optimization in generative modeling. While the authors claim to be the first to apply diffusion models to this domain and to uniquely capture architectural semantics, these claims are not fully substantiated, as prior works such as HouseDiffusion and Tell2Design have already explored similar territory. The main technical contributions appear to be in the specific engineering choices and integration of upscaling for floor plans, representing an incremental rather than a fundamental advance. The submission omits direct comparison with the most relevant prior work and tends to overstate its novelty, particularly regarding \"firstness\" and semantic capability. Reviewers are advised to focus on the empirical results and robustness of the engineering, but to calibrate expectations regarding conceptual innovation and to request more thorough citation and comparison with existing literature."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces a new diffusion model designed for generating architectural floor plans, adapting the U-Net architecture and employing upscaling techniques to enhance efficiency and detail. The application of diffusion models to architectural design is a relatively new and promising field. However, the proposed method lacks novelty, as it primarily adapts an existing diffusion model (U-Net) for floor plan generation without introducing significant innovations. The paper does not cite several important works in the field, such as DreamPlan, HouseDiffuser, and HouseGan++, which further highlights the absence of originality and differentiation from prior work."
      }
    ]
  },
  "InWaCoIMMN": {
    "human": [
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes 'competence-based analysis of language models' (CALM), which introduces a formal notion of competence in terms of causal models and applies insights from linguistics that seem to be novel and original. The main contribution is a theoretical framework, but my impression is that this framework, applied in its current form, is unlikely to yield novel or significant insights about LM behaviour. While the introduction of a formal notion of competence is interesting and appears original, I do not find the case compelling that the framework actually shows meaningful or unique results, as the empirical demonstrations are limited to showing that the proposed competence metric is reasonably correlated with accuracy across a handful of task/model combinations. I would be open to revising my score if the authors can address this concern, particularly by providing precise situations where CALM could explain otherwise unexpected behaviours."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces an evaluation framework that seeks to understand language models by examining their internal representations of linguistic properties, using causal probing to measure the alignment of an LM’s representations with human-understandable causal structures of specific tasks. The strength of the paper is that it uses an evaluation approach that combines causal probing with Gradient-Based Interventions (a concept similar to adversarial perturbations), and the introduction of GBIs allows for a broader and more precise range of probing interventions, addressing some possible previous limitations in probing techniques. However, overall the novelty of the paper is limited as it uses an existing dataset (ConceptNet) and the established method of causal probing. It seems to me that your 5 contributions are summarized by this strategy and its application, and I would like to see some motivation regarding the usefulness of this framework, perhaps by comparing the CALM framework with other existing causal probing frameworks to indicate its importance. Given that previous studies on lexical inference tasks have also highlighted inconsistencies in model performance, I question what novel evaluative insights the CALM framework provides."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the CALM framework is a well-integrated, incremental extension of existing causal probing and competence benchmarking methods for LLMs, introducing a gradient-based adversarial intervention technique and a tailored competence metric. While the authors claim several \"firsts,\" such as the first quantitative measure of LLM competence based on causal alignment and the ability to handle arbitrarily-encoded relational properties, these claims are somewhat overstated, as similar metrics and interventions exist in prior work (e.g., IIA, causal abstraction, BEAR, CausalGym). The main technical novelty lies in the use of gradient-based optimization for interventions and the formalization of a competence metric specifically adapted to LLMs, but these are adaptations rather than conceptual breakthroughs. The submission sometimes underplays the flexibility and scope of previous frameworks and omits direct comparisons to highly relevant prior work, which would help clarify its true contribution. Overall, CALM represents a meaningful but incremental advance, with its primary value in integration and generalization rather than in introducing fundamentally new methodologies."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a framework, CALM, for measuring the competence of a language model at a particular task. The proposed measure of competence is novel. However, the authors did not compare their method with existing methods for evaluating model robustness, such as the ones mentioned in Section 4.2, and it is unclear how this measure is different from existing measures of model robustness."
      }
    ]
  },
  "c4w1TqcSi0": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This work proposes a framework that improves the effectiveness and efficiency of LLMs in multi-agent dialogue systems by iteratively optimizing data and training LLMs using SFT/DPO, where iSFT uses sampling to generate better data and iDPO uses MCTS to generate paired data, along with a carefully designed reward function to ensure overall system effectiveness and efficiency. While this paper offers a well-structured approach to enhancing the effectiveness and efficiency of MAS, iSFT and iDPO have been extensively explored in prior works. Thus, the main contributions here—reward function design and data improvement mechanism in MAS—offer limited novelty within the existing research landscape. The data generation mechanism for MAS is interesting and may facilitate further improvements in MAS, including frameworks like AutoGen, but the methods are relatively general and do not address the key issues in MAS."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces a framework based on an iterative generate, rank, select, and train paradigm to address inter-agent communication and task inference challenges within LLM-based MAS, building iSFT and iDPO on this iteration paradigm. However, the methods iSFT and iDPO seem to lack innovation. iDPO merely combines MCTS from ToT with DPO, while iSFT simply adds a step of supervised fine-tuning (SFT) after removing the prompt. These methods seem incremental rather than novel, and similar approaches can already be found, such as [1], [2], [3], [4], [5]."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper proposes a training method for multi-agent systems of LLMs. Unfortunately, reviewers were concerned with novelty and incrementality of the approach."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "**Summary for Reviewer:**\n\nOPTIMA is a multi-agent LLM framework that formally integrates multi-objective reward optimization—balancing task performance, token efficiency, and communication readability—using an iterative generate-rank-select-train paradigm and MCTS-inspired DPO data generation. While the approach is well-positioned at the intersection of multi-agent debate, process-level optimization, preference modeling, and communication efficiency, most of its components and their combinations have been explored in prior work, making the main technical delta the formal unification and application context rather than fundamentally new algorithms. The authors’ claims of novelty, particularly regarding unified optimization and MCTS-inspired DPO in MAS, are somewhat overstated, as similar ideas have appeared in related literature, though OPTIMA’s explicit reward formalism and MAS adaptation are more formalized. The empirical improvements reported are substantial, but may be partly attributable to implementation choices and task selection, and the work would benefit from more direct comparisons to closely related methods (e.g., SimPO, ReST-MCTS*). Overall, OPTIMA is a strong, contemporary integration of recent trends in the field, but its contributions are primarily incremental and formal rather than radically innovative."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper introduces OPTIMA, a novel training framework designed to optimize LLM-based multi-agent systems for enhanced communication efficiency and task effectiveness. The proposed method is novel, particularly in its use of MCTS to generate diverse trajectories for DPO training, which I find to be a smart approach to explore different interaction paths and identify high-quality data for training. The results demonstrate that OPTIMA consistently outperforms both single-agent MAS baselines and vanilla MAS, highlighting significant improvements in communication efficiency and task performance."
      }
    ]
  },
  "29sul3tAEa": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper presents a novel rehearsal-free approach for continual learning based on hypernetworks that generate so-called adapters to adapt the pre-trained model to different tasks. Overall, the technical novelty is rather modest: hypernetworks have been used for continual learning in the past, the ideas of exploring a combination of faster and slower adapting models have been explored in the past, and the idea of recognizing already observed/recurrent tasks/concepts has been studied in the past too. However, the studied combination of ideas in the context of adapting pre-trained models is novel to the best of my knowledge. It is not fully clear how the technical novelty is positioned, as well as what baselines should be included for demonstrating what ideas actually work (e.g. other hypernetworks-based continual learning? other rehearsal-free continual learning approaches?)."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper introduces HyperAdapter, which leverages hypernetworks to generate task-specific adapters for pre-trained models, addressing data privacy concerns and enabling effective knowledge transfer while requiring fewer additional parameters as the number of tasks increases. However, the core idea of using hypernetworks to generate model parameters for continual learning has already been extensively explored in the literature [1-6], and this paper merely applies these existing methods in the context of prompting-based continual learning with pre-trained models, which significantly limits its novelty and contribution. Several of the innovative designs introduced, such as block-wise hyper-adapters, bear strong similarities in motivation and methodology to chunk embeddings and network partitioning discussed in [1], further constraining the novelty of the work. Additionally, one of the claimed main advantages—eliminating the necessity of knowing the task identities during inference—was previously addressed in [1] under the concept of unknown task identity inference, and the query-key matching mechanism used to address this issue is a well-established practice [7-9]."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "The paper addresses the problem of catastrophic forgetting in continual learning and introduces a pre-trained model-based continual learning framework, HyperAdapter, which utilizes a hypernetwork to generate adapters based on the current input, adapting the pre-trained model to the corresponding task. A key to the method is that HyperAdapter uses representative features from pre-trained models, eliminating the necessity to know the task identities during inference or the dependence on any rehearsal buffers. There is currently a lot of interest in avoiding catastrophic forgetting in the continual learning setting, and the authors have summarised and categorised the main approaches. However, the paper discusses the main approaches at a high level and fails to clearly describe the key novelty of the proposed approach. Additionally, the comparison of the proposed method with how people learn is repeated in a number of places, but the points around this are well known and widely documented. Removing the replication provides space to describe the novelty in more detail and room to discuss the implications of the results."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "**Summary for Reviewer:**\n\nHyperAdapter is positioned as a novel method at the intersection of adapter-based and hypernetwork-based continual learning, using a hypernetwork to generate per-task adapters for a frozen pre-trained backbone, thereby aiming to prevent catastrophic forgetting with improved scalability and task-agnostic inference. While the authors claim to be the first to use hypernetworks to generate adapters in this context, this novelty is somewhat overstated, as very recent works (e.g., PHA) employ similar ideas, and several relevant adapter-based and hypernetwork-based baselines (SEMA, MoE-Adapters, PHA) are omitted from comparisons. The reported empirical gains over strong rehearsal-free baselines (DAP, EASE, CODA-Prompt) are promising, but the absence of some recent methods in the evaluation weakens the state-of-the-art claim. The architectural integration of hypernetworks, adapters, and input-conditioned task embeddings is a meaningful advance, but the conceptual delta is narrowing as the field rapidly evolves. Overall, HyperAdapter is a substantive contribution, but reviewers should interpret its novelty and empirical results in the context of an active, fast-moving research landscape with increasingly incremental advances."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper proposes HyperAdapter, a novel pre-trained model-based continual learning framework that uses a hypernetwork to generate adapters based on input, adapting a pre-trained model to specific tasks without a rehearsal buffer. The idea of using hypernetworks to generate adapters for continual learning is interesting. HyperAdapter outperforms existing methods and even exceeds multi-task learning upper bounds, setting a new state-of-the-art for pre-trained model-based continual learning."
      }
    ]
  },
  "Nx1XZWcLcW": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes a modality-aware mixture-of-experts architecture for pre-training mixed-modal, early-fusion language models, where image and text tokens are processed with two groups of expert modules in the FFN layer and expert selection is controlled by two types of routers. The technical novelty of the proposed method is limited, since grouping has been used in FFN layers, and previous works have separately processed image and text tokens, for example, using image encoders to process the image tokens. It requires further justification of the novelty of the proposed method, and I seek clarification of the technical novelty."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper introduces MoMa, a novel architecture for early-fusion pre-training of mixed-modal language models that utilizes modality-specific mixture-of-experts to enhance computational efficiency and model scalability across various multimodal tasks. I have some concerns regarding the novelty of the paper. To my knowledge, there have been numerous attempts to apply MoE architecture in MLLMs to achieve a balanced performance and efficiency. These attempts have explored two paradigms: shared modal experts (e.g., MoE-LLaVA [1]) and decoupled modal experts (e.g., CogVLM [2]), which correspond exactly to the MoMa 8x and MoMa 1t1i baselines. Given that these studies share a similar research motivation with MoMa, a comprehensive comparison with related works would better highlight the novelty of this paper."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces MoMa, a modality-aware mixture-of-experts (MoE) architecture tailored for early-fusion language models that process images and text in arbitrary sequences by dividing expert modules into modality-specific groups. While this approach represents a creative evolution in multimodal language processing by combining modality-specific experts that process either text or image tokens separately, the innovation in the methodology presented in this paper is limited. The proposed decoupled modality approach lacks originality, as it has already been introduced in Wang et al. (2023), and the use of mixture of experts (MoE) is not a novel contribution, as multimodal MoE has been previously established by Lin et al. (2024)."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission (MoMa) introduces modality-aware Mixture-of-Experts (MoE) with expert-choice routing into early-fusion, autoregressive, mixed-modal language models—a combination not previously explored, though the underlying techniques are well established in encoder or masked modeling contexts. The main technical novelty lies in adapting modality-aware MoE to the Chameleon-style early-fusion, autoregressive setting, representing a logical but incremental extension of prior work such as VL-MoE, VLMo, and EVE. The explicit division of experts into modality-specific groups and the use of expert-choice routing are minor architectural variations, with efficiency gains (e.g., FLOPs savings) over standard MoE baselines that are real but modest. The authors’ claims of being the first to apply modality-aware MoE in this context are accurate, but the distinction between “modality-aware” and “modality-agnostic” experts is less stark than implied, and the novelty may be somewhat overstated. Overall, the work is a routine adaptation of established ideas to a new architecture, offering incremental improvements rather than a conceptual breakthrough."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes MoMa, a novel architecture for pre-training mixed-modal early-fusion language models, which processes images and text in arbitrary sequences by dividing expert modules into modality-specific groups. However, the novelty of the proposed method is limited, as the core idea—to divide experts into modality-specific groups—is a straightforward extension of existing MoE architectures. The techniques used in MoMa, such as expert-choice routing and load balancing, are also well-established in previous work."
      }
    ]
  },
  "lpt4ADbacU": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "The paper proposes incorporating real-world constraints into multi-objective optimization tasks by converting the reward function into a newly introduced soft-hard function (SHF). The novelty of the proposed algorithm is arguable. The algorithm essentially converts the multi-objective optimization objective using the introduced SHF utility function to incorporate constraints, and the SHF utility ratio measures the coverage of the SHF Pareto frontiers. Beyond these introduced notions, components like random scalarization, the Bayesian Optimization acquisition procedure, and submodular optimizations, along with their theoretical results, are mostly off-the-shelf. Existing work by Zuluaga et al. (2016), as mentioned by the authors, focuses on sample-efficient identification of the $\\epsilon$-accurate Pareto set, which I consider a more principled and interpretable objective for sample-efficient algorithms. Additionally, the use of existing methods—especially the MOBO algorithm by Paria et al. (2019) and the GPC algorithm by Krause et al. (2008)—is not clearly stated in the main paper but deferred to the appendix. Another reference by Malkomes et al. (2021) is arguably classified as a Level Set Estimation (LSE) method, but from my perspective, it also addresses identifying sparse, diversified coverage of constrained optimal candidates."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper presents an interesting problem of multi-objective optimization considering soft and hard lower bounds. Perhaps the more interesting thing is the max-min framework to achieve robustness against user preference weights is novel, which might provide a new perspective of enhancing diversity in Pareto frontier search. The practical approximation using a two-step process and leveraging submodular optimization techniques is sound, as it offers a feasible solution to an otherwise intractable problem."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper presents a two-step framework for multi-objective optimization (MOO) that incorporates user-defined soft and hard bounds on objectives and generates a compact set of Pareto optimal solutions for human decision makers to review. This approach creatively incorporates the concepts of hard constraints and soft preferences into Bayesian optimization formulation, providing a more nuanced way to express decision-maker preferences. The two-step framework is more user friendly, as through sparcification of the originally found Pareto optimal solutions, it is easier for human decision makers to pick the solutions to their preferences, which suggests fair practical usage. However, the algorithmic novelty is fairly limited: for the dense sampling procedure, the primary modification is the incorporation of soft-hard functions (SHFs) into the acquisition function, while the rest largely follows existing techniques from Paria et al. (2019). The sparcification procedure follows standard active learning approach. While the problem formulation is novel and interesting, I do not think there is enough methodological innovation to meet the high standard of ICLR. MoSH addresses a practical challenge in MOO by providing a way to generate a small but diverse set of solutions that respect user defined constraints, which has great potential impact to MOO applications."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper studies multi-objective optimization, with an emphasis on incorporating soft and hard constraints into the optimization framework. The paper introduces a two-stage approach that first generates a set of potential solutions and then refines this set by leveraging algorithms for robust submodular optimization. The reviewers raised significant concerns regarding the strength and novelty of the contribution, and overall, the consensus among the reviewers was that this work does not meet the high bar for acceptance."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment reviews \"MOSH: Modeling Multi-Objective Tradeoffs with Soft and Hard Bounds,\" which introduces Soft-Hard Functions (SHFs) for explicit modeling of both soft and hard bounds in multi-objective Bayesian optimization (MOO-BO), aiming to better capture practitioner preferences. The submission integrates SHF-based utility modeling with a two-step process: dense Pareto sampling via Bayesian optimization, followed by robust submodular set selection for compact, actionable solution sets. While the explicit SHF framework is a substantive and interpretable extension over prior preference modeling approaches, the overall methodology—combining preference-based MOO, robust optimization, and submodular selection—largely builds on established techniques, with novelty primarily in their integration and the form of utility modeling. The empirical results demonstrate improved utility and compactness, though these gains may partly reflect alignment between the proposed metric and method. The assessment notes that claims of conceptual novelty are somewhat overstated, as related work on flexible preference modeling and robust set selection is not fully acknowledged, and the main contribution is a clear, practical synthesis rather than a fundamentally new paradigm."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a method to sample Pareto-optimal points using a utility function that transforms the original objectives based on hard constraints and soft preferences, employing a two-stage process combining Bayesian optimization and submodular maximization. The proposed method is a straightforward combination of existing techniques; the first stage is a standard Bayesian optimization, and the second stage applies a robust submodular observation selection method. The technical contribution is limited, as the theoretical results are not particularly strong—the regret bound in the first stage is a simple extension of Paria et al. (2020), and there are no new theoretical contributions in the second stage. Overall, the method does not present significant novelty or originality beyond combining existing methods."
      }
    ]
  },
  "9aIlDR7hjq": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents an approach of augmentation-conditioning, which leverages real images with data augmentations to create synthetic images that are both realistic and diverse, aiming to bridge the domain gap between synthetic and real data and enhance downstream classification performance without requiring extensive fine-tuning of the diffusion model. However, the technical novelty of the proposed method seems limited, as it mainly combines existing data augmentations, like Mixup, before inputting images into an existing diffusion model. More discussion of the method’s novelty is necessary, and additional experiments comparing the superiority of the proposed method with current works, such as recent tuning-free approaches for diffusion models, would strengthen the paper. While the method focuses on tuning-free augmentation approaches, there are low-cost tuning methods, such as using LoRA, that also deliver strong performance, raising the question of why tuning-free methods are prioritized when low-cost tuning options might achieve better results with only a minor increase in computational cost. The application scope of the proposed method appears limited, primarily for long-tail or few-shot classification, and the experimental validation seems insufficient to fully establish its contribution."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "The authors propose a frozen alternative to increase the diversity of synthetic training images generated by diffusion models, which conditions the diffusion model not only on few-shot images (done previously) but augmentations (novelty). The method itself is quite simplistic from a novelty perspective (simply adding augmentations to the conditioning). I would consider this a strength if the results were consistent and strong with a clear storyline for effective use-cases; however, I do not see this as being the case. I do not find the CFG scale experiments as adding significant value, as they are consistent with previous work and do not provide more interesting, surprising, or novel results, which in my opinion waters down the impact of the experimental section."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper demonstrates that conditioning the generation process on an augmented real image and a text prompt produces effective synthetic datasets, which benefit downstream tasks, particularly for long-tailed (LT) classification and few-shot classification. However, the technical novelty of this paper is unclear. The concept of combining both augmented images and text prompts seems useful for LT and few-shot classification but lacks novelty. If this approach is not technically original, the paper should at least show a broad variety of downstream tasks that benefit from it, which it did not. The contribution is not clearly articulated, as it’s evident that the synthetic dataset is effective, but it’s unclear for which specific tasks it is most useful, with the focus confined to LT and few-shot classification. Expanding the application scenarios would improve the paper’s impact."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission introduces \"augmentation-conditioning,\" a method that conditions a frozen, pretrained diffusion model on augmented real images and text prompts to generate synthetic data for classification tasks, aiming to improve diversity and in-domain quality without fine-tuning. The approach is most closely related to SDEdit (He et al., 2023) and DA-Fusion (Trabucco et al., 2023), but differs by using augmented (rather than unmodified) real images as conditioning and by avoiding any generative model training. The main technical contribution is incremental: leveraging augmentations as conditioning is a logical extension of prior real-image guidance methods, yielding practical gains in diversity and efficiency, though the conceptual novelty over SDEdit and feedback-guided approaches is limited. The authors' claims of improved diversity and efficiency are generally accurate, but the uniqueness of their approach is somewhat overstated, as similar benefits are achieved by related methods with different trade-offs. Overall, the work offers a meaningful, efficient adaptation in a rapidly evolving field, but should be viewed as an incremental rather than a radical advance."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents a simple method to improve the diversity of generated images with the goal of increasing their effectiveness to train downstream image classification models. While the proposed method is simple and easy to follow and achieves good performance on various benchmarks, I do not find experiments to support the claims of improved diversity or effectiveness of synthetic images for training classifiers. The authors claim good performance without fine-tuning the image generation model, but I do not see comparisons with prior methods such as Fill-Up [1] that fine-tune and achieve better performance. Similarly, the claim of superior performance on long-tail classification tasks lacks experimental comparison with existing methods [2,3,4] specifically designed for these tasks. Overall, the novelty and contributions of the proposed method over prior work are unclear without these comparative evaluations."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper proposes a new method of generating synthetic training images by conditioning the generation process on an augmented real image and a text prompt. The idea of using augmented real images as a reference to guide the generation process is interesting. However, the paper lacks a comparison with a strong baseline that fine-tunes the diffusion model on the target dataset using a method like textual inversion, which makes it difficult to assess the true effectiveness of the proposed method."
      }
    ]
  },
  "pdzHpQbGrn": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents a framework for prompt learning in vision-language models, addressing the active test-time adaptation problem by proposing a dynamic threshold selection algorithm that identifies the most valuable samples for adaptation. While the paper presents performance improvements and a novel framework for vision-language models, its novelty is somewhat limited, specifically restricted to the threshold adaptation method. I noticed that an active prompt-learning method exists, but it is not compared in this paper."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper studies the active test-time prompt tuning for the vision-language model and proposes a novel test-time prompt tuning method, outperforming existing test-time prompt tuning methods. The integration of active learning with test-time prompt tuning presents a novel approach. However, previous studies have explored the benefits of employing active learning for test-time adaptation, and the novelty of this paper is marginal. The active test-time adaptation, test-time prompt tuning, and data buffer are not novel and have been proposed by existing studies [1-3]. Although some studies have been discussed in this paper, the major technical improvement of this paper has not been highlighted."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper explored the intersection of test-time adaptation (prompt tuning) and active learning for adapting foundation models, proposing an uncertainty-based thresholding method to improve the existing test-time prompt tuning (TPT) method, along with a class-balanced buffer-maintaining technique and class-wise representation alignment. However, the considered modification is not novel at all as the thresholding technique and the class-wise treatment are just trivial engineering extensions of an existing method. The authors directly adopt the MaPLe framework as a learning target model and adopt the PromptAlign loss (which is constructed with the combination of TPT loss with feature alignment loss) with two modifications—entropy-based filtering and class-wise alignment with balanced buffer. If the author wants to claim novelty here, they should provide at least their unique motivation for leading these components in the active test-time prompt tuning framework whether theoretically or empirically. However, the authors do not provide these, and the motivation for the thresholding method is not solid. Overall, the improvements noted in the tables are very marginal (when it comes to the comparison with baseline or ablation studies), which makes me speculate about the effectiveness of each component in the proposed method and the framework itself."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission \"Active Test Time Prompt Learning in Vision-Language Models\" (ATPL) is positioned at the intersection of test-time prompt adaptation, active learning, and distribution alignment for VLMs, and is the first to integrate active querying and class balancing at test time in a streaming, single-sample setting. The authors’ characterizations of prior work are generally accurate, though some recent related works (e.g., C-TPT, DPCore, VPTTA) are omitted, which may understate the field’s maturity. The main technical contributions—dynamic thresholding for label querying, class-aware alignment, and buffer management—are logical extensions of existing methods, with the primary novelty being their integration in this specific test-time context. While the \"first\" claim is accurate in a narrow sense, the conceptual advances are incremental, reflecting a broader trend in the field toward parameter- and label-efficient adaptation. Reviewers should recognize the submission as a well-motivated and carefully executed integration of established techniques, with its main value in the adaptation and combination of known ideas rather than in introducing fundamentally new concepts."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a novel method that combines active learning and prompt tuning for test-time optimization of VLMs. The method incorporates a dynamically adjusted threshold and a class-balanced replacement policy to improve the selection of samples for active labeling. The paper provides a fair evaluation protocol for the proposed method and compares it with existing methods on multiple datasets."
      }
    ]
  },
  "jCNRcHrfLo": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes two modules to enhance the standard Faster R-CNN-based paradigm for open-vocabulary object detection: one that calibrates the original textual features with those derived from superclass features, and another that leverages visual context information from images to further refine the text embeddings. Enriching textual description of the categories beyond a single word name is not a novel idea, as previous work like [R1] has already proved the effectiveness of it. Instead of improving the textual description, the paper involves another module to calibrate the original textual feature for classification, which enrolls more computation overhead and is less elegant than directly finetuning the existing textual encoder. Compared to methods that utilize enriched information from large language models (LLMs), the use of superclasses from a word tree offers limited enhancement to the textual features, providing only modest complementary information. In the experiments, both modules show only limited improvements in the essential AP-novel metric, which is critical for evaluating generalization in open-vocabulary detection."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes HiCA, a novel approach for open-vocabulary object detection. In HiCA, Hierarchical prompts leverage coarse-grained superclass knowledge to avoid biasing the model towards base classes, and Context-Aware calibration revises detector results by learning category distribution through environmental context knowledge. The idea of using Hierarchical prompts to capture the shared knowledge between both base and novel classes is interesting and proven. Experimental results demonstrate that HiCA consistently outperforms state-of-the-art methods in detecting novel objects."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper introduces a novel approach, Hierarchical Prompts with Context-Aware Calibration (HiCA), to enhance Open Vocabulary Object Detection (OVD). HiCA improves generalization by utilizing hierarchical prompts that map object regions through both coarse- and fine-grained knowledge, capturing shared information across base and novel classes. Additionally, context-aware calibration refines detection by linking contextual information with object categories, reducing background interference. Despite the emphasis on enhancing detection for novel classes in OVD, the improvement shown in Tables 1 and 4 is relatively minor, and most modifications do not outperform simpler, template-based methods for novel classes, which raises concerns about the practicality and overall impact of the proposed approach for new class detection."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper proposes Hierarchical Prompts with Context-Aware Calibration (HiCA) to improve open-vocabulary object detection (OVD), introducing two modules: Hierarchical Prompts (HP) that use superclass information to enrich textual representations and establish shared knowledge between base and novel classes, and Context-Aware Calibration (CAC) that refines classification scores by linking contextual image features with category distributions. The approach does not introduce substantial innovation compared to prior works leveraging textual enrichment or contextual features. Several reviewers pointed out that similar ideas, such as using LLM or superclass information, have been explored in prior research. During the rebuttal, while the authors addressed some questions raised by reviewers, the core concerns on its originality remain."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission introduces HiCA, a method for open-vocabulary object detection that combines explicit hierarchical prompts (superclass/class) with context-aware calibration via unsupervised visual context clustering and a distribution generation layer, all within a prompt learning framework. While the technical implementation—particularly the use of hierarchical prompts and unsupervised context clustering for calibration—is novel, the underlying conceptual ideas of leveraging semantic hierarchies and contextual information are well-established in recent literature (e.g., HierKD, DetCLIPv3, HTRPN). The authors tend to overstate their novelty, underrepresenting the sophistication and relevance of prior work that already integrates hierarchy and context, albeit through different mechanisms such as distillation or generative modeling. The main delta lies in the specific combination and technical realization of these ideas within prompt learning, rather than in a fundamentally new conceptual advance. Reviewers should note that, in this mature and rapidly evolving field, most recent contributions—including this one—are incremental and technical, and direct comparison to the most relevant recent works is needed to fairly assess the true contribution."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes to use a superclass prompt in addition to a class prompt for open-vocabulary object detection, where the superclass prompt is a learnable vector used to compute a coarse-grained similarity score with region features, and the class prompt combines a learnable visual prompt and a learnable text prompt. However, the idea of using a superclass prompt is not new. For example, [1] uses a superclass prompt and a class prompt, and [2] uses a superclass prompt, a class prompt, and a hybrid prompt. The authors should cite and discuss these two papers. Experiments show the proposed method outperforms existing methods, but the performance improvement is not significant; for instance, Table 1 shows only a 0.2 mAP improvement over BARON on novel classes, and Table 3 indicates the context-aware calibration has a negative impact on the base classes."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper proposes hierarchical prompts with context-aware calibration (HiCA) for open-vocabulary object detection, leveraging both coarse-grained superclass knowledge and fine-grained category knowledge to improve generalization to novel classes while utilizing the visual context of the image to enhance these effects. However, I find that the proposed hierarchical prompts method is quite similar to the learnable multi-modal prompts method in [1], where multiple learnable text tokens are also used to prompt the text encoder for generating text embeddings. While the authors do not use learnable visual prompts as in [1], I still think the proposed hierarchical prompts method is an incremental work based on [1]. The originality of the method appears limited by this similarity, and a comparison experiment with [1] is necessary to better establish the specific contribution."
      }
    ]
  },
  "aueXfY0Clv": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The proposed Depth Pro model employs a ViT architecture for zero-shot metric monocular depth estimation, targeting applications such as novel view synthesis. While Depth Pro benefits from pretrained ViT backbones, its architecture primarily builds on existing elements rather than introducing fundamentally new mechanisms for depth estimation, which limits its architectural novelty. The paper introduces a two-stage training approach that integrates synthetic and real-world datasets, enhancing depth boundary accuracy, and includes new metrics for evaluating depth boundaries that address a gap in existing benchmarks by focusing on boundary precision, which is critical for applications like view synthesis that demand fine details."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper introduces a foundational model for single-image depth estimation, trained on a large collection of datasets, resulting in high-resolution depth maps with sharp object boundaries and enabling the estimation of camera intrinsic parameters. Although effective, the proposed method's novelty is somewhat constrained as it leverages existing approaches; the loss functions and training strategy draw heavily from DPT (Ranftl et al., 2022), and the metric depth estimation follows ideas from Metric3D (Yin et al., 2023). The contributions of this work are significant, with the pre-trained model holding substantial potential for a range of downstream tasks, but more discussion on the unique aspects would strengthen the paper."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents an approach whose contributions were initially questioned for limited novelty by reviewers eL31 and a8Yt. The authors clarified the contributions of their work in the rebuttal, particularly emphasizing the innovative aspects of their approach. While the critical components influencing performance and boundary accuracy among the technical contributions were not clearly highlighted at first, the authors included additional ablation studies to clarify the importance of specific components. Overall, the paper makes a valuable contribution to the field."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "Depth Pro is presented as a foundation model for zero-shot, high-resolution, metric monocular depth estimation without requiring camera intrinsics, emphasizing sharp boundaries and fast inference. While the submission claims several firsts—including zero-shot focal length estimation and new boundary evaluation metrics using matting datasets—many of these contributions are incremental extensions or empirical integrations of strategies already present in recent works such as UniDepth, DMD, Metric3D v2, PatchFusion, PatchRefiner, and SM4Depth. The main substantive advance appears to be the combination of speed, sharpness, and metric accuracy in a single model, with the use of matting datasets for boundary evaluation being a modest but useful addition. However, the claim of being the \"first foundation model\" for this problem is overstated, as several recent models address similar challenges, sometimes with different technical means. Reviewers should recognize Depth Pro as a strong empirical advance within a rapidly evolving field, but should calibrate novelty claims in light of the incremental nature of most contributions and the omission of some highly relevant prior work."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a new method for zero-shot monocular depth estimation that is efficient, fast and accurate, and is also capable of predicting metric-scale depth, which is a challenging task. The experiments demonstrate the superiority of the proposed method over the existing methods."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper presents a foundation model for zero-shot metric monocular depth estimation that is capable of producing metric depth maps with absolute scale on arbitrary images without requiring metadata such as camera intrinsics. The model's ability to produce metric depth maps with absolute scale on arbitrary images without requiring metadata such as camera intrinsics is a significant advancement in the field of monocular depth estimation. The high-resolution output and fast processing time make it suitable for real-time applications. The model's superior performance in sharp delineation of object boundaries and focal length estimation across multiple datasets demonstrates its robustness and accuracy."
      }
    ]
  },
  "nrvoWOWcyg": {
    "human": [
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes a novel text generation method called CD-LM, which combines a language model with a simple structured retrieval module to generate multi-token text chunks in a single decoding time step. However, CD-LM completely follows the Copy Generator framework proposed in the paper \"Copy is all you need\", and its novelty is slightly insufficient. Nevertheless, the authors successfully extend the application scenarios of CoG by enabling the retrieval and utilization of chunks constructed by stronger models, the model itself, or high-quality human data, thus improving generation efficiency and quality in various ways. Additionally, there is a lack of important reference to the work \"Nearest Neighbor Speculative Decoding for LLM Generation and Attribution\", which also applies chunk-level generation mechanisms to speculative decoding for improved efficiency; the authors need to clarify the main differences between their method and this prior work in terms of inference speed and generation quality. Despite these concerns, I think the contribution of this paper is still good due to its effective extension of existing frameworks to new application scenarios."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper presents a chunk-distilled LM, where the authors incorporate retrieved chunks in the generation phase, and the chunk knowledge can be distilled by self-distillation, from larger models, or from experts. In general, the paper does not present much novelty, as it follows previous copy/retrieval-based LMs and the main difference is using chunks as the granularity."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper proposes a new decoding technique called Chunk-Distilled Language Modeling (CD-LM), which introduces a novel phrase-level retrieval mechanism to enable phrase-level autocompletion during decoding. The authors introduce a new decoding paradigm that is quite novel, allowing for fine-grained, phrase-level grounding to an external source without incurring any extra context length like RAG does, while potentially saving decoding time by skipping decoding steps. In essence, CD-LM introduces a phrase-level cache with fuzzy matching, giving the model the ability to autocomplete from the cache with some confidence threshold. The flexibility of being able to fill the cache with anything is a powerful paradigm, as demonstrated in the paper. Unlike speculative decoding, this method can change the sampling model's output distribution, potentially for the better or to enable novel applications, and unlike retrieval augmentation, it does not increase the context window length. The paper also introduces a novel method to compute perplexity given this phrase-level retrieval insertion mechanism. I believe this efficient, granular grounding technique will be of value to the community and will inspire future work, with many ways to extend it or make it more practical for production. Even as-is, there are specific use cases that would likely benefit from such an approach, such as private information retrieval, where it outperforms RAG-based few-shot alternatives, and as a novel way to distill performance from a larger model during only decoding, without any online decoding cost."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "**Summary for Reviewer:**\n\nCHUNK-DISTILLED LANGUAGE MODELING (CD-LM) proposes a training-free, chunk-based retrieval-augmented language model aimed at improving inference efficiency and enabling plug-and-play knowledge or domain adaptation without retraining. The method is positioned as a generalization of chunk-based retrieval from translation to language modeling, combining elements from recent works like NEST, REST, and COG, but distinguished mainly by its specific implementation (e.g., trie-structured datastore, direct context matching in LM space). While the authors claim significant novelty, the core ideas—training-free, chunk-level retrieval and flexible datastore swapping—are already present in closely related recent work, making CD-LM an incremental rather than a fundamental advance. The main contribution is the practical integration of these elements in a general-purpose LM framework, with empirical efficiency gains that are typical of chunk-based methods. Reviewers should note that the authors somewhat overstate their uniqueness, omit some relevant prior work (e.g., RETRO), and that the field is rapidly evolving with many similar incremental contributions."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a method to cache frequently generated text chunks in a trie structure and reuse them during generation to improve both efficiency and performance. The idea of caching frequently generated chunks is reasonable and has the potential to improve both efficiency and performance. The method is applicable to different scenarios: distilling knowledge from stronger models, improving efficiency for the same model, or injecting external knowledge."
      }
    ]
  },
  "2prShxdLkX": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper introduces MoDGS (Monocular Dynamic Gaussian Splatting), a novel approach for rendering dynamic 3D scenes from casually captured monocular videos by incorporating depth supervision into the domain of dynamic Gaussian Splatting (DGS) for monocular dynamic input. This approach is novel yet intuitive, filling a key gap in the field for cases where the input consists of casually captured videos with minimal camera movement. Compared to other papers in the field that mechanically combine various complex input feature streams or loss functions, the proposed solution is conceptually straightforward but impactful, pushing forward the capabilities of monocular dynamic scene reconstruction. The method introduces a novel ordinal depth loss to address depth inconsistency in single-view depth maps, enhancing the robustness and continuity of 3D scene reconstruction, and the results clearly show MoDGS’s robustness and superiority over baseline methods, adding confidence in its effectiveness."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper introduces MoDGS, a method for dynamic view synthesis from monocular videos that leverages a Gaussian-based splatting technique combined with deformation fields and an ordinal depth loss to reconstruct scenes. I think the 3D-aware initialization process is a strong point, as it specifically addresses a common issue in monocular reconstruction by initializing Gaussians instead of relying on random initialization, potentially adding more consistency. The ordinal depth loss is, in my view, an interesting idea that tries to tackle scale ambiguity in monocular depth estimation and promotes depth consistency across frames, which is particularly relevant in dynamic scenes. However, I think the innovation is quite incremental for the reason that compared to closely related works like Deformable 3DGS and 4DGS, the methodological innovation appears incremental, mainly optimizing existing elements (depth consistency and deformation) rather than proposing a new structural approach. Besides, the approach relies heavily on pre-trained depth models, leveraging external models as priors, which potentially limits its novelty and raises questions regarding knowledge distillation. While MoDGS integrates external depth estimation for initialization, there is no formalized knowledge distillation to adaptively refine the model during training, and this absence may reduce the adaptability of MoDGS across different dynamic scenes."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents MoDGS, a dynamic scene modeling algorithm using casually captured monocular videos that integrates single-view depth estimation, 3D-aware initialization, and robust ordinal depth loss to overcome the rapid camera motion assumption in previous methods. The proposed components are technically novel and interesting, with the 3D-aware initialization improving the consistency of the initial Gaussians and the ordinal depth loss addressing the problem of depth consistency across frames. While the method builds incrementally on existing approaches such as deformable 3DGS by optimizing depth consistency and deformation, its performance depends significantly on external single-view depth estimators, which may limit the algorithm in challenging conditions. All reviewers agree that the technical contribution of the proposed algorithm is significant."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The paper proposes MoDGS, a novel pipeline to render high quality novel views of dynamic scenes from casually captured monocular videos, specifically addressing the limitations of existing methods for such videos by reducing the dependency on rapid camera motion. MoDGS represents an original approach within novel view synthesis and dynamic scene modeling, introducing a 3D-aware initialization mechanism and an innovative ordinal depth loss that maintains depth order among frames rather than relying solely on absolute values. This novel use of ordinal depth loss offers a new perspective on addressing depth consistency issues, with practical implications for improving depth coherence in dynamic scenes captured casually. The method’s ability to handle both static and dynamic elements in monocular videos opens new avenues for monocular depth estimation and dynamic scene modeling, where single-camera approaches have been historically constrained by depth inconsistency issues. By enabling high-quality novel view synthesis from single-camera footage without multiview camera motions, MoDGS broadens the scope of dynamic scene reconstruction and makes it accessible to a wider range of use cases."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "MoDGS is an incremental contribution within the dynamic monocular Gaussian Splatting (GS) field, targeting novel view synthesis from casually captured monocular videos with static or slow-moving cameras—a problem also addressed by recent works like MoSca, Dynamic Gaussian Marbles, and Shape of Motion. The main technical advances claimed are a \"3D-aware\" initialization scheme and the integration of an ordinal depth loss, which may improve stability and depth consistency but are refinements of existing ideas rather than conceptual breakthroughs. The authors' claims of being the \"first\" to solve this problem are overstated, as several concurrent works address similar settings with related techniques, and the distinctions are primarily in implementation details. Prior work is sometimes characterized as less capable than it may actually be, and the novelty of MoDGS lies in the specific adaptation and integration of known methods rather than in the overall approach. Reviewers should focus on the empirical significance of these refinements and the thoroughness of comparative evaluation with the most relevant recent methods."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents MoDGS, a method for dynamic scene reconstruction from a monocular video that proposes 3D-aware initialization and an ordinal depth loss to improve performance. The proposed method is a straightforward extension of the deformation-based dynamic Gaussian splatting methods, such as Deformable 3DGS and CogS, and the technical novelty of this paper is limited. While the proposed 3D-aware initialization and ordinal depth loss are intuitive and effective, the overall contribution appears incremental."
      }
    ]
  },
  "JytL2MrlLT": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This study considers learning on tabular data, and proposes ModernNCA -- a deep version of the classic Neighborhood Components Analysis algorithm, where the transformation is non-linear and powered by a neural network. The SNS strategy looks simple and effective, and also differentiates the method from TabR. The proposed NCA extensions are not conceptually novel, so I believe that the story on the first six pages could be more compact. There is a missing related work: \"Improving Generalization via Scalable Neighborhood Component Analysis\" ECCV 2018, which also describes how to efficiently train a deep NCA, and I think their method is more advanced than the one proposed in this submission. Though their method can be too complicated for the scope of this paper, I recommend discussing this related work and explaining why the proposed SNS is a better choice for this work compared to the method from the referenced paper."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper revisits the Neighborhood Components Analysis (NCA) and adapts it for tabular data learning, proposing ModernNCA as an enhanced approach. While the approach effectively leverages modern deep learning techniques to enhance classical NCA and demonstrates strong empirical results, the core modifications—using a representation space for distance calculations, employing SGD, and mini-batch training—have already been explored in prior research. This raises concerns regarding the originality of the contribution, as the changes appear more like tunings of established techniques rather than introducing a fundamentally new method."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper explores the potential of modernizing the classical Nearest Neighbor approach for tabular data by leveraging a differentiable K-nearest neighbors variant, Neighborhood Components Analysis (NCA), and introduces MODERNNCA, an improved version of NCA that integrates deep learning techniques such as stochastic gradient descent (SGD), nonlinear embeddings, and a Stochastic Neighborhood Sampling (SNS) strategy to boost computational efficiency and model performance. Revisiting a classic nearest-neighbor approach with contemporary deep learning techniques is a novel approach, particularly since NCA had been previously limited by computational efficiency and scalability, and this approach aims to unify insights from both traditional and modern tabular prediction methods. However, the paper’s contribution feels incremental rather than pioneering, as the improvements in MODERNNCA rely on established techniques (SGD, SNS, and nonlinear embeddings) without introducing a fundamentally new concept or method, making the novelty limited and essentially optimizing an existing algorithm rather than providing a unique advancement. The paper also lacks direct comparisons with other KNN-inspired deep learning methods that have similarly benefited from modern optimization strategies, which weakens the argument for MODERNNCA’s distinctiveness and impact."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper explores more traditional approaches for supervised tabular data modelling integrated with modern deep learning techniques, using a Neighbourhood Components Analysis approach in the latent space of a neural network which can be optimized with SGD. The primary outstanding concern from several reviewers, but not all, was around novelty, in that this work revives old techniques through a modern lens. In my view this approach is sufficiently novel, at least compared to the typical paper accepted at ICLR, and considering the positive empirical results I am recommending acceptance as a Poster."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission presents MODERNNCA, a deep learning method that modernizes classical Neighbourhood Components Analysis (NCA) for tabular data by introducing deep neural projections, feature transforms, and, most notably, stochastic neighborhood sampling (SNS) for efficient training. While the approach is well-executed and empirically strong—matching or outperforming state-of-the-art baselines on large benchmarks—the core methodological advances are incremental, with the main technical novelty being the SNS strategy for scalable training. The work is closely related to recent retrieval-augmented models like TabR and LoCalPFN, and the distinction from these is primarily in training efficiency and the explicit use of the NCA loss, rather than a fundamentally new modeling paradigm. The authors’ claims of \"revitalizing\" nearest neighbor methods are somewhat overstated, as similar approaches already exist, and much of the empirical gain may stem from careful engineering and tuning. Reviewers should note that while MODERNNCA is a strong new baseline, its conceptual delta over recent retrieval-augmented methods is moderate, and a more direct comparison with closely related work would further clarify its unique contributions."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper revisits the classic KNN method and explores ways to improve it using modern deep learning techniques. The proposed MODERNNCA is a deep learning version of KNN, which achieves good performance in both classification and regression tasks, essentially serving as a strong deep baseline for tabular tasks. However, the idea of using a deep learning version of KNN for tabular data is not novel, as it has been explored in previous works like TabR."
      }
    ]
  },
  "pbDqZBn2X2": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes a new Cost-Aware Diffusion solver for combinatorial Optimization (CADO) method via RL finetuning. Overall, the method is really simple, and lacks a strong sense of novelty. In two-phase training, the first phase is the same as DIFUSCO, and the second phase is just adoption of the existing decoder and RL algorithm. The proposed method incorporates the cost information and the decoder, which enhance the overall performance with significant efficacy, but the approach does not present a substantial departure from existing methods."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper introduces CADO, a novel framework that integrates Reinforcement Learning (RL) fine-tuning with diffusion models to address Combinatorial Optimization (CO) problems such as the Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). The methodological innovations are perceived as incremental, primarily extending existing RL and diffusion model techniques without introducing substantial novel contributions. Comparisons with closely related methods, particularly T2T, are insufficiently addressed, raising concerns about the relative significance of CADO’s improvements. The reliance on multiple hyperparameters and the lack of clarity in explaining key design choices further obscure the paper’s contributions, making it difficult to assess the true impact and scalability of CADO in diverse CO scenarios."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper proposes a cost-aware diffusion solver (CADO) for combinatorial optimization and a fine-tuning framework based on reinforcement learning for diffusion models in this domain. To the best of my knowledge, current techniques using diffusion models as well as RL fine-tuning are not common in combinatorial optimization, and this paper provides a detailed open-source implementation that serves as a baseline for subsequent similar studies. However, the innovation of the method is relatively low; the method of training and fine-tuning is very simple, which is slightly lacking as the innovation point of this paper. I do not see the authors' approach as a significant change from existing algorithms based on reinforcement learning fine-tuning. The method of supervised learning and fine-tuning is more like a trick than sufficient to support the innovation of the article, and taking feasible punishment and optimization goal as reinforcement learning reward is a very simple idea in the field of combinatorial optimization. There is no obvious groundbreaking in addressing the difficult problems in combinatorial optimization, and I am still not sure why this framework is necessary or what necessary adjustments have been made to ensure its applicability to combinatorial optimization problems."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "CADO is a diffusion-based solver for combinatorial optimization that distinguishes itself by integrating reinforcement learning (RL) fine-tuning and explicit cost-awareness, as well as incorporating the postprocess decoder into the training loop—features not present in prior diffusion-based approaches like DIFUSCO. The main methodological novelty lies in combining diffusion models with RL fine-tuning for direct cost optimization, which is new in the context of combinatorial optimization, though similar ideas (e.g., fine-tuning generative models with RL) exist in other domains and in works like BRAID (not cited). The authors accurately characterize the limitations of prior supervised learning-based solvers and demonstrate empirical improvements on both synthetic and real-world benchmarks, but the source of these gains (conceptual vs. implementation) is not fully disentangled. Some relevant related work (e.g., BRAID, DeepACO) is omitted, and certain claims about the limitations of prior RL-based solvers (e.g., DIMES) lack detailed quantitative support. Overall, CADO represents a meaningful and timely advance in neural combinatorial optimization, but reviewers should be aware of incremental aspects, omitted comparisons, and the need for more thorough ablation to clarify the origins of its improvements."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes CADO, a novel 2-phase training framework for diffusion models in combinatorial optimization (CO), incorporating Reinforcement Learning (RL) fine-tuning. The main contribution is that the authors introduce reinforcement learning to fine-tune the pre-trained diffusion model; however, fine-tuning the pre-trained diffusion model is not a novel idea and has been proposed in many papers, such as [2]. While the paper claims novelty, the proposed method is incremental, and similar approaches have already appeared in the literature. Additionally, the challenge of decoding strategies during training has been addressed by many previous papers, such as [1]. The paper would benefit from comparisons to more recent works, such as [3] and [4], to clarify its contribution and novelty."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "The paper introduces CADO, a framework that combines supervised learning (SL) pre-training and reinforcement learning (RL) fine-tuning for large-scale combinatorial optimization (CO) problems. However, the novelty of the proposed approach is limited, as it combines existing techniques in a straightforward way. The use of RL for fine-tuning is not new, and the paper does not provide a clear motivation for why this particular combination of SL and RL is superior to other possible approaches."
      }
    ]
  },
  "JHoC430Nxi": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper proposes CTNet, which combines CNN and Transformer architectures to extract hybrid backbone features for 6D pose estimation with low computational complexity. However, the primary contribution of this work appears to rely heavily on the universality of an improved backbone rather than on advancing pose estimation-specific knowledge. While leveraging a stronger backbone has been shown in prior studies to enhance model performance across tasks, this work does not provide a substantial methodological or conceptual contribution specifically tailored to the pose estimation domain. The method integrates several high-performing modules from existing backbones without introducing substantial innovations in either architecture design or task-specific adaptations. Merely combining known techniques, albeit effectively, does not present a strong enough level of technical novelty or innovation, limiting its impact. Furthermore, the paper lacks a thorough discussion and comparison with other CNN-transformer hybrid networks specifically designed for pose estimation, such as DFTr, and does not include comparisons with state-of-the-art methods in the pose estimation field."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper considers the task of 6DOF object detection from RGBD images with CNN+Transformer architectures. The main contribution is the development of the architecture, with various design choices that optimize for efficient compute, including the use of pyramidal transformers and partial convolutions. The idea of building transformers on top of CNN features has been widely explored in the community (SWIN transformers, Vision Transformers for Dense Prediction), and it seems like the design choices explored here serve mainly to reduce the number of parameters and latency by 2X. I do not view such reductions as sufficient for publication (though I admit the error reduction is significant). The paper is missing references to well-known and state-of-the-art work in this space such as FoundationPose [CVPR 2025], whose empirical performance is similar to the presented work. This paper clearly wants to position itself as an architecture paper, but the writing is so convoluted that it makes it hard to articulate the key architectural novelty. I would like the authors to clearly articulate the technical novelty of the proposed hierarchical feature extractor \"HFE\" module that differs from past work."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper introduces a hybrid architecture combining CNN and Transformer for estimating object poses from a single RGBD image, with a Hierarchical Feature Extractor (HFE) proposed for extracting local features at low computational cost. The whole method has a very incremental novelty; the main novelty lies in the network architecture, however, it is a combination of many existing modules with minor improvements. As the idea of combining CNN and Transformer architectures for object pose estimation is not new, the first contribution of the work (L99-101) is not an actual contribution. The performance on the datasets is not state-of-the-art, which also confirms that the proposed architecture offers limited improvements over existing solutions."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper proposes a hybrid CNN and Transformer architecture to improve 6D object pose estimation with lower computational complexity. While the paper does achieve those goals, it relies on well-known developments in prior works and misses out on positioning with respect to important prior works with very similar aims and methods."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "CTNet is positioned as a hybrid CNN-Transformer network for 6D pose estimation, with its main technical novelty being the specific design of its Hierarchical Feature Extractor (HFE) module. However, the overall hybrid architecture and fusion strategy closely resemble recent works such as FGCT6D (not cited in the submission), making the claimed conceptual advances largely incremental rather than groundbreaking. The efficiency and adaptability claims are supported by quantitative results but are not unique, as many recent methods (e.g., FGCT6D, ES6D, SCCN) also target similar trade-offs and routinely introduce custom fusion modules. The submission tends to overstate its novelty and efficiency gap, while omitting direct comparison to the most methodologically similar prior works, which is a significant oversight. For a fair assessment, the paper should provide a more balanced discussion of related work and clarify that its main contribution is a new instantiation of a common design pattern, rather than a fundamentally new paradigm."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents a novel approach by combining CNNs and Transformers for 6D pose estimation, leveraging the strengths of both architectures. However, the novelty of the proposed method is limited. The authors primarily combine existing techniques (C2f, ELAN, PointNet, and PVT) to form their hybrid network. The contributions are incremental, with the main novelty seeming to lie in the integration of these modules."
      }
    ]
  },
  "miIE56qM10": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents a training-based method to estimate uncertainty in LLMs by training a fully connected layer on top of RoBERTa on the accuracy data of the base LLM and then performing a convex combination with another uncertainty estimation method. However, the mathematical formalization of overconfidence and underconfidence is not very novel since it is a known issue in the literature, and while formalizing it into a mathematical framework is a nice addition, it's not clear its purpose since it is discarded soon after its definition and not used in the following sections. The proposed Corrector is not novel since training an accuracy classifier is a known practice in the literature (e.g., p(IK), Accuracy Probes) as well as training a more advanced classifier like BERT. Framing uncertainty as a classification task is the standard practice in the literature, and several methods in this same class (trained methods) are not evaluated and compared. Overall, the paper lacks novelty and does not provide a significant original contribution beyond existing approaches."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces an approach that trains an external lightweight model to mitigate inherent biases in LLMs, such as overconfidence and underconfidence. While the proposed model demonstrates enhanced performance and generalization capabilities across multiple models and datasets, the novelty is limited by the fact that existing work has explored training an external calibrator specifically for LLMs. The paper proposes a post-processing calibration method that requires task-specific training data, but the baselines used for comparison are confidence elicitation techniques (pre-processing methods) that do not require training data, making the comparison potentially unfair. Established post-processing techniques like isotonic regression, Platt scaling, and recalibration with Bayesian neural networks should be considered, and the authors should cite relevant work, include applicable baselines, and justify any inapplicable baselines."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper focuses on uncertainty estimation of LLMs and proposes to introduce an external corrector for refining uncertainty estimation, which is a Roberta-based regression model predicting a value added to the uncertainty score estimated by baselines. However, Liu et al. [3] generated their training set in a manner similar to this work and introduced a calibration mechanism to adjust logits, reducing the novelty of the proposed data construction process. Additionally, Ulmer et al. [4] propose an external model to estimate the confidence of LLM outputs, closely resembling the proposed method, which also trains an external model for confidence prediction. This potentially diminishes the novelty of the external corrector."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission introduces a modular, external \"Corrector\" model that refines uncertainty estimates from large language models (LLMs) by post-hoc correction of outputs from existing methods, aiming to address issues like over/under-confidence and ranking inversions. The approach is positioned as orthogonal and complementary to current uncertainty estimation techniques, and is designed to be lightweight, model-agnostic, and easily integrated atop various base estimators. While the implementation is practical and flexible, the conceptual novelty is incremental, as similar learnable or ensemble correction models (e.g., MARS, LARS, LM-Polygraph) already exist in the literature. The authors somewhat overstate the uniqueness of their contributions and the limitations of prior work, particularly regarding inversion correction and robustness. Reviewers should focus on the empirical robustness and integration benefits of the Corrector, but recognize that the main advance is a practical extension rather than a paradigm shift."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a novel approach to uncertainty estimation in LLMs by training a corrector model to fix the miscalibration of logit-based uncertainty scores. The method is novel and addresses a significant challenge in uncertainty estimation for LLMs. The method can be integrated with various logit-based uncertainty methods, and the empirical results show consistent improvements on TriviaQA and SciQA."
      }
    ]
  },
  "6tyPSkshtF": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper provides gap-dependent regret bounds for Q-learning-like algorithms that use variance estimation and also achieve variance dependent regret, as well as an algorithm with a gap-dependent policy switching cost. The algorithms used (or small variations) appear in prior work, so the contribution appears to be somewhat limited, since it is a re-analysis of existing algorithms and the level of technical contribution of the analysis is not fully clear to me. The authors describe a novel error decomposition and a surrogate reference function technique (which assists in the application of concentration inequalities) as main analytical contributions. The regret bounds achieved by the paper improve upon those of prior works, and the gap-dependent analysis of the switching cost is new; I think it is interesting to expand gap-dependent analyses beyond the regret performance metric. It is very common in RL for the analysis of the same/similar algorithms to be gradually refined, but then I think it is very important that the authors do a good job highlighting the analytical improvements. I am somewhat unclear on the level of technical contribution of the paper, but it seems like the analysis techniques may be useful for future work involving reference-advantage decomposition algorithmic ideas."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This work studies the instance-dependent regret guarantee in tabular Markov Decision Processes and first proposes a novel algorithm that achieves a variance-aware regret bound with respect to the minimal sub-optimality gap. The author also proposes an instance-dependent policy-switching cost for the UCB-Advantage algorithm, which could be of independent interest. However, the improvement in this work over existing results appears too limited. In comparison to previous instance-dependent guarantees, this work achieves a variance-aware regret bound that improves by a factor of H even under maximum variance, but the instance-dependent regret bound depends on the minimal sub-optimality gap across all state-action pairs, which can lead to weaker performance since the sub-optimality gap often varies significantly. For the instance-dependent guarantee with zero variance, a similar result already exists without relying on the minimal sub-optimality gap assumption, and compared with previous results, this work demonstrates worse dependency on the episode length H and the sub-optimality gap. Regarding the gap-dependent policy-switching cost, the claimed improvement is minor, amounting to only log T rather than a factor of A when the optimal action set is small. Regarding technical novelty, the introduction of a surrogate reference function is claimed, but its importance is not clearly explained."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper analyzes the UCB-Advantage algorithm and a slightly modified version of the Q-EarlySettled-Advantage algorithms, providing gap-dependent regret bounds and switching-cost bounds for them. The analysis of \"gap-dependent bound + reference-based algorithm\" is novel and of interest to the RL theory study. The technique of introducing an auxiliary \"surrogate reference function\" via cut-off based on optimal value function and $\\beta$ to avoid non-martingale if using the last step reference function is new to gap-dependent bound. Similarly, the gap-dependent regret bounds of such algorithms provided in this paper are better than the gap-dependent bounds of the algorithms without references in the literature."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper gives novel gap-dependent variance-aware regret bounds, and provides an algorithm with a gap-dependent policy switching cost. These theoretical contributions could be of interest to the RL theory community."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission makes a substantive technical advance by providing the first gap-dependent regret and policy switching cost bounds for Q-learning algorithms that use reference-advantage decomposition and variance estimators, addressing a notable open problem in the field. The authors introduce a novel error decomposition framework and a martingale surrogate construction, which are necessary technical innovations enabling this new analysis. Their results extend prior work (e.g., UCB-Advantage, Q-EarlySettled-Advantage) from worst-case to gap-dependent regimes, and complement existing gap-dependent analyses for other algorithms (such as UCB-Hoeffding), but are distinct in their focus on reference-advantage Q-learning. While the \"first\" claims are accurate within this specific context, similar gap-dependent results exist for other RL algorithms, and the submission could more fully acknowledge related work in broader settings (e.g., linear MDPs, Markov games). Overall, the contribution is substantive and timely, with technical novelty that is nontrivial, though the conceptual approach builds directly on recent advances in the area."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper considers the problem of learning a tabular Markov decision process with a gap-dependent regret and claims to be the first to do so. I believe this is the first paper that provides a gap-dependent regret bound in this setting, as previous work only provided the trivial lower bound or worst-case $\\sqrt{T}$-type regret bound. The bound obtained in the paper is smaller than the trivial lower bound, and the bound obtained in the paper is better than the worst-case bound. The paper should provide a table that compares the results in this paper to the related work to make the contribution clearer."
      }
    ]
  },
  "VLuJL8cnGk": {
    "human": [
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes a training-free method to combine the benefits of 3D-free and 3D-based novel view synthesis (NVS) methods by leveraging pretrained NVS models for weak guidance and integrating this knowledge into a 3D-free approach. However, the novelty is weak and the method is not well explained. While a training-free method is a good motivation, the author still relies heavily on a pretrained NVS model. To cope with the complexity of real scenes instead of object-level synthesis, many works have already been proposed and the author should include them, such as ZeroNVS (CVPR 2024), MegaScenes (ECCV 2024), and Generative Camera Dolly (ECCV 2024)."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "The paper tackles the problem of novel view synthesis from a single image input of a scene by using textual camera view description, introducing an additional guidance image obtained through a pre-trained model Zero123++. The novelty of the work mainly consists of the combination of methods, whereas the actual novelty drags in the cost of extensive inference optimization. The main task of aligning the camera appears to be the result of the Zero123++ method, while the test-time optimizations generally improve the image quality. Due to this, it may be a bit dubious to stipulate object + background novel view synthesis as a contribution, as both the contributions could be attributed—at least in part—to Zero123++. The camera alignment largely appears to be performed by the guidance method Zero123++, which is trained on Objaverse and more importantly on the same task, so the method implicitly uses Objaverse via Zero123++, a competing method used in the proposed method."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper proposes to combine a \"3d-free\" method which uses CLIP and image guidance with a \"3d prior\", an existing model which has already been trained to do novel view synthesis (though on objects). The paper repeatedly says that their method does not use 3D data, but they directly use Zero123+ which is trained on 3D data. In the rebuttal, they clarify and say that they don't use \"additional\" 3D data and that Zero123++ is object-level; while this is true, it runs completely against how the current intro is written. The comparisons used in the paper are weak, and the evaluation avoids many more standard datasets such as CO3D, RE10K, Acid, or DTU. ZeroNVS should be a standard comparison, and the justification for not including it is inaccurate. Overall, the paper lacks clear originality and does not convincingly establish its contribution relative to prior work."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission presents an incremental but notable architectural advance in 3D-free, diffusion-based novel view synthesis (NVS) by explicitly integrating pretrained 3D priors (Zero123++) as weak guidance within a plug-and-play, inference-time optimization framework, aiming to improve camera control from a single image. The main technical novelty lies in the combination of 3D prior guidance with a CLIP-based regularization loss for angle embedding alignment, though both components individually build on well-established strategies in the field. While the authors claim significant empirical improvements over both 3D-free and 3D-aware baselines, the gains are more pronounced over weaker baselines, and the improvements over state-of-the-art are incremental and may be influenced by implementation details. The assessment notes some overstatements in the submission, such as inaccurately grouping DreamBooth with NVS methods and overstating the limitations of prior 3D-free approaches regarding viewpoint control. Overall, the submission’s primary contribution is in the integration and application context rather than in fundamentally new models or learning paradigms, and direct comparison with recent multi-view consistency modules would further clarify its impact."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper proposes a method to generate camera-controlled novel views from a single image by combining 3D-free and 3D-based approaches, which leverages the advantages of both. The paper provides a detailed analysis of the CLIP model's understanding of 3D space and the role of guidance images in 3D-free methods, which supports its solution. The paper presents both qualitative and quantitative results on synthetic and real images, demonstrating improvements over baseline methods in terms of text consistency and fidelity. However, the paper does not compare its method with other 3D-free methods such as Zero123, Zero1$^{2}3^{+}+$, and Free3D, which are relevant for novel view synthesis from a single image."
      }
    ]
  },
  "Rp3DjldbSc": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This work proposes an intent oriented, multi-turn, context-aware conversational search dataset, and the method proposed to construct this synthetic dataset is novel considering the multi-intent nature and contextual information for formulating natural language queries. The automated data construction approach also develops a dialogue discriminator model to control for dialogue quality and evaluates existing systems. The data construction method is novel and the manual and automated evaluations are comprehensive. However, some key related works are missing, such as ProMISe and ConvSDG, which are relevant recent datasets on conversational search with information-seeking queries and are critical to compare and distinguish ICConv contributions to this work. Likewise, some relevant query reformulation approaches for conversational search, such as ConvGQR and ConQRR, are not included for comparison, and the baseline comparisons are a bit outdated."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents ICConv, a large-scale conversational search dataset comprising over 100k conversations generated through an automated pipeline, aiming to address gaps in existing datasets by accommodating multiple user intents and integrating contextual dependencies across dialogue turns. While the authors claim that their method is \"novel,\" they do not provide a statistical comparison between the features of their dataset and those of other conversational search datasets, nor do they explicitly explain how their approach differs from previous works, such as recent research [4] that also trains smaller models to simulate user-engine interactions with more rigorous workflows and metrics for quality control. The unique contributions and novel aspects of this dataset remain unclear, especially since the paper does not clarify what specifically makes the dataset more \"real-world\" or complex compared to prior datasets, and lacks concrete examples, illustrations, or comparisons to demonstrate these advantages. The authors should better articulate their distinct contributions and novel aspects to more convincingly establish the originality and significance of their work."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper introduces a conversational search dataset, ICConv, and its construction method, highlighting the novel approach of considering multiple intents within a single keyword-based query. Authors point out that ConvTrans neglected that “keyword-based query usually corresponds to multiple natural language queries under different intents”, which is an important claim to support the significance of the proposed method. Both ConvTrans and ICConv are generated by extending keywords to NL and CNL questions with T5. While the key novelty of the proposed method is generating multi-intent questions via concatenating responses as inputs (section 3.2.2) and filtering the path (section 3.4), the novelty of the other components is minimal."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents a new synthetic multi-turn conversational search dataset and its construction method. The justification for this work, including comparison to prior works in terms of novelty, and the new challenges introduced by this dataset, is not clearly described."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the ICConv submission presents an incremental advance in automated conversational search dataset construction, integrating multi-intent query generation, context-aware session structuring, and a contrastive-learning-based dialogue discriminator for quality control. While the pipeline’s integration is somewhat unique, each component—such as query trees, reverse reformulation, and automated filtering—has clear precedents in recent works like ConvTrans, ConvSDG, BlendX, and MIntRec2.0. The submission’s claims of being the \"first\" large-scale, multi-intent, context-aware dataset are overstated, as several similar datasets already exist, and some are not cited or compared in the paper. The technical novelty is primarily in the specific combination and engineering of existing methods, rather than in fundamentally new concepts. Reviewers should note that the submission understates prior work’s capabilities and omits key recent contributions, so its contribution should be viewed as a routine extension within a rapidly evolving field."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper presents a new dataset for conversational search, which is a large-scale and high-quality dataset. However, the motivation for creating the dataset is not clear, as the paper mentions that previous datasets for conversational search either overlook the multi-intent problem and contextual information or create a biased dataset where all queries in a conversation are related to a single positive passage, but it is not clear how the proposed dataset addresses these issues. The paper does not provide any analysis or evidence to support the claim that the proposed dataset is not biased and takes the contextual information into account. Additionally, the paper does not provide any comparison between the proposed dataset and previous datasets for conversational search, so it is not clear how the proposed dataset is different from previous datasets and how it can benefit the research community."
      }
    ]
  },
  "WVVu6B8knx": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "The paper proposes Supervised Batch Normalization (SBN), a novel normalization technique aimed at enhancing the effectiveness of Batch Normalization (BN) for diverse data distributions by introducing “contexts” to normalize different subsets of data using multiple means and variances. However, the novelty of SBN may be limited, as it could be perceived as a specific case of Mixture Normalization (MN) or Mode Normalization (ModeN). Additionally, the cited literature lacks references from recent years, and SBN lacks comparison with recent methods, which further calls into question the originality and significance of the contribution."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes a supervised batch normalization (SBN) method that splits various modes using the dataset's contexts and builds a multi-mode BN to replace the traditional BN in the model. However, technical novelty is limited, as the core algorithm of the SBN has been discussed in mixture BN, and the innovation of this article lies only in using prior information to replace the original clustering center."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper proposed a supervised batch normalization, which actually has limited novelty. Similar supervision in batch normalization has been explored in [R1] Haifeng Xia, and Zhengming Ding. Cross-Domain Collaborative Normalization via Structural Knowledge. Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI), 2022."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission introduces Supervised Batch Normalization (SBN), a context-aware normalization method that extends Batch Normalization by grouping data into explicit (supervised) or implicit (clustering-based) contexts, aiming to address BN’s limitations with heterogeneous data and small batch sizes. While SBN is positioned as a general-purpose, easily integrated solution and demonstrates strong empirical results on standard and domain adaptation benchmarks, its core idea closely parallels prior work such as Feature Aware Normalization (FAN), Unsupervised Batch Normalization (UBN), and Local Context Normalization (LCN), which are not adequately cited or compared against. The main substantive difference is SBN’s explicit use of supervised or clustering-based grouping for normalization, but this represents an incremental generalization rather than a fundamentally new principle. Claims regarding seamless integration and pre-training mode identification are standard for normalization methods and do not constitute significant novelty. Reviewers should note that while SBN is a logical and practical extension with promising results, its conceptual contribution is incremental, and the omission of closely related context-aware normalization methods overstates its novelty."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper proposes Supervised Batch Normalization (SBN) to improve the performance of Batch Normalization (BN) for heterogeneous datasets, by applying BN with different statistics to data samples from different distributions. The idea is simple and easy to implement. However, the motivation of SBN is not clear, the principle of the proposed method is not explained, and the relationship and difference between the proposed context construction methods are not discussed. I do not see a clear explanation of how the proposed method advances beyond existing approaches, and the paper fails to clarify its contribution or the originality of SBN in comparison to previous work."
      }
    ]
  },
  "7P7FsPL05D": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The paper \"DuRND: Rewarding from Novelty to Contribution for Reinforcement Learning via Dual Random Networks Distillation\" proposes an extension to classical RND by introducing two distinct random network modules—one for states deemed \"successful\" and another for states associated with \"failure.\" This innovation allows for the derivation of both a \"novelty\" and a \"contribution\" reward signal, striking a balance between exploratory and exploitative behavior. While the additional novelty introduced by DuRND is incremental compared to classical RND, I still believe the contribution is valuable and fills a gap in the current literature."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents an approach to better balance novelty-based exploration and exploitation by introducing Dual Random Network Distillation (DuRND), an extension of the novelty-based bonus from RND that combines two bonuses based on novelty and contribution to success. Unlike RND, DuRND aims to focus later exploration behavior on novel states that led to successful trajectories or milestones. While experiments show that DuRND outperforms some novelty-seeking and reward-shaping approaches, the approach relies on strong assumptions about what constitutes success or failure and requires knowledge about the environment, such as milestones and $T_{max}$. Overall, DuRND's success seems to depend heavily on how milestones and other hyperparameters are set, and thus it lacks the level of applicability that may be required in general reinforcement learning. The contributions would be more significant if the authors could design ways to reduce dependence on the environment and training-specific information."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "The paper presents a framework (DuRND) that integrates two groups of lightweight random network pairs to jointly generate novelty and contribution rewards, with coefficients for these rewards scaled throughout training to balance exploitation and exploration, and integrates DuRND into PPO. While the problem the paper tries to solve is important and the idea is simple and clearly presented, some clarifications are needed to help me gauge the contribution of this work. It is unclear how challenging the evaluated tasks are, making it hard to gauge the contribution, and the results raise questions about the paper's assumption regarding the limitations of novelty-based reward."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "The paper proposes a novel algorithm DuRND, a refined version of RND (Random Network Distillation), by categorizing the states into successful states and failed states, allowing the agent to obtain a more refined intrinsic reward structure. The idea of splitting the successful and failed states is interesting, but not convincing, as this can be simply done by computing the intrinsic reward conditional on value [1], which is a more general indicator of the quality of current state. The proposed algorithm integrates this intuition but significantly increases the complexity of the algorithm compared to RND. I am not convinced that the improvement of the performance is solely coming from the characterization of the successful/failed states, as the design process introduces three hyperparameters that may not be chosen in a systematic or unified way across all types of environment."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents DuRND, a novel exploration method for reinforcement learning that builds upon Random Network Distillation (RND). DuRND introduces a dual network structure to differentiate between \"successful\" and \"failed\" states, aiming to guide exploration towards promising areas. Some reviewers considered the contribution incremental compared to RND, suggesting the core idea might not be entirely novel. Experiments across various environments demonstrate DuRND's potential to outperform existing novelty-seeking and reward-shaping methods."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "DuRND is a direct extension of Random Network Distillation (RND), introducing a dual random network architecture to compute both novelty (exploration) and \"contribution\" (exploitation) rewards, with a dynamic scheduling mechanism that shifts emphasis over time. While the use of random networks for both signals is a novel implementation detail, the underlying concepts—combining exploration and exploitation signals and dynamically adjusting their balance—are well-established in the literature (e.g., SORS, ROSA, Sibling Rivalry, SERENE). The authors accurately position DuRND as a low-overhead, scalable method, but somewhat overstate its conceptual novelty and the dichotomy between prior approaches. Empirical results show improved performance on standard sparse-reward benchmarks, though such gains may be attributable to implementation choices rather than fundamental innovation. Overall, DuRND offers a practical and well-engineered architectural tweak to RND, with its main contribution being a simple, dual-network design for joint intrinsic reward computation, rather than a conceptual breakthrough."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper proposes a method for shaping rewards in RL algorithms based on two random network modules that assign a contribution and novelty reward to states. The novelty reward is computed as the average of the prediction error of two random networks that have seen successful and failed trajectories respectively. While the proposed method shows better performance than the baselines on the evaluated environments, the paper does not provide any theoretical justification for the proposed reward shaping method or discuss the theoretical foundations of using the beta distribution for the contribution reward, and it does not provide any ablations on the proposed method, such as using only one random network for the novelty or contribution reward."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper proposes a novel framework, Dual Random Networks Distillation (DuRND), to address the exploration-exploitation dilemma in reinforcement learning by combining novelty-based exploration and value-based exploitation in a unified framework. The novelty and contribution rewards in DuRND are based on prediction errors from random network modules, which may not capture complex state relationships in certain environments, and the reliance on prediction error as a proxy for novelty might be insufficient where the state space has a complex structure or the prediction error does not align with actual information gain. The approach introduces an interesting combination of novelty and contribution rewards but relies on computationally efficient heuristics that might not capture the true novelty in complex environments. While the framework demonstrates effectiveness and low computational overhead, the linear adjustment of weights for novelty and contribution rewards is a heuristic that may not be optimal or universally applicable. Overall, the originality lies in the integration of novelty and value-based signals, but there remain significant limitations in the robustness and generalizability of the contribution due to dependency on specific proxy measures and heuristic schedules."
      }
    ]
  },
  "f4b0YVwKUO": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper introduces FASP, a structured pruning framework designed to enhance the efficiency and maintain the performance of large language models (LLMs) through structured pruning. While FASP offers practical improvements, its core ideas rely heavily on existing pruning strategies, such as those proposed by Wanda and similar structured pruning frameworks. The novelty primarily lies in the integration of these techniques, which does not constitute sufficient methodological novelty."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper proposes a new structural pruning technique, FASP, that achieves better performance and is also faster than previous baselines. However, the innovation from this work seems limited. The adapted Wanda is very similar to Wanda, only adding one summation operation along the column, and the knowledge restoration is basically layer-wise knowledge distillation."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes FASP (Fast and Accurate Structured Pruning), a structured pruning algorithm for LLMs that emphasizes both pruning efficiency (speed) and accuracy. The main weakness of this paper lies in its novelty. This paper proposes (1) formulation, (2) importance metric, and (3) restoration method, but all of these ideas can be found in previous works [1, 2, 3, 4] with slight modifications in some cases. In detail, the pruning of neurons is used in [1,2,3] and the importance metric of FASP is a straightforward modification of Wanda [4]; it just sums up the importance score of weights in each column to measure the importance of the column. The restoration method of FASP which solves the least-square problem is also used in previous works [1,2]. Therefore, FASP has limited novelty and originality."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "FASP is presented as a structured pruning framework for large language models (LLMs), positioned among recent methods like SliceGPT, FLAP, Wanda, and LLM-Pruner, with its main claimed novelty being the explicit, algorithmic interlinking of column/row pruning across sequential layers. The assessment finds that this \"interlinked\" structure is an incremental refinement of existing cross-layer or rotation-based pruning strategies, rather than a fundamentally new approach. FASP’s use of a Wanda-inspired pruning metric and a restoration mechanism are also routine adaptations seen in several recent works, with the restoration step differing mainly in implementation details rather than conceptual novelty. While FASP demonstrates competitive pruning times and practical efficiency, the source of these improvements may be due to implementation optimizations rather than core algorithmic advances. Overall, the submission’s claims of novelty are somewhat overstated, and the work would benefit from more comprehensive comparisons with several omitted, highly relevant recent methods to clarify its true contribution within a rapidly evolving field."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a structured pruning method for LLMs that removes rows and columns of weight matrices, introducing a heuristic metric to identify columns for removal inspired by the Wanda score and adjusting remaining weights via a least-squares problem. However, the method is a simple extension of Wanda, which limits the novelty of the proposed method. While the proposed method is simple and seems to outperform existing methods in terms of perplexity on the WikiText dataset, its originality is constrained by its similarity to prior approaches."
      }
    ]
  },
  "JfgBhEqk6F": {
    "human": [
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper presents Honey, a memory-efficient Federated Learning (FL) paradigm that overcomes the memory constraints of edge devices by deploying a progressive training scheme, utilizing holistic view and block-wise feedback, and introducing an elastic training strategy. However, the ideas of Honey lack novelty and are not quite significant. First, the idea of bottom-first progressive training in Honey has already been explored in [1], with the only difference being that Honey trains one block fully and freezes the rest, while [1] trains one block fully and partially trains other blocks. Second, the ideas of holistic view and block-wise feedback are highly similar to those in DepthFL [2], as both Honey and DepthFL add a loss function to the end of each block in training and distill knowledge between blocks by comparing their outputs; the only distinct point is that Honey combines the between-block losses with different coefficients. Based on these two points, it seems that the work of Honey is just a combination of different components from [1] and [2] with slight modification, which lacks original contents and is non-innovative. Lastly, the idea of elastic resource harmonization is too simple, as it just lets clients with more resources unfreeze more layers in training, which is insufficient to be listed as a contribution."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper studies federated learning over heterogeneous, memory-limited, edge devices and proposes a solution that can be applied to a wide range of models, including vision transformers. However, the paper lacks novelty, as progressive training is not a new concept and has been explored in several prior works, such as Hettinger et al. [1], and in federated learning settings like ProgFed [4], Fednet2net [5], and especially [6], which addresses heterogeneous memory constraints using progressive training. The primary novelty of the proposed method appears to lie in combining the loss from the last layer with that from the early exit, but similar approaches have already been investigated, for example in [8], where only the middle blocks are trained while the early layers and head remain frozen, and the cross-entropy loss is backpropagated through the frozen head to the trained layers, similar to the approach proposed here. Even setting aside these related works, the contribution remains minimal compared to those referenced in the paper (e.g., SmartFreeze and NeuLite), as the current submission offers only modifications to these existing techniques, which does not sufficiently justify a new publication."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "**Summary for Reviewer:**\n\nHoney proposes incremental improvements to memory-efficient federated learning by explicitly combining holistic and block-wise feedback for block collaboration and introducing a more granular elastic resource harmonization protocol. While these mechanisms are positioned as novel, similar ideas—such as block collaboration and adaptive resource allocation—are already present in recent works like NeuLite, SmartFreeze, and FedRolex, making Honey’s contributions refinements rather than conceptual breakthroughs. The authors overstate the lack of collaboration in prior methods and omit direct empirical comparisons to the most relevant baselines, which weakens the case for strong novelty. Reported empirical gains are unusually large for this mature field and may be influenced by experimental choices rather than core algorithmic advances. Overall, Honey is a modest extension of existing progressive training paradigms, and reviewers should scrutinize both the novelty claims and the experimental setup."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper proposes a progressive training approach called Honey to address the memory limitation issue in federated learning. The proposed approach is novel and has the potential to have a significant impact on the field of federated learning."
      }
    ]
  },
  "AfZH9EEuRR": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes a QR code processing system that consists of QR code detection, an image enhancement module, and QR code decoding, aiming to resolve stylistic and small QR code challenges by utilizing image enhancement modules such as color inversion, multi-scale processing, contrast enhancement, morphological operation, and super-resolution. However, the main weakness of the paper is novelty; it is a naive systematic paper that combines existing methods to resolve the QR code problem and isn't sufficient enough for ICLR's novelty standard. The paper proposes a simple pipeline that combines a QR code detection module, an image enhancement module, and a QR code decoding module, and it is difficult to find any new idea, novelty, or new perspective for each component. ICLR requires new and brilliant ideas, perspectives, and contributions to various research fields, but the paper's contribution is not enough to meet ICLR standards."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "The paper presents a system for QR code detection and recognition in egocentric settings by integrating existing methods such as Faster R-CNN for detection, traditional image processing techniques, and LRSRN for super-resolution, along with a disambiguation method based on ROI detection from another system (Lumos). However, the paper primarily combines existing tools and techniques without introducing significant algorithmic innovations, and the image enhancement pipeline is a straightforward combination of standard techniques like color inversion, multi-scale processing, CLAHE, and morphological operations. The disambiguation approach also relies on existing methods, and overall, the work has limited technical novelty, making it more suitable for an applied computer vision or system-focused venue rather than ICLR."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper introduces EgoQR, a system for efficient QR code reading on smart wearable devices in egocentric settings. While the approach is practical and effective, its novelty lies in the system integration rather than algorithmic innovation. Limited novelty is present, as the method combines existing techniques (e.g., Faster R-CNN, standard image processing) without significant innovation."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This work introduces a system for reading QR codes from first-person perspectives on wearable devices, tackling challenges like wide field-of-view and code distortion using a detection and decoding approach that includes a tailored Faster R-CNN model and image enhancement techniques. However, I find a lack of novelty, as the paper builds upon well-established techniques such as Faster R-CNN for detection and common image enhancement methods for decoding. While the system demonstrates a significant 34% improvement in QR code reading success over existing state-of-the-art readers, I would appreciate explanations on the novelty of this system to better understand its unique contributions compared to prior work."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "EGOQR is presented as the first QR code reading system explicitly designed and empirically validated for egocentric (wearable device) images, addressing challenges such as wide field-of-view, motion blur, and resource constraints. The work’s main novelty lies in its domain-specific integration and optimization of existing detection and decoding techniques, rather than in fundamental algorithmic advances. While the authors accurately position their system as unique in targeting egocentric data, the technical methods are largely adaptations of prior approaches, and some relevant related work (e.g., on motion blur and egocentric datasets) is not fully acknowledged or compared. The reported 34% improvement in decoding success rate over existing readers is a strong empirical result, though it may reflect the lack of prior systems optimized for this domain rather than a conceptual breakthrough. Overall, EGOQR’s contribution is a practical and incremental advance, with its primary strength being the explicit focus on wearable, egocentric scenarios, but reviewers should note that the underlying techniques are evolutionary and that the distinction from prior work is sometimes more rhetorical than technical."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents a QR code reading system that is designed for wearable devices and evaluated on a dataset of egocentric images, showing a 34% improvement in reading QR codes compared to existing state-of-the-art QR code readers. However, the technical contributions of the paper are limited, as the system uses existing techniques such as Faster R-CNN, Otsu’s method, CLAHE, morphological operations, and super-resolution in the proposed system, and does not present any new techniques or algorithms for QR code reading. The paper does not have a related work section to discuss the existing literature on QR code reading, making it difficult to understand the novelty and contributions of the proposed system without comparing it to existing approaches."
      }
    ]
  },
  "9I6UOIfbwf": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents a solution to the temporal consistency issue in video face-aging approaches by introducing a video data generation pipeline for synthetic video datasets, a video face aging framework with a recurrent U-Net structure, and new metrics (TRWC and Temporally Age Preservation) to validate temporal consistency and age transformation. The novelty of the paper is limited as most sections are \"inspired\" or \"motivated\" from previous approaches. For the data generation process, it relies on StyleGAN and SAM for single-frame aging results, and adopts the OSFV technique for generating faces at different poses and expressions as well as motion generation for temporal smoothing. The video aging architecture is not novel, as it is just a recurrent U-Net with commonly used losses. The structure of the paper is more about putting multiple (previous) approaches together in an engineering manner rather than emphasizing novelty."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper presents a novel synthesis video dataset for the video face re-aging task, created via a proposed pipeline that features many subjects covering a diverse range of age groups, along with a baseline video face re-aging architecture and two tailored novel metrics for evaluating temporal consistency. However, overall, lack of novelty is a disadvantage of this manuscript. The video face re-aging dataset is constructed by a pipeline with three stages, each relying on off-the-shelf methods: Style-based Age Manipulation (SAM) for image-based face re-aging, OSFV for key frame generation, and FILM for motion generation, resulting in a general pipeline for constructing video datasets. The proposed baseline architecture is composed of off-the-shelf building block stacks, such as a recurrent block (RB) and Unet-based Encoder-Decoder, and even the input fashion is borrowed from Zoss et al., such as 5 channels with age masks, as well as the discriminator with PatchGAN proposed by Isola et al. The proposed Temporal-Age (T-Age) metric measures the age difference between two adjacent frames utilizing an off-the-shelf age classifier from Rothe et al. In short, this manuscript can be considered as a regular technical report, and it has a gap to meet the novelty requirement for acceptance."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This is an interesting work."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission addresses video face re-aging with a focus on temporal consistency, but its claimed contributions—namely a synthetic paired video dataset, a baseline architecture, and new temporal metrics—are largely incremental extensions of recent work rather than fundamental advances. The authors overstate the novelty of their dataset and evaluation metrics, as similar resources and approaches (e.g., Re-Aging GAN++, STIT, Zoss et al., 2022) already exist and are not adequately cited or compared against. The baseline architecture is a routine recombination of established modules, and the proposed metrics are adaptations of prior identity/attribute consistency measures, with differences mainly in terminology and regional focus. Empirical improvements reported may stem from dataset scale or implementation details rather than conceptual innovation, and the lack of direct comparison to the most relevant recent methods weakens the claims of superiority. Overall, the submission’s main delta lies in the specific combination and scale of existing techniques, with technical novelty that is incremental rather than groundbreaking."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a pipeline for video face re-aging, including the creation of a synthetic video dataset using StyleGAN, SAM, OSFV, and FILM, the design of a baseline architecture trained on this dataset, and the introduction of two metrics to evaluate the temporal consistency of video re-aging methods. However, the proposed method seems like a simple combination of existing methods, and the contribution is limited."
      }
    ]
  },
  "Zmu3lVw6bm": {
    "human": [
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This work studies the trade-off between personalization and global generalization in Federated Learning (FT), identifying that personalized finetuning (PFT) causes overfitting and inconsistent performance across distribution shifts, and proposing LP-FT, a two-stage fine-tuning method that first performs linear probing and then full fine-tuning to improve local adaptation while preserving global features. The contribution and novelty need to be strengthened, as except for making LP-FT adapt to decentralized datasets in FL scenarios, it is unclear what else is vital and seldom studied; at present, the contribution of LP-FT is solid in centralized learning but not customized for FL. The evaluation shows that LP-FT consistently adapts to distribution shifts and outperforms existing work, but the related work is insufficient, as some state-of-the-art Parameter-Efficient Fine-Tuning methods (e.g., LoRA, IA3) and FL methods considering distribution shifts (e.g., FedTHE, FedIIR) are not discussed or compared."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper examines the challenges of adapting global models to local data in federated learning with diverse client data distributions. Through extensive evaluations, the authors identify limitations in existing personalized fine-tuning (PFT) methods, such as overfitting and inconsistent performance across distribution shifts. To address these issues, they propose LP-FT, a strategy that combines Linear Probing with full fine-tuning."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The authors propose an approach LP-FT to tackle the key challenge of balancing local personalization and global generalization by combining linear probing with full fine-tuning and providing a solid theoretical analysis using a two-layer linear network. The novelty of the proposed method is somehow limited, as it is mostly based on LP-FT. While the contribution on comprehensive analysis is significant, the novelty of the technical method seems weak. The overall approach of using linear probing with full fine-tuning to prevent feature distortion is interesting, but the paper would benefit from more comparison with PFL baselines, as it only considers different fine-tuning paradigms."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission addresses personalized fine-tuning (PFT) in heterogeneous federated learning (FL), introducing and analyzing the adaptation of LP-FT (Linear Probing + Full Fine-Tuning) to the FL context, with both empirical and theoretical contributions. Its main substantive novelty lies in the theoretical analysis of LP-FT for FL, which is not present in prior work and represents a clear conceptual advance. The empirical evaluation is thorough—covering seven datasets and dual local/global metrics—but is incremental, as similar benchmarking and fine-tuning approaches have been explored under different names in recent literature. The claim of “first-time identification” of federated feature distortion is somewhat overstated, as related phenomena have been discussed previously, though the submission’s explicit analysis adds value. Overall, while the adaptation and systematic evaluation of LP-FT are useful extensions, the primary innovation is the theoretical analysis, and reviewers should note that some novelty claims are exaggerated due to omission of recent adaptive and feature-level methods in the related work discussion."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper investigates the challenge of balancing local personalization and global generalization in personalized federated learning (PFL). The paper provides a rigorous and comprehensive empirical evaluation of personalized fine-tuning (PFT) methods under diverse distribution shifts, which is a significant contribution to the field. The introduction of LP-FT as a robust and versatile solution to the challenges of PFL is a novel contribution. The method's ability to preserve pre-trained global features and mitigate the adverse effects of excessive personalization is well-supported by the experimental results. The theoretical analysis using a two-layer linear network provides valuable insights into the conditions under which LP-FT excels and contributes to the theoretical foundation of the proposed method."
      }
    ]
  },
  "f3jySJpEFT": {
    "human": [
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "The author devised an algorithm called FS-WLasso, which is based on a greedy algorithm with the addition of forced exploration. This work is particularly valuable in the sense that the authors united all various assumptions in sparse contextual linear bandits, and allows researchers to focus on a single general condition. Additionally, their meticulous preparation is evident in the various remarks and supplementary materials to address anticipated questions and to emphasize their significance/novelty. They also introduced a novel proof technique using induction—like a domino, they've constructed an interesting proof scheme to maintain their compatibility condition. To compare it with Oh et al., 2021, it feels like 'an algorithm which is easy to run but should satisfy some strong assumptions on the environment' vs 'an algorithm which works on a weaker assumption but requires more information about the environment'. Some readers might think of this as an incremental improvement."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes a weaker sufficient condition for high dimensional bandit algorithm and proves regret bounds for bandit algorithms under this weaker condition. I think in general the weaker condition, and the algorithm induced, would be a good addition to the literature. The novelty claimed, such as the cyclic induction (Lemma 6?), should be moved to the main text as well. Assumption 3 is claimed to be \"strictly weaker\" than the existing conditions, but some counterexamples should be provided to show that Assumption 3 holds but not any of the existing conditions. It's hard to read and understand the novelty for the current version."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper examines the stochastic sparse linear bandit problem and proposes an algorithm that achieves $O(\\mathrm{polylog}(T))$ regret under relaxed assumptions compared to prior work. This work can be highly valued for presenting solid improvements on a problem of significant interest within the bandit community. While the authors highlight that the conditions in prior work are unverifiable, they do not address the verifiability of their own conditions, which is unfair and warrants additional discussion. Furthermore, concerns about the clarity and impact of the technical contributions were also noted."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission addresses the sparse linear contextual bandit problem in high dimensions, presenting a Lasso-based algorithm that achieves polylogarithmic regret under strictly weaker structural assumptions than prior work—specifically, requiring the compatibility condition only on the optimal arm rather than globally or on the averaged arm. The main technical contribution is this relaxation of assumptions, supported by a new high-probability regret analysis, though the algorithmic structure (forced sampling with Lasso estimation) closely follows established methods. Empirical results demonstrate superiority over prior approaches only in synthetic settings where standard diversity assumptions are violated, leaving the practical impact and real-world applicability of the relaxed assumptions unquantified. The submission accurately situates itself within the literature, though it sometimes overstates the restrictiveness of prior assumptions and the novelty of its analysis technique. Overall, the work represents a substantive but incremental advance in a mature field, with its primary novelty lying in the assumption regime rather than in algorithmic or conceptual innovation."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes an algorithm that is more flexible than the algorithms in Oh et al. (2021), Ariu et al. (2022), Li et al. (2021), and Chakraborty et al. (2023) because it is based on a weaker assumption. I believe the authors should clarify these advantages more explicitly and in detail, particularly by discussing and stating that their algorithm's use of a weaker assumption contributes to its greater flexibility compared to prior work. I also expect a detailed discussion of the relationship between Assumption 3 and both Assumption 2 and the aforementioned algorithms, explicitly showing the advantage and significance of Assumption 3."
      }
    ]
  },
  "4y6Q98hJzr": {
    "human": [
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper explores the LLM behaviors when adapting to specific domains via continual pre-training and points out an unexpected \"stability gap,\" which is an initial drop in performance before subsequent recovery. The proposed strategies seem similar with existing works' conclusions; for example, high quality data is important for model training [1,2], and using a similar data mixture rate to the pre-training data to alleviate data distribution shift [3,4]."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper uses the concept of stability gap to explain the initial drop in LLM performance during continual pretraining in a new domain and proposes three training strategies to address the initial instability. The proposed strategies improve the accuracy of the LLM in the new domain when compared to the existing continual pretraining techniques. However, the drop itself does not look significant (less than 1% averaged accuracy), making the observation less strong, and the performance improvement when compared to the baseline seems to be less than 1%, which does not look very significant."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission addresses efficient domain continual pre-training of LLMs, focusing on the \"stability gap\" (initial performance drop) and proposing a combination of multi-epoch subset training, high-quality token selection, and data mixture matching to mitigate it. While the empirical analysis is broad and systematic—covering multiple domains and model sizes—the individual strategies and the stability gap phenomenon have been discussed in recent literature, albeit sometimes under different terminology. The main contribution lies in the comprehensive empirical evaluation and integration of established methods, rather than in introducing fundamentally new concepts or techniques. The authors somewhat overstate their novelty, particularly regarding the identification and analysis of the stability gap and their data selection approach, as similar work exists but is not always fully acknowledged. Overall, the work is a solid and timely incremental advance with strong practical value, but reviewers should be aware that its methodological innovations are primarily in execution and scope rather than in conceptual originality."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper investigates the behavior of LLMs when continually pre-trained on a new domain's corpus, observing and explaining the stability gap phenomenon, and proposes three strategies aimed at improving domain performance and reducing computational costs by reducing the stability gap. However, the proposed method lacks some novelty; the concept of stability gap is borrowed from existing works, and the proposed strategies—continually pre-training the LLM on a corpus subset across multiple epochs, continually pre-training on the corpus subset with the highest quality, and using a data mixture rate similar to the pre-training data—are also not new in the field of continual learning."
      }
    ]
  },
  "qrTOtUdz4Z": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper studies the problem of Generalized Category Discovery, which aims to assign unlabeled samples with both known and unknown class clusters. They design a novel two-branch concept-based architecture, which introduces a novel shared derivable concept learning branch that consists of a frozen labeled known classes pre-trained encoder to capture concepts of known classes, which are then used to generate the derivable concept of novel classes. With this shared branch, the model can boost the representation learning of unlabeled samples with concepts learned from known classes. They also propose a concept generation module and a novel covariance loss to derive new concepts of novel classes from known classes, and consequently learn independent underivable concepts for novel classes. For the final model to be fine-tuned jointly, they expand the derivable concept generate layer to learn more independent concepts of novel classes with a proposed knowledge transfer constraint, and a normalization term is introduced to balance the learning process between labeled and unlabeled samples. They elaborately design a three-stage training strategy for the proposed ConceptGCD to alleviate the influence of the noise introduced by the uncertainty of unlabeled samples."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes a novel framework, ConceptGCD, that uses a covariance loss to learn distinctive concepts to facilitate category discovery in the Generalized Category Discovery (GCD) task. The novelty of the proposed framework is limited: the major contribution is the application of covariance loss in GCD. While the framework is well-designed to allow the model to use both \"derivable concepts\" to aid novel class discovery and \"underivable concepts\" customized to novel classes, the main originality lies in leveraging covariance loss to promote diversity in concept learning. The method achieves strong performance across six benchmarks, outperforming previous approaches, but the contribution is primarily incremental in applying covariance loss within the GCD context."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper addresses the GCD problem by introducing ConceptGCD, a novel approach that presents a concept categorization strategy to assist with novel class discovery in generalized category discovery. ConceptGCD enhances class discovery by categorizing concepts as either derivable or underivable from known classes and learning them in stages using a generator layer with a covariance-augmented loss and a concept score normalization strategy. While the approach is described as novel, the concept of derivable concepts is similar to existing works [1,2], and a comparison to these would strengthen the paper."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper introduces a concept learning framework for the generalized category discovery (GCD) problem, featuring a novel concept-based architecture. However, the novelty is somewhat limited, as the frameworks have been widely used in the GCD tasks."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment reviews \"COMPOSING NOVEL CLASSES: A CONCEPT-DRIVEN APPROACH TO GENERALIZED CATEGORY DISCOVERY,\" which introduces ConceptGCD—a stage-wise framework that explicitly separates and learns \"derivable\" and \"underivable\" concepts for Generalized Category Discovery (GCD). The main substantive novelty lies in this explicit, stage-wise separation, which is not present in prior GCD works that typically use a shared encoder and unified loss. Additional contributions, such as covariance-augmented loss for diversity and concept score normalization, are incremental technical adaptations from representation learning, newly applied in the GCD context. The authors’ characterization of prior work sometimes overstates their limitations, as some recent methods (e.g., SPTNet, DCCL) do address knowledge transfer and representation adaptation, though not via explicit stage-wise separation. Overall, the submission’s architectural change is a meaningful advance, but some claims of novelty and prior work limitations are somewhat exaggerated, and the literature review omits several relevant recent works."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces ConceptGCD, a novel framework for Generalized Category Discovery (GCD) that leverages a concept-based approach to improve the discovery of novel classes. The introduction of the ConceptGCD framework, which classifies concepts into derivable and underivable classes and addresses them in a staged manner, is innovative and well-justified."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper proposes a novel framework for Generalized Category Discovery (GCD), named ConceptGCD, which introduces a new way of learning with known classes by derivable and underivable concepts. The proposed method is novel. It is interesting and effective to introduce the concept of concepts into the GCD field and learn the different concepts separately. The experimental results show that the proposed method outperforms the previous state-of-the-art methods on several GCD datasets."
      }
    ]
  },
  "YCdag94iZs": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper introduces a novel approach to Multiple Instance Learning (MIL) by incorporating counting and attention mechanisms, where instead of relying on a single aggregated bag embedding for classification, the method identifies representative features across the dataset and encodes each bag as a count of these features, with informative features identified using statistical methods like the Mann Whitney test. While the concepts and issues presented in the paper have been previously discussed, the methods introduced offer a fresh perspective, and the method proposes a seemingly novel approach for encoding bags as a count of dataset-wide features. However, there is limited technical novelty, as the idea of counting in MIL is not novel and was already explored in other works, and the model reuses a lot of existing modules such as the Mann-Whitney test, thus novelty is also limited. The proposed approach seems to have better performance than the baseline on the disease classification using T cells repertoires benchmark, and the paper introduces a new dataset, named Wiki dataset, as an additional contribution. The Table 1 should compare with recent or state-of-the-art methods, otherwise the second novelty claim is not valid."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes a new multiple instance learning (MIL) method using counting and attention to address the overfit problem and the shortcoming of existing methods which are unable to enumerate the features. While the method demonstrates superior performance on MIL datasets and new real-world MIL tasks, the originality may be limited, and the method's novelty is also less pronounced. Compared with existing works, the main improvement appears to be in performance rather than in fundamental methodological innovation, and it is unclear what is improved for MIL by the proposed method except for the performance. The motivation for designing this method is not specific, and there is a lack of theoretical analyses and explanation of why the proposed method is more effective than existing methods. The section on novelty should be incorporated with the introduction, including further discussion of the shortcomings of existing MIL methods and an analysis of how the proposed method addresses these issues."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper attempts to analyze the impact of certain intrinsic feature attributes on classification within several specific datasets by applying feature selection in multi-instance learning. This idea may be meaningful in some specific scenarios, for example, when the data features themselves are well-normalized and highly indicative. However, feature selection strategies have been widely applied in early simple datasets, which limits the degree of originality. The contributions of the paper are actually based on the analysis of feature attributes, and I believe this is not a new concept, nor is this approach highly relevant to multiple instance learning. I would like the author to explicitly clarify the novelty and substantial advancement when compared with the paper by Raykar et al. (2008) and Akerman et al. (2023), as currently it is hard to say that the author's research essentially advances the field of multiple-instance learning."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces a technique called MILCA, designed to perform counting and summing of features, where feature weights are predicted using a fully connected network (FCN) with a projection replacing the softmax layer to produce coefficients within a specified range. In my view, this paper lacks sufficient novelty. The counting-based approach appears to be a straightforward extension within the MIL space, and it does not introduce any new theoretical contributions either."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper proposes a Multiple Instance Learning using Counting and Attention (MILCA) model, which argues that counting or averaging relevant features across instances can form a simple yet effective classifier for multiple instance learning. While the simplicity of the proposed approach is appealing, its technical novelty and overall effectiveness are not well established. As noted by multiple reviewers, similar ideas have already been explored in existing works."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "MILCA is a simple, efficient Multiple Instance Learning (MIL) method that extends counting/summing approaches by introducing learned feature weights and a projection step, aiming to improve interpretability and efficiency over attention-based models. The submission positions itself as an alternative to attention-based and graph-based MIL, but the technical novelty is incremental, mainly involving a different normalization (projection vs. softmax) and learned weighting, both of which have been explored in related motif-based and aggregation methods. Empirical results show modest accuracy improvements (about 3%) and efficiency gains, particularly in high-dimensional, low-sample regimes, though these may be context-dependent and not unique to MILCA. The authors somewhat overstate the conceptual distinction between \"counting\" and \"attention,\" as both are forms of weighted aggregation, and the practical impact of their technical variations may be limited. Overall, MILCA offers a practical, interpretable, and efficient extension of existing counting-based MIL methods, with its main contribution being empirical performance rather than a fundamentally new paradigm."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The authors propose a simple method for multiple instance learning (MIL) that involves feature selection and either a counting or a weighted sum of the selected features. The proposed method is very simple and can be considered a basic baseline for MIL; while it is interesting that it is competitive with state of the art methods, it is not surprising given that MIL datasets are small and the models are prone to overfitting. I don't think this simple method brings any new insights to the MIL community. The experiments show that it is competitive with state of the art methods."
      }
    ]
  },
  "RtFWWAXIyH": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper introduces VideoMV for multi-view 3D content generation, with the key insight of fine-tuning existing video generation models for multi-view generation by leveraging the inherent frame consistency in video models. The proposed 3D-aware denoising sampling strategy is well-motivated and effective in improving multi-view consistency, and I believe it is the most novel part in this paper. However, there are several existing works that also fine-tune video diffusion models for multi-view generation, such as SV3D and VFusion3D, and the lack of those baselines makes me not convinced about the superiority of the proposed methods. If we take papers like SV3D into consideration, the author needs to prove their proposed 3D-aware denoising sampling strategy is somehow comparable with the fine-tuning strategy in SV3D. The paper's emphasis appears misaligned with its claimed contributions, and if the main novelty lies in the fine-tuned video diffusion model rather than the adapted reconstruction network, the experimental results should focus more on comparing multi-view generation capabilities instead of reconstruction results."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes a method for both image-based and text-based 3D generation, introduces new metrics for evaluating 3D consistency, and claims to achieve multi-view image generation with fast convergence speed using pre-trained video generation models. However, fine-tuning a video generation model for 3D generation is not novel, as leveraging the implicit knowledge of video generation models to help 3D generation has been investigated in recent works (e.g., SV3D and IM-3D) and achieves significant performance. Although the paper introduces IM-3D and SVD (likely SV3D), it does not provide a clear comparison with them, and the differences claimed—such as optimization requirements and view conditioning—are not convincingly established. Therefore, I feel the novelty of this paper is limited and it also does not comprehensively discuss related works. The reconstruction network is presented as another main contribution, but its role as an intermediate feature rather than the final 3D representation raises questions about its significance."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes a framework for the multi-view generation task utilizing video generative models, introducing a 3D-aware denoising strategy to enhance multi-view consistency in object reconstruction. However, the paper lacks novelty. The current novelty of this paper is limited; although the proposed 3D-aware denoising strategy is intended as the primary innovative contribution, the guided denoising approach, which substitutes the denoised result with the output from the GS, appears overly simplistic and lacks the strategic depth necessary to be considered truly novel. Moreover, as indicated in Table 3, the 3D-aware denoising strategy only improves the model by 0.4 dB in PSNR and 0.009 in SSIM, and these incremental gains appear minimal, which raises questions about the overall effectiveness and necessity of this approach. The visual differences are minimal, making it difficult to observe any significant improvement from the proposed method."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission, VideoMV, presents a technically solid approach for multi-view image synthesis by fine-tuning video diffusion models and integrating a feed-forward 3D reconstruction module into the denoising loop. While the method demonstrates impressive training efficiency and competitive multi-view consistency, its core ideas—leveraging video diffusion models and explicit 3D feedback—are shared with several recent works (e.g., IM-3D, SV3D, VFusion3D), making the claimed novelty incremental rather than fundamental. The main technical differentiation lies in the specific procedural integration of 3D reconstruction within the denoising process, but similar concepts have been explored in concurrent literature. The authors’ characterizations of related work tend to group prior methods by single features and sometimes overstate the uniqueness of their own contributions, omitting detailed, head-to-head comparisons with the most similar approaches. Overall, VideoMV is a strong, efficient variant in a rapidly evolving field, but its novelty claims should be calibrated against the backdrop of parallel, incremental advances and substantial methodological overlap with recent work."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a novel framework for consistent multi-view generation, built upon pretrained video diffusion models and fine-tuned with object-centric videos. The authors introduce a 3D-aware denoising sampling strategy involving a feed-forward reconstruction module that obtains an explicit global 3D model, which replaces the original multi-view images in the denoising sampling loop. The proposed method is novel and achieves state-of-the-art performance in both text-based and image-based multi-view generation tasks, and the 3D-aware denoising sampling strategy is interesting."
      }
    ]
  },
  "INzc851YaM": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "The paper presents an empirical analysis on how advances in transformer architecture affect model performance on MORL tasks, specifically proposing Preference-Attended Multi-Objective Decision Transformer (PA-MODT), a new architecture based on MODT to facilitate the preference encoding problem. The novelty of the proposed method and contributions are very limited. In my opinion, this paper exhibits very limited novelty and applicability, since the methods only fit the transformer-based architectures in MORL, and their properties are not clarified clearly."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "The paper proposes the Preference-Attended Multi-Objective Decision Transformer (PA-MODT) to facilitate the learning across large preference spaces and handling unknown preferences during evaluation, specifically by integrating a preference-attention block into the MODT architecture to enhance preference encoding. The novelty and contributions are very limited. I think the proposed method has very limited novelty and contributions. Besides, the method’s applicability is also limited since it only suits the transformer-based architectures in MORL."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment reviews a submission introducing PA-MODT, a transformer-based architecture for offline multi-objective reinforcement learning (MORL) that features a novel preference-attention block and modular design, aiming to improve preference integration and generalization over prior transformer-based MORL models. The main technical contribution is the explicit modeling of preference interactions via attention, moving beyond the simple preference concatenation used in works like PEDA, MODT, and MORvS, though similar modular and attention-based adaptations exist in related domains. While the empirical results on the D4MORL benchmark are promising, the improvements may partly reflect implementation details or routine architectural refinements rather than a conceptual breakthrough, and the claimed limitations of prior work may be somewhat overstated. The submission also offers a more comprehensive analysis of evaluation metric sensitivity, which, while incremental, adds rigor to the evaluation. Overall, the work represents a solid but incremental advance in transformer-based MORL, with its main novelty being architectural rather than a paradigm shift, and would benefit from broader comparisons and a more balanced characterization of related literature."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper introduces the Preference-Attended Multi-Objective Decision Transformer (PA-MODT), a novel architecture for multi-objective reinforcement learning (MORL) that integrates a preference-attention block within a modular transformer structure to address challenges in handling large preference spaces and unknown preferences. This architecture enhances the model's ability to generalize across different preferences and trajectories, leading to improved performance in generating optimal Pareto fronts. However, although the paper presents this novel architecture, experimental evaluation could be strengthened by comparing PA-MODT with a broader range of state-of-the-art baselines beyond just MODT and MORvS; including more established MORL algorithms would provide a more robust validation of PA-MODT's claimed improvements in handling complex multi-objective tasks."
      }
    ]
  },
  "rWIrdAo2xC": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper introduces a method that renders novel views of human avatars, given only a single image, by proposing a pipeline of multiple stages that first learns a prior and then renders a human under novel views, highlighting their paradigm as \"direct\" and using extracted attributes for supervision in contrast with other \"indirect\" approaches. Even though the authors show good results, the overall contribution and novelty of the paper is not well supported. Components like transformers, stable diffusion, etc. have been used by other papers as well. Learning pixel-aligned features from different views has been proposed by previous works, like the generalizable NeRF ActorsNeRF (ICCV 2023) and MonoHuman (CVPR 2023). Stable diffusion has been used by works for humans, like DreamHuman (NeurIPS 2024), DreamAvatar (CVPR 2024), AvatarPopUp (ECCV 2024), and other works for 3D objects (e.g. DreamFusion). It is not clear to this reviewer what the additional challenges of monocular reconstruction are in the particular set up of this paper and how the particular method addresses them."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes to train a conditional diffusion model for generating novel views of humans from given single-view images, directly supervised by proxy ground truth 3D Gaussian attributes. The method outperforms other generalizable novel view synthesis techniques like LGM, and the idea of using a neural network to constrain the distribution of target 3D Gaussian attributes makes sense and is effective. However, although the method demonstrates improvement compared to previous methods like LGM, the image quality remains limited since only one single input image is used, and another potential direction in this field is incorporating a 2D diffusion prior to enhance information, as demonstrated in Human 3Diffusion. A comparison to these baselines is needed."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper introduces Human-DAD, whose main contribution is the adoption of a \"direct\" supervision paradigm for learning 3D Gaussian attributes in the 3D Gaussian Splatting (3DGS) space, setting it apart from existing methods that rely on pixel-level supervision in 2D image space. I appreciate the novelty of the approach, including the innovative pipeline design, the use of direct supervision for 3D Gaussian attributes, and the introduction of 'distribution unification' for pseudo-ground truth construction. However, I am skeptical about the core claim that direct supervision in 3DGS space is superior to indirect supervision in 2D space, as the provided evidence is unconvincing and the major issue regarding this claim remained unresolved."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission introduces a novel direct attribute-level supervision paradigm for 3D Gaussian Splatting (3DGS) in generalizable monocular 3D human rendering, diverging from the prevalent pixel-level (indirect) supervision used in prior works. The approach leverages a two-stage proxy attribute construction and a conditional diffusion model to directly supervise 3DGS attributes, which is a substantive technical difference from existing methods like GPS-Gaussian, Zou et al., LGM, and FreeSplat. While the use of diffusion models for 3D attribute generation is not new, its application to 3DGS attributes in this context is novel, though the underlying methodology follows established trends in the field. The authors’ claims of inefficiency and suboptimality in prior pixel-level supervision are sometimes overstated and not always empirically substantiated, and some relevant recent works are not cited or compared. Overall, the main contribution is the new supervision strategy for 3DGS, but the practical impact should be carefully evaluated with empirical results and broader comparisons to recent literature."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a novel method for generating novel views of humans from single-view images by leveraging 3D Gaussian Splatting. The authors propose a two-stage process to create consistent and smoothly distributed proxy ground-truth 3D Gaussian attributes, which is a novel approach to address the challenges of precise error backpropagation and local optima convergence in existing methods. The method, named HUMAN-DAD, demonstrates significant performance improvements over state-of-the-art methods through extensive experiments."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "The paper presents a method for novel view synthesis of humans using a proxy-ground-truth creation process and a diffusion model. I provide a quantitative comparison of the proposed method with several existing methods for novel view synthesis, including those that utilize mesh-based models and other volumetric approaches. I highlight the strengths and weaknesses of our method relative to existing approaches and provide a clearer understanding of its advantages and limitations in the context of the current state-of-the-art."
      }
    ]
  },
  "Y5mm3Yb36I": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper proposes a framework to measure originality in diffusion models, introducing a metric that, unlike previous memorization studies which need access to training data, is computed at test time and could be quite useful in practice. Understanding originality in T2I generated images is an important and timely topic, and the synthetic data experiments are well motivated and very well-done. However, the authors skipped citing a large number of relevant memorization and prompt inversion literature, and a major concern I have is the authors' definition of originality, which I believe is not quite rigorous and is associated with a lot of subjectivity. The metric’s reliance on dreamsim score as a measure of originality is not fully justified, and even in cases with low dreamsim scores, I still feel the reconstructions are original. The claims are only demonstrated on a small dataset and the earliest SD models, so they might not extend to more recent models with alternative architectures and training data. Some contradictions to the proposed framework might exist, and it would be useful to see how the method performs on challenging cases from prior work to better establish its contribution."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes a framework for evaluating the originality of text-to-image (T2I) generative models, particularly in the context of copyright considerations. The method leverages the number of hidden states in a new and innovative way, presenting a fresh and interesting idea. While the paper introduces some interesting ideas, there are concerns regarding novelty, as existing methods can reproduce the original image by selecting specific noise seeds, and several studies have explored this area [1,2], which raises questions about using the number of hidden states as a measure of originality. Additionally, even if the relationship between \"easier to describe\" and \"shorter token length\" holds, the model appears to measure relative originality/creativity [3], which depends on the training dataset of the diffusion model and raises questions about its effectiveness for applications such as copyright infringement detection."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper studies the originality of generated content from text-conditional diffusion models, proposing that the number of tokens required to invert a generated image is a strong indicator of originality. The detection method is simple to use and is built upon existing works such as Textual Inversion, which can enable widespread adoption by the community. I like that the idea is motivated by a simple observation of “description complexity” as a measure for originality. While the paper curates a method—and baselines in this regard are difficult to find—I suggest the authors at least provide a discussion around the missing baselines. I believe the method can be useful, but the paper needs to be supplemented with solid applications using it, such as how it can be useful for copyright infringement detection."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents a new technique for measuring the originality of an image leveraging diffusion models and textual inversion, where originality is quantified by the number of tokens needed for high-quality reconstruction or by the reconstruction error when restricted to a single token per image. I like the overall idea; the notion that something more 'original' would be harder to reconstruct is sensible, and using the number of tokens needed for successful textual inversion is neat. However, originality is defined with respect to training data, which I think is particularly problematic. It seems like your method more so measures a T2I model's familiarity or ease of replication for a given concept or image, which directly relates to how frequently the concept is seen during training, but not with how unique the concept is. I think claiming this work is built on legal notions is an overstatement."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission introduces a novel, model-internal metric for quantifying image originality in text-to-image diffusion models, specifically by measuring the number of tokens required for accurate image reconstruction via textual inversion. Unlike prior work, which typically relies on dataset-driven or output-based originality measures, this approach leverages prompt-based reconstruction difficulty as a proxy for originality, offering a practical advantage by not requiring access to training data. The method is empirically validated using both synthetic and real data, with results showing correlation to DreamSim, a human-aligned perceptual similarity metric, though the main innovation lies in the metric itself rather than the validation methodology. While the adaptation of textual inversion for originality measurement is a substantive methodological advance, the underlying conceptual link to memorization and generalization literature is incremental rather than radical. Overall, the work is timely and relevant, but would benefit from broader comparison to dataset-driven and human-judged originality metrics, and some claims about the limitations of prior work are somewhat overstated."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces a new approach to quantify the originality of images generated by text-to-image (T2I) diffusion models, focusing on copyright originality. The paper introduces a novel method to quantify image originality in T2I models by correlating it to the number of tokens required for reconstruction. The authors argue that stable diffusion models can create novel, unseen elements when trained on diverse data. However, the paper does not provide a detailed comparison with existing methods for measuring originality or similarity in generative models, nor does it address how this method compares with existing techniques for assessing the originality or similarity of generated images, such as similarity metrics based on feature extraction."
      }
    ]
  },
  "7hM5597bCv": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "The paper proposes an offline RL algorithm utilizing latent diffusion skill models for temporal abstraction, and Q learning with these skills. The method is a combination of existing offline RL algorithms (primarily LDCQ and IQL), but there is no proper reason given for this particular configuration of components. The only novel addition seems to be the use of the value function for deciding when to stop executing a skill, but this is a simple iterative improvement. Novelty is not strictly necessary, but the additions made here are not well justified at all with no coherent story surrounding it. Small engineering improvements to boost the score in this benchmark does not give any signal to the true value of the method."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper suggests improving IQL to train a value function using OOD but constrained actions, by instead using a skill prior learned by diffusion, and further proposes adaptive re-evaluation, which re-plans the trajectory if the future value function becomes worse than the current value function. However, it seems that DIAR is an incremental improvement of LDCQ, which changed the base offline algorithm from BCQ to IQL, and specifically, DIAR uses the same procedure of LDCQ for getting the latent priors, using $\\beta$-VAE for latent representation and getting the latent priors via diffusion. DAIR seems to be the IQL version of LDCQ plus Adaptive re-evaluation, and I would appreciate clarification on the differences between LDCQ and DIAR."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents a model-based offline RL method using diffusion models that outputs sequence-level distributions to handle long horizons and compounding error issues, and addresses out-of-distribution samples with a learned value function. The adaptive reevaluation component is novel, but most of the contributions come from existing works, as the method combines components from prior literature. The key novelty, adaptive reevaluation, demonstrates limited applicability and its empirical effectiveness remains unclear, making the overall contribution appear weak."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "DIAR is positioned as an incremental extension of recent diffusion model-based offline RL methods, particularly LDCQ and IDQL, by integrating diffusion-based trajectory modeling with implicit Q-learning and introducing an adaptive revaluation mechanism for dynamic decision length adjustment. The main technical novelty lies in this adaptive revaluation, which allows the policy to adjust decision horizons dynamically—a feature not present in the most closely related prior work, though conceptually related to adaptive horizon methods in RL. Other aspects, such as alternating training between real and generated samples and the integration of diffusion models with IQL, are routine adaptations already explored in the literature, making these contributions less distinctive. The authors’ claims of consistent state-of-the-art performance and strong differentiation from prior work are not fully substantiated in the provided excerpt, and some rhetorical distinctions (e.g., “assisting” vs. “guiding” Q-learning) are not technically meaningful. Overall, while DIAR offers a modest technical advance with its adaptive revaluation mechanism, its other contributions are incremental, and a more thorough comparison to recent diffusion-based RL methods would be necessary to fully establish its impact."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces Diffusion-model-guided Implicit Q-learning with Adaptive Revaluation (DIAR), an offline reinforcement learning approach that integrates a diffusion model to generate diverse latent trajectories, an Adaptive Revaluation mechanism for adjusting decision lengths, and a Q-network learning approach with value function guidance from the diffusion model. The idea of using diffusion models to generate diverse latent trajectories for offline RL is interesting. However, the proposed method is not novel. The idea of using diffusion models to generate diverse latent trajectories for offline RL has already been explored in the literature, and the proposed method seems to be a combination of existing techniques. The authors claim that the existing diffusion-based offline RL methods do not avoid the Q-function, which is not true; for example, the Diffuser learns a policy without any Q-function. The authors should clearly state their novelty and contributions compared to these existing methods."
      }
    ]
  },
  "Ax3uliEBVR": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The authors propose an equivariant Topological Deep Learning framework that deals with geometric node features and can be generalized to many topological domains including simplicial, cell, combinatorial, and path complexes. The authors add an important piece of work for the Topological Deep Learning (TDL) community as there is not much literature on Equivariant TDL, and the novel benchmark based on geospatial information is novel. However, novelty is the key disadvantage of the paper, as it seems that the work just extends prior works on graphs to TDL; even though the theoretical insights are important, they are mostly an extension from graphs. Furthermore, the property of “heterogeneous interactions” was actually mentioned in prior literature, so it is not fair to claim that ETNNs are set up for this characteristic, but more like TDL in general already possesses this property."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper introduces an equivariant model within the framework of topological deep learning, generalizing the equivariant graph neural network architecture from Santorras et al. from the setting of graphs to message passing over combinatorial complexes, and allowing for message passing with cells that have heterogeneous node features over differing ranks. I really like that the authors introduced a novel geometric prediction task into the literature, as many of the benchmarks for TDL were somewhat old and outdated, and the new task appropriately features integration of data over different dimensional regions in a way that showcases the central feature of the paper—reconciling data with features on subspaces of differing dimension, i.e., the ‘heterogeneous interactions’ promised in the abstract. For me personally, I find it hard to understand the framing of this as a part of an entirely new conceptual field of topological deep learning beyond GNNs, and question the genuine novelty of papers like this; unless I’m mistaken, the basic content of proposition (1) is that an ETNN can be reformulated as an EGNN, meaning that the main novelty is the clever choice of ‘lifting' the data into a certain graph and some delineation of the learning based on ‘rank’, and even the proof of theorem (1) is basically a straightforward adaption of the corresponding result for EGNNs. I personally get the sense that the ‘topology’ part—and hence the novelty of these kinds of architectures—is overplayed a little, as it seems that the inclusion of domain-specific data along with the design of the graph is doing most of the work, and I don’t personally see a strong connection with the ‘lifted’ combinatorial complexes used in this paper and topology in the classical sense. The claim that this is the first work to explore combinatorial topological modeling of multi-resolution irregular geospatial data seems overstated given existing literature in Topological Data Analysis applied to geospatial data, and I would appreciate clarification on how this approach differs from or advances beyond these existing works."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper presents the ETNN architecture, a novel framework for $\\mathrm{E}(n)$-equivariant processing of a wide class of topological data objects, which elegantly unifies and generalizes existing architectures such as EGNN and EMPSN. While the proposed method generalizes previous work to general combinatorial complexes, the architectural changes are incremental. The paper theoretically proves expressivity improvements over $\\mathrm{E}(n)$-equivariant graph methods and introduces a principled approach to modeling irregular multi-resolution geospatial data using combinatorial complexes. The air pollution downscaling benchmark introduced in the paper is a new dataset for benchmarking TDL architectures, and the construction and analysis of geospatial combinatorial complexes is in itself an interesting contribution. ETNN variants achieve clear performance gains over standard $\\mathrm{E}(n)$-equivariant graph methods for QM9 molecular property prediction tasks and improve upon EMPSNs while using less memory and having a faster runtime."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper extends TNN to ETNN, similar to the extension of GNN to EGNN. I agree with some of the reviewers that the work is not entirely novel given the similarity with extension of GNN to EGNN. In this sense, I tend to think this is a borderline paper. On the other hand, given the many empirical comparisons and insights of add equivariance to existing methods, I tend to recommend an accept."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission introduces ETNNs, a general framework for E(n)-equivariant message passing that extends prior models (notably EGNN and EMPSN) from graphs and simplicial complexes to arbitrary combinatorial complexes, thereby unifying and generalizing existing approaches in equivariant topological deep learning. The main technical novelty lies in this abstraction, which enables modeling of more general higher-order structures and allows geometric features to be incorporated at all cell orders, though the practical benefits of these extensions are only modestly demonstrated in standard domains. Claims of increased expressivity and efficiency over prior models are supported by theoretical and empirical analysis, but the improvements are incremental and may partly result from implementation choices rather than fundamental algorithmic advances. The application to a new geospatial dataset is a useful demonstration of generality, but does not constitute a methodological advance. Overall, the work is a technically sound and well-positioned generalization, but reviewers should be aware that its practical impact and novelty may be somewhat overstated unless further empirical evidence is provided for domains where combinatorial complexes offer clear advantages."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces a novel framework called E(n)-Equivariant Topological Neural Networks (ETNNs) that unifies and generalizes existing methods for geometric/topological deep learning on graphs and higher-order structures. The paper presents a significant theoretical contribution by providing a unifying framework for topological deep learning on different combinatorial structures. The authors provide a thorough theoretical analysis of the expressiveness of ETNNs, including their ability to distinguish certain non-isomorphic geometric graphs and higher-order structures. The comparison with existing methods like EGNNs and EMPSNs is insightful. The results demonstrate clear improvements in prediction accuracy and computational efficiency over multiple baselines, establishing ETNNs as a state-of-the-art approach."
      }
    ]
  },
  "RdG7LVGnQi": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper argues that, in the context of training multi-width LLM OFA supernets, decoupling shared weights to eliminate interference from weight sharing between subnets, using low-rank adapters for training efficiency, and dynamically adjusting the sampling strategy across training steps improves deployment time. Although not extremely novel, it is a fun and interesting paper to read. I think this is incremental work—not very novel but useful and has some novelty. It applies concepts developed in other contexts in a new related context. The observations on the implications of uniform subnet sampling during training are interesting, but not well explained. There has been some recent work claiming that interference mitigation in OFA training isn't very helpful. The authors of the paper under review claim that this problem is important and describe a potential solution. Contrasting with prior work that makes a contrasting argument would improve the paper. Prior work chose \"subnets\" for uniform sampling, while this paper chooses \"subnet mean widths,\" and your evaluation shows that it works a little bit better, though it is unclear if this is sampling noise or a genuine improvement."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper introduces LLM-QFA, a supernet-based approach that fine-tunes multiple quantized models with identical architectures but varied bit widths to cater to different deployment scenarios. The main contribution lies in integrating quantization-aware training (QAT) with LoRA, allowing different subnets to share the same LoRA component. The approach primarily combines existing techniques—quantization-aware training and LoRA—within an OFA context, but it does not substantively test the full implications of this setting. Without new insights or substantive contributions, the approach appears to merely integrate established methods without yielding notable theoretical or practical advancements."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper presents a methodology for Fine-tuning Quantized LLMs in a train once-for-all (OFA) setting, involving fine-tuning multiple quantized models with identical architectures but varied bit widths to cater to different deployment scenarios. On the negative side, ideas were deemed interesting but not particularly novel—existing techniques are stitched together. The importance of the \"quantize-for-all\" approach, the use of LoRA finetuning to deal with interference, and the ideas introduced around sampling to improve speed while remaining performant in terms of accuracy are strengths, but overall the paper misses important and quite pertinent related work, and the contributions are not seen as particularly original."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission presents \"LLM-QFA,\" a once-for-all (OFA) quantization-aware fine-tuning (QAT) framework for large language models (LLMs) that enables a single model to support multiple quantization configurations, leveraging LoRA-based parameter-efficient fine-tuning, decoupled weights, and a resource-balanced sampling strategy. The work is positioned within the QAT + LoRA integration cluster and claims novelty in combining OFA QAT, LoRA, and explicit interference/resource balancing for LLMs, though similar concepts exist in vision supernet literature and in recent LLM works such as Any-Precision LLM (which is not cited). While the technical adaptation to LLMs is substantive—particularly the decoupling of weights for different quantization configs—the use of LoRA and resource-balanced sampling are established ideas, and the \"first\" claim is somewhat overstated. The authors' characterization of prior work is generally accurate but omits relevant comparisons and underplays the sophistication of some related methods. Overall, the submission offers a meaningful incremental advance by adapting OFA QAT to the LLM+LoRA context, but would benefit from a more comprehensive discussion of related work and a more measured presentation of its novelty."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces the LLM-QFA framework, a novel approach to reduce the training cost of deploying large language models across diverse scenarios. The idea of using a once-for-all training paradigm for large language models to reduce training costs across diverse scenarios is interesting. The paper presents LLM-QFA's interference-less fine-tuning and resource-balanced sampling strategy, and shows that LLM-QFA produces optimal quantized models and outperforms existing methods like QA-LoRA and GPTQ in terms of performance and efficiency."
      }
    ]
  },
  "X9OfMNNepI": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This work investigates whether large language models (LLMs) can autonomously generate novel and valid hypotheses in chemistry based solely on a research background. Generating research hypotheses is a complicated task, and the authors heuristically decomposed hypothesis generation into two steps: inspiration retrieval and hypothesis refinement. In the hypothesis refinement step, the authors propose a novel “mutate and recombine” trick to help generate good hypotheses. The reviewer suggests it would be great if the authors could briefly discuss why the decomposition of the major question is necessary, and what’s the difference or connection between the proposed inspiration identification and Retrieval Augmented Generation (RAG), indicating a need for clearer articulation of the contribution to the field."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces a framework for scientific hypothesis discovery and generation using large language models (LLMs), proposing \"MOOSE-CHEM\", a method based on previous work \"MOOSE\" but with modifications to adapt it for chemistry literature and improve overall performance through refinement and mutation. I think the paper is generally a great and novel contribution to the field. The authors introduce a scientific benchmark, along with a novel framework for hypothesis generation, and mention a novel \"Evolutionary Algorithm (EA)\" that mutates, refines, and recombines hypotheses. While the work includes a fair comparison with previous work, the proposed method of Evolutionary Algorithm (EA) sounds to be related to other methods in LLMs that try to improve the consistency/quality of the answers, such as Self-Consistency (SC), Universal SC, Multiple Chain of Thought (MCT), self-reflection, and self-correction approaches, so I believe that some related work could be included regarding this algorithm in the paper."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The authors propose to combine LLMs with traditional search strategies for generating/refining hypotheses in scientific domains (e.g., chemistry). Hence, the problem setting is very challenging and rather unexplored until now."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that MOOSE-CHEM is a domain-specialized, more formally decomposed extension of agentic LLM frameworks for scientific discovery, specifically targeting chemistry hypothesis generation. Its main substantive contributions are a new, expert-annotated benchmark of post-2024 chemistry papers and rigorous evaluation protocols that minimize data contamination, both of which set a higher standard for empirical rigor than prior work. While the submission introduces a more explicit mathematical formalism for workflow decomposition and integrates evolutionary search with multi-step inspiration retrieval, these components individually have clear precedents in the literature; the novelty lies primarily in their integration and domain focus. Some claims—such as the uniqueness of the mathematical decomposition and the sophistication of prior methods—are somewhat overstated, with the practical workflow (retrieval, composition, ranking) remaining conceptually similar to existing modular frameworks. Overall, the work’s strongest differentiators are its benchmark and evaluation rigor, while its technical advances are incremental, representing a thoughtful but evolutionary step in the field."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a framework for scientific discovery in chemistry based on LLMs, in which the authors decompose the task of generating a hypothesis given a background question into three steps: finding inspirations from the literature, composing the hypothesis from the background and inspirations, and ranking the hypotheses. The authors construct a new benchmark dataset of 51 recent chemistry papers and evaluate the performance of LLMs on this task, including an ablation study and comparisons with other methods. The contribution of a new benchmark dataset for this task could be useful for future research."
      }
    ]
  },
  "CXIiV1iU3G": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents a novel method for large-scale parameter generation, termed Recurrent diffusion for large-scale Parameter generation (RPG). Distinct from previous approaches, RPG incorporates parameter correlations and employs a recurrent model to learn the interrelationships among non-overlapping parameter tokens. The paper introduces a new approach that combines recurrent neural mechanisms with diffusion-based generative modeling to effectively capture and stabilize dependencies between model parameters, specifically targeting complex parameter interdependencies that arise in large models. The paper shows a creative reimplementation of classification task to show that one can treat parameter generation as a conditional generative task, which shows promising results on par with original performance on unseen tasks in Section 4."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents a new approach for parameter generation combining autoregression and diffusion, using SSMs (Mamba) to easily and effectively perform large-scale parameter generation, and introduces a method for parameter tokenization that performs significantly better than tokenization methods used by previous works. However, the approach is very limited in novelty; autoregressive models feeding embeddings into a diffusion model is not new in general."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission presents a meaningful but incremental advance in diffusion-based neural network parameter generation, primarily through the explicit use of a recurrent model to capture correlations among parameter \"tokens\" and a practical demonstration of scaling to very large models (e.g., LLaMA-7B LoRA) on a single GPU. While the authors claim to be the first to achieve such scaling and correlation modeling, prior works like SANE and D2NWG have addressed similar challenges with different technical approaches, and the novelty is more in implementation than in conceptual leap. The submission’s main technical distinction is the recurrent modeling of parameter dependencies, which is a substantive improvement over p-diff and D2NWG, but only incrementally different from SANE’s sequential autoencoder. Claims regarding improved generalization and token-based modeling are less differentiated, as conditional generation and sequential partitioning have been explored in recent literature. Overall, reviewers should recognize the practical value and technical refinement of the work, but also calibrate novelty claims against the field’s rapid, incremental progress and the sophistication of closely related prior art."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a novel method for generating large-scale neural network parameters using a combination of parameter tokenization, a recurrent model, and a diffusion model. The authors compare their approach with several existing methods and demonstrate its superior performance."
      }
    ]
  },
  "e8c7XDRJcg": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "The paper investigates an unsupervised modality adaptation method for the action recognition model, proposing Cross-modal Transfer Through Time (C3T), which performs alignment on each temporal step rather than on video-level feature representation as in Contrastive Alignment (CA). The novelty is minor, which is only on doing the alignment per temporal step, without proposing a new alignment method or an architectural change to current state-of-the-arts. The lack of state-of-the-art comparison also makes it hard to justify the significance of the novelty besides its improvement upon Student-Teacher (ST) and CA methods."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "The paper presents three methods to perform Unsupervised Modality Adaptation (UMA): Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer Through Time (C3T). Unsupervised Modality Adaptation is frequently used for cross-modal learning in many applications and cannot be regarded as a new contribution of this paper. Compared to ST and CA, the Cross-modal Transfer Through Time (C3T) performs pseudo label generation and alignment in temporal sequences, while other operations, including feature learning, RGB-IMU alignment, and classification, follow the same procedures, so the novelty of the proposed method needs to be reconsidered. The experiment comparison is not reliable since the proposed approach is not compared with related frameworks, and there are many existing works as listed in Section 2. I suggest considering refining the key innovations and redefining the key representations for innovation."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission addresses Unsupervised Modality Adaptation (UMA) for Human Action Recognition (HAR), formalizing a strict setting where no target modality labels are available during training—a meaningful but incremental specialization over prior cross-modal transfer work. The proposed C3T method combines contrastive alignment and teacher-student paradigms, introducing temporal convolutions and shared self-attention for cross-modal temporal alignment; however, these are technical variations rather than fundamentally new concepts, as similar mechanisms exist in prior transformer-based models. The authors’ claims of novelty are somewhat overstated, particularly regarding the unexplored nature of UMA and the assertion that all prior methods require labeled data in both modalities, since some recent works (e.g., IMU2CLIP, ImageBind) do support unsupervised or zero-shot transfer. The strongest differentiation lies in the explicit formalization and systematic evaluation of UMA in HAR, as well as the robustness analysis to temporal noise, though empirical gains may stem from implementation details rather than conceptual advances. Overall, the submission is well-situated within the evolving literature on cross-modal transfer and multimodal HAR, but its technical contributions are incremental, and a more nuanced comparison to closely related works is warranted."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper focuses on the unsupervised modality adaptation problem, i.e., transferring knowledge from a modality with labeled data to another modality with unlabeled data. The authors propose three methods, including Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer Through Time (C3T), to perform the task. The idea of aligning unlabeled modality data without labels is interesting."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper proposes a cross-modal transfer method for human action recognition, based on the assumption that there is a shared latent representation space for different modalities, and includes three variants: student-teacher, contrastive alignment, and cross-modal transfer through time. The novelty of the paper is limited, as the assumption of a shared latent representation space for different modalities has been proposed by many previous works. While the idea of aligning modalities in a common latent space is well-established, the paper needs to more clearly articulate the specific novel contributions beyond the existing literature on shared latent spaces. For example, the 'cross-modal transfer through time' (C3T) method should be explained in more detail, highlighting how it differs from existing temporal alignment techniques, and the paper should emphasize the unique aspects of its approach."
      }
    ]
  },
  "h5D0JICV3s": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents an LMM-based NR-PCQA method that is capable of consuming text, images and point clouds to predict quality scores. Both novelty and contribution of the paper are limited. The introduced model, database construction, and model design are all based on the existing LMM-based IQA and VQA studies. Only small adaptions are made to adapt the format of 3D point clouds, which heavily limits the novelty and contribution of the paper."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes the PIT-QMM model, which bridges the gap between 2D and 3D quality assessment through a point-image-text multimodal approach. However, the novelty of this paper is questionable. The use of LMM to evaluate the quality of images and videos has already been validated in prior studies, such as Q-Align, so applying a similar approach to assess the quality of point clouds does not offer incremental contributions to the field. The training method used to derive the quality scores for point clouds remains identical to that of Q-Align. Additionally, LMM-PCQA adopts a similar approach (with only slight differences in point cloud feature extraction), which was already published in ACM MM 2024. The multi-modal feature fusion of 2D projections and 3D point clouds also closely resembles the approach used in MM-PCQA."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "PIT-QMM introduces the first end-to-end model that jointly fuses point cloud, image, and text modalities for no-reference point cloud quality assessment (NR-PCQA), extending prior multimodal approaches such as LMM-PCQA and MM-PCQA, which use partial or sequential fusion. The main technical novelty lies in this joint, end-to-end integration, while the proposed dataset construction strategy—combining text, 2D projections, and 3D data—is an incremental extension of existing methods. Empirical results show improved performance and efficiency, but these gains may be influenced by model scale or training strategies rather than architectural breakthroughs. The authors’ characterizations of related work are generally accurate, though they sometimes understate the sophistication of prior fusion strategies and omit some recent relevant works, potentially overstating the contribution delta. Overall, PIT-QMM represents a logical next step in the field’s progression toward richer multimodal fusion, with its “first” claims being technically correct but reflecting incremental rather than transformative advances."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper proposes a novel approach to NR-PCQA by developing a Large Multimodal Model (LMM) that can consume multiple modalities of data, including text, images, and point clouds. This is a significant contribution as it allows for a more holistic understanding of point cloud quality. The authors construct a new dataset that combines different types and levels of information, which is a valuable resource for the research community. The dataset construction strategy is carefully designed and leverages the unique strengths of each modality. The proposed PIT-QMM model is extensively evaluated on popular benchmarks and compared with state-of-the-art methods. The results show that the proposed method outperforms existing approaches with fewer training iterations, which is a significant advantage. While the proposed method outperforms state-of-the-art approaches, it is not compared with the most recent method [1], and this comparison is crucial to establish the superiority of the proposed method."
      }
    ]
  },
  "ZdHa3y0DeB": {
    "human": [
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper presents a framework called SceneLock for privacy protection in autonomous driving data, utilizing adversarial attacks through two modules: NSE, which transforms original images into adversarial counterparts to prevent detection and recognition by downstream models, and NSD, which restores these images for authorized use. The reversible adversarial learning framework proposed is straightforward and has practical applicability. However, the novelty is limited, as the paper introduces adversarial perturbations to images to thwart malicious data users and then employs image steganography techniques to remove the perturbations—both of which are well-established techniques. As a result, the novelty and contribution of this work are limited."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This work proposes a novel reversible adversarial learning framework, termed SceneLock, for the protection of camera-based autonomous driving scenes. Although there is some innovation in this work, the essence of this paper lies in steganography and encryption of images to safeguard privacy during transmission, preventing unauthorized access. While the author summarizes the innovation points, there is a lack of a brief overview of the current research landscape regarding this issue, and it would be beneficial to highlight existing research gaps and the challenges addressed by the solutions proposed in this paper. The application of these concepts to camera-based autonomous driving raises important questions about whether such scenarios introduce unique features or research opportunities for image steganography."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment finds that the submission, *SceneLock*, adapts established reversible adversarial example (RAE) and steganography techniques to the domain of autonomous vehicle (AV) camera data, introducing a dual-module system for reversible data protection that targets both AI models and manual annotation. While the application to AV data and the explicit dual focus are new, the underlying technical methods—reversible adversarial encoding/decoding and integration with steganography—are well-documented in prior work across other domains (e.g., medical imaging, IoT). The authors’ claim of conceptual novelty is overstated, as the main contribution is an application-driven integration rather than a fundamental methodological advance. The submission notably omits citation and comparison to a substantial body of closely related RAE and adversarial steganography literature, which weakens its positioning. Reviewers should recognize the value of the application context but calibrate expectations regarding technical innovation, and are encouraged to request more thorough engagement with prior work."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents a novel dual-layer protection mechanism that combines adversarial perturbations with image steganography. This approach effectively safeguards data against unauthorized use while preserving its integrity for legitimate users. The proposed method is well-grounded in existing literature, with a clear explanation of the techniques used and their integration into the SceneLock framework."
      }
    ]
  },
  "E1Tr7wTlIt": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The paper introduces a novel approach called Partial Vector Freezing (PVF), designed to reduce computation in Secure Aggregation Protocols (SAPs) without increasing communication overhead, and further proposes the disrupting variable extension to PVF to support enhanced privacy. While I appreciate the clarity and straightforwardness presented in your methodology, I am concerned about the apparent simplicity of the proposed solution. The approach, as described, seems to lack the level of innovation. Consider expanding on the theoretical background, comparing your method with others in detail, and emphasizing any novel insights or improvements that your solution offers."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper introduces a novel method called λ-SecAgg, which integrates a module named Partial Vector Freezing (PVF) into Secure Aggregation Protocols (SAPs) for federated learning. The concept of freezing and unfreezing vector entries in the context of secure aggregation is very novel, effectively reducing the computational burden on SAP, which has been a significant bottleneck in real-world federated learning applications, especially for large-scale models such as Large Language Models (LLMs). Additionally, the paper proposes a Disrupting Variables Extension (DVE) that enhances privacy by adding noise to the frozen entries using Differential Privacy (DP). The strengths of the method lie in its innovative design, theoretical rigor, and comprehensive evaluation, showing significant performance improvements."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper proposes $\\lambda$-SecAgg, a secure aggregation protocol for federated learning (FL), to reduce computational and communication overhead using Partial Vector Freezing (PVF). However, most reviewers raised their concerns that PVF significantly reduces this privacy."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This assessment reviews the λ-SecAgg with Partial Vector Freezing (PVF) submission, which proposes a modular approach to reduce computation and communication in secure aggregation for federated learning by freezing most of the update vector and processing only a fraction, with mechanisms for full aggregate recovery and privacy preservation. The submission is well-situated within the active field of secure aggregation, closely related to prior work on sparsification, partial encryption, and modular protocol innovations, though it sometimes overstates its distinctness and omits discussion of several relevant works. The main claimed advance is the ability to achieve efficiency gains without information loss or extra communication, but this claim requires stronger empirical and conceptual comparison to advanced sparsification and partial encryption methods, which may offer similar benefits. Integrating PVF with existing secure aggregation protocols and introducing a privacy-preserving element appear to be logical extensions rather than fundamentally new contributions. Overall, while the approach is timely and potentially useful, the novelty and significance of the technical contributions depend on clearer differentiation from prior art and more comprehensive evaluation."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "The paper introduces a novel method, Partial Vector Freezing (PVF), that significantly reduces the computational overhead of secure aggregation protocols in federated learning without incurring additional communication overhead. This is a novel contribution to the field. The authors provide a thorough evaluation of PVF, including comparisons with seven baselines and five masking schemes, and discuss the integration of PVF with different secure aggregation protocols, showing its portability and versatility. While the paper compares PVF with several existing methods, it could provide a more in-depth analysis of the comparison with the most closely related works."
      }
    ]
  },
  "7zJDTnogdG": {
    "human": [
      {
        "id": "human_review_5",
        "type": "human",
        "label": "Human Review (review_5)",
        "content": "This paper presents a novel, ECG-specific self-supervised learning approach that incorporates temporal augmentations (random cropping) over a patient contrastive methodology. I have not personally seen the random cropping approach they introduced as their temporal augmentation, which speaks, in part, to its originality. This paper is introducing a novel method more than it does a foundation model, and while TA-PCLR clearly builds off the PCLR method, it is not totally clear how—one would have to read the PCLR paper to learn this. Even by name, it is apparent that TA-PCLR is a combination of individual existing methods which form a novel aggregate approach, but whether it is simply adding random temporal cropping to the PCLR method or if the method/cropping approach was inspired by other work in the ECG or timeseries space is not made explicit. The literature background gives a relatively strong, concise overview of existing pretraining approaches, but refers to just a couple existing foundation models (and only in passing), and certain methods, such as Song et al., 2024 and 3KG, should likely be referenced in greater detail. Calling the dataset-specific training and evaluation \"the first investigation of its kind\" is an ambitious statement, and while the analysis emphasizes their multi-source dataset and is important for understanding how data distributions affect performance, the explicit value it adds to this particular study is left somewhat unclear. Overall, the work seems sufficient for simply introducing a novel pretraining method, but is lacking for a foundation model paper."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents an ECG-based foundation model using a large-scale dataset of six million ECGs from different institutions and proposes to use zero masking and random cropping as augmentation strategies. The proposed work lacks novelty; the framework does not possess originality expected by ICLR and is very similar to existing frameworks. In fact, it is unclear to me what the novelty is, as the methods section presents the vanilla InfoNCE loss and then discusses the augmentations, preprocessing, architecture, and training scheme. Comparison to existing SOTA frameworks is very limited, and while the authors mention relevant papers, they only compare to a few in the results section, despite there being many notable works in this area that are worth comparing to."
      },
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents a contrastive learning based foundation model for ECG data that predicts patient Age, Sex, 5-year mortality, and various cardiac abnormalities, claiming novelty through the introduction of temporal augmentation to the previously proposed contrastive learning algorithm PCLR. However, I found a lack of originality and novelty, as the proposed method is built upon the previously published PCLR backbone (Diamant, Nathaniel, et al., 2022), with the only enhancement being the addition of temporal augmentations such as zero-marking and random clipping—augmentations that are not new and are not specifically inspired by any in-depth understanding of ECG signals or their unique characteristics. A commonly used loss function was also employed. I failed to see the technical originality and contributions here and would like the authors to further highlight and justify their technical novelty and contributions in this work."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper proposes temporally augmented patient contrastive learning for ECG data, introducing the idea of randomly cropping ECG data as a novel form of augmentation that minimally alters the underlying clinical pathology. I think the specific choice of augmentation used here is novel, but most of the reviewers felt this alone was not sufficient. One suggestion to expand the contributions by the work is to study whether this form of augmentation works across different training and neural network architectural choices and to better update their proposal with improved graphics and visual aids on why their method works."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission presents a large-scale integration of patient-based contrastive learning and temporal augmentations for ECG foundation models, introducing the TA-PCLR approach and pretraining on a new, diverse, multi-center dataset. The main contribution is the empirical demonstration that combining these established techniques at scale yields improved generalization and performance on external validation, though the conceptual novelty is modest since both elements have been explored separately in prior work. The authors’ claims of being the first to combine these methods are somewhat overstated, as similar augmentations and patient-based strategies have appeared in the literature, albeit not always together or at this scale. The work is well-executed and thorough in evaluation, but would benefit from more direct comparisons to closely related, sometimes uncited, methods (e.g., BCL, MedAug) and a deeper analysis of dataset diversity. Overall, the submission offers a meaningful incremental advance through integration and scale, rather than a fundamental methodological breakthrough."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces a novel approach, TA-PCLR, which combines temporal augmentations with patient-based contrastive learning for ECG data. This is a creative combination of existing ideas applied to a new domain. The paper compares the performance of TA-PCLR with both fully supervised models and other pretraining approaches, and the comparison with fully supervised models is particularly relevant as it highlights the potential advantages of self-supervised pretraining."
      }
    ]
  },
  "xy9yv5siYQ": {
    "human": [
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This work presents a framework to efficiently reconstruct dynamic scenes from casually captured monocular videos, introducing a camera estimation module for frame-wise camera poses and representing deformations of Gaussians using a HEX-Plane representation. While camera pose estimation with relative initialization and joint optimization is novel, the technical novelty is limited overall; combining HexPlane representation with Gaussian Splatting does not seem novel, as many published works have combined TriPlane representation with Gaussian Splatting. Although disentangling dynamic and static objects in scenes is sound, the adoption of HexPlane representation is not well-presented, especially since a simple Fourier series, as used in Splatter-a-Video, can also represent Gaussian dynamics in the center position and rotations, and HexPlane introduces significantly more computation and storage overhead. This work is concurrent with similar Gaussian video representations, such as Splatter-a-Video and GFlow, and discussion on these works is still necessary, as these two works have been publicly available on arXiv for about four months before the ICLR submission deadline. The relative camera pose module is designed only with depth priors, focusing on relative camera movement between two frames, which is consistent with that in Dust3R; it is unclear why Dust3R was not directly applied."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper addresses the task of dynamic scene reconstruction, specifically developing a deformable 3DGS representation from an unposed monocular video. However, there is a lack of novelty, as the proposed method is more like a mixed bag which combines [1], [2], [3], and [4]. Specifically, the hexplane-based deformable Gaussian field has been explored by [1], the relative pose initialization is adopted from [2], the reprojection loss in Eq. 12 is similar to that of [3], and the depth alignment loss in Eq. 13 is similar to Eq. 15 in [4]. The authors may have to make their contributions more explicit."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission presents an incremental advance in dynamic, pose-free 3D Gaussian Splatting (3DGS) for monocular video-based scene reconstruction, introducing a Hexplane encoder and SE(3)-based pose initialization as its main technical novelties. While the authors claim to be the first to achieve unified, high-quality dynamic 3DGS reconstruction without known camera poses, similar capabilities have already been demonstrated in recent works such as MoSca and Shape of Motion, with differences primarily in architectural and optimization details. The claimed contributions—unified static/dynamic modeling, pose-free operation, and geometric regularization—are routine in the field, with the main delta being the specific implementation of the Hexplane encoder and pose estimation module. The authors’ positioning of prior work sometimes exaggerates their own novelty and underrepresents the unification and capabilities present in related methods. Overall, the work is a solid, well-executed technical improvement within a rapidly evolving area, but reviewers should recognize that its advances are incremental rather than fundamentally new."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents a method for reconstructing dynamic scenes from monocular videos without known camera poses, proposing a unified Hexplane-based Gaussian field to capture both static and dynamic effects in the scene. The authors propose an interesting attempt with the Hexplane-based representation and a two-stage optimization strategy. The proposed method outperforms existing approaches on both dynamic novel-view synthesis and camera pose estimation tasks across multiple datasets. However, while the authors claim that the Hexplane representation can effectively disentangle static and dynamic regions, the paper lacks sufficient evidence to support this claim."
      }
    ]
  },
  "uuvujfQXZy": {
    "human": [
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes learning post-hoc Concept Bottleneck Models (CBMs) in an unsupervised manner, using an unsupervised non-negative matrix factorization to discover concept vectors, an input-dependent alignment score selection, and a sparse linear layer for class prediction. Unlike previous methods, UCBMs do not assume a set of concepts learned by black-box models, but instead discover the concept vectors in an unsupervised manner. The idea of input-dependent sparsity constraint is interesting and non-trivial. However, the proposed method has limited novelty, as it uses an existing matrix decomposition method called CRAFT for feature generation, and the novelty is limited to proposing input-dependent selection applied before learning the sparse linear layer. The paper lacks comparisons with newer methods, particularly with [2], which trains a learnable dictionary to approximate the embedding space of VLMs in a supervised manner without using a pre-defined concept set."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposed a novel type of CBM called UCBM, which utilizes concept discovery methods to discover concepts learnt by pretrained, black-box models and convert them into CBM. The paper proposed an unsupervised concept discovery mechanism for CBMs, which is novel in the field of CBM, as the method does not need pre-defined concept sets. Additionally, a novel input-dependent concept selection mechanism is proposed to dynamically retain a sparse set for prediction; however, the idea of input-dependent concept selection is not novel, as, for example, Panousis et al. (2023) introduced a binary latent indicator to dynamically select activating concepts according to input embedding and mask out other concepts. I agree that the introduction of unsupervised concept discovery into CBM is interesting, but the missing of text representation limits the novelty."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The paper proposes Selective CBMs (UCBMs), a novel mechanism that converts any pre-trained model into a CBM by decomposing features into interpretable concept representations using a sparse matrix and introducing input-dependent concept selection. However, I find a lack of novelty in the proposed modules: the assumption that concepts form the basis of the latent space and can be linearly decomposed using a sparse matrix has been well studied in prior work [2,3], and the concept selection module has already been utilized in [4,5,6] where an auxiliary branch predicts the most important concepts for predictions. This severely limits the novelty of the proposed modules, as the approach largely builds on top of known and effective methods."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission introduces Unsupervised Concept Bottleneck Models (UCBMs), which combine unsupervised concept discovery (without text alignment or annotation) with input-dependent, highly sparse concept selection in a CBM framework, and add VLM-guided model editing for error correction. While each component—unsupervised concept discovery, per-input TopK sparsity, and model editing—has prior art, their integration in this context is novel and yields improved interpretability and sparsity (e.g., 1.4% of concepts used per input). The authors’ claims are generally well-supported, though they sometimes overstate the novelty of unsupervised concepts and underplay the sophistication of prior label-free CBMs and unsupervised interpretability methods. The main technical advance is the per-input, input-dependent concept selection within a CBM, which is a direct but meaningful adaptation of known sparsity techniques. Overall, the work represents a substantive, if incremental, advance in the rapidly evolving field of concept-based interpretability, and is differentiated primarily by its specific combination of existing methods."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents a novel approach to converting a black-box model into a concept bottleneck model (CBM) using unsupervised concept discovery, and proposes an input-dependent concept selection mechanism where only a sparse set of concepts relevant to the input are dynamically retained. The idea of using concept discovery for CBM is interesting and novel. However, the paper does not provide comparisons with other concept discovery methods such as [1], other methods that convert a black-box model into a CBM such as [2,3], or with CBMs trained from scratch, which limits the ability to fully assess the originality and significance of the contribution."
      }
    ]
  },
  "p6eQRlaxGo": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents ATOMFLOW, a novel deep generative model under the flow-matching framework for the design of ligand-binding proteins from the 2D target molecular graph alone, co-designing the protein binder structure and the target-molecule flexibility. I think the major weakness lies in its novelty. First of all, using flow-matching with full atom generation in peptide or protein or other bio-molecules is not a new idea. Besides, the equivariant generation process and FAPE loss in AlphaFold still follow previous works. Therefore, this work appears to be a fusion of several previous studies, applied in a new task, lacking innovation in terms of the methodology. I have doubts about whether the level of novelty in this paper meets the standards expected for a top-tier conference like ICLR. I hope the author could clarify the difference between the techniques used in this paper and the original ones, and the improvements upon previous approaches, as this would give the authors an opportunity to highlight any methodological innovations that may not be immediately apparent. I also hope the authors can clarify if AlphaFold3 is directly applicable to this specific task of designing ligand-binding proteins from 2D molecular graphs, and if so, explain the key differences or advantages of their approach compared to AlphaFold3 for this particular application. My major question is about the novelty."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper presents AtomFlow, a flow matching model for designing a protein structure to bind a small molecule ligand, which jointly denoises the structure of the protein and ligand and does not require knowledge of the ligand pose, unlike RFDiffusionAA. The methodology can be described as a flow-matching version of RFDiff-AA and does not score high on originality or novelty from a ML perspective. Further, the flow model architecture and noising process are based on AlphaFlow, with different justification but no difference in practice as far as I can tell. The overall significance of the contribution is unclear as it represents an incremental methodological advance over RFDiff-AA with more or less the same model capabilities. The authors argue that not needing to specify the ligand pose is a big plus, but no meaningful evidence or use case is provided for this distinction."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper presents a method, AtomFlow, to design protein structures that bind to input ligands, and, contrary to prior work, it does so without the assumption that the ligand’s bound pose is given by adopting a flow that jointly generates the structure of the protein and the binding ligand. The general method is a parallel to the diffusion-based RFDiffAA that is based on flow matching. The reviewers acknowledged the novel use of flow matching, but were concerned about the lack of technical novelty in light of prior work such as AlphaFlow and RFDiffAA. The paper can better highlight what technical contribution it has that goes beyond the application of the paper."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents the ATOMFLOW architecture for the design of protein folds conditioned by the ligand SMILES description. It's a well written technical paper about a novel architecture that can be useful for the bioinformatics community, but there is little technical novelty in the architecture blocks, loss function, or flow matching process (but the architecture is novel and useful). When generating folds for a purine ligand, the results were extremely conservative, with zero novelty and extremely low diversity, preferring the fold that dominantly occurs in the PDB, which suggests a significant bias towards folds that are abundant in the PDB. The designs have scores > 0.5, meaning that the fold structure is the same as in the PDB and no new folds have been discovered. It seems AtomFlow has memorized the space of all protein folds in the PDB and simply selects the one it has seen during training, so I suggest the authors run more cross-validation tests hiding some fold classes from the training and trying to reproduce them in the test."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "ATOMFLOW introduces a generative model for protein-ligand complex design that jointly generates both ligand and protein structures from 2D molecular graphs, addressing ligand flexibility and protein-ligand interactions in a unified, end-to-end framework. The main technical advance is the application of SE(3)-equivariant flow-matching at the atomic level, enabling iterative, joint updates of both ligand and protein, which distinguishes it from prior models that either require fixed ligand conformers (e.g., RFDiffusionAA) or are limited to pocket refinement (e.g., PocketGen, FlowSite). The authors’ claim of being the first to enable de novo, all-atom complex generation from 2D ligand graphs is accurate within the generative modeling context, though unified atomic representations have been explored in predictive models like Umol. While the unified \"biotoken\" framework and atomic flow-matching are meaningful extensions, the underlying concepts are increasingly standard in the field; the novelty lies in their integration for this specific generative task. Overall, the submission represents a substantive and well-substantiated advance, though some claims slightly overstate conceptual novelty, and additional comparisons to related predictive and sequence-structure co-design models could further clarify its scope."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces ATOMFLOW, a generative model designed to create ligand-binding proteins from 2D molecular graphs without requiring the pre-known 3D structure of the target molecule. The authors propose a flow matching framework for designing protein-ligand complexes that can generate both the protein and ligand conformations simultaneously, leveraging a unified biotoken representation for both proteins and ligands to enable seamless interaction modeling between different biomolecules. While the model demonstrates comparable performance to RFDiffusionAA, a state-of-the-art model, the innovation compared to RFDiffusionAA is limited and the model does not seem to outperform it. The paper also lacks a comparison with other pocket design methods, such as PocketGen and FlowSite, and it would be beneficial to discuss the advantages of ATOMFLOW over these methods."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper presents a deep generative model called ATOMFLOW, which is designed to generate ligand-binding proteins from scratch and operates on the quotient space of all possible alignments of N atoms in R^3, with each structure represented as a series of 3D tokens. The proposed method is technically sound and innovative, and the experimental results demonstrate that ATOMFLOW outperforms the state-of-the-art model, RFDiffusionAA, in terms of self-consistency, binding affinity, diversity, and novelty."
      }
    ]
  },
  "jqmptcSNVG": {
    "human": [
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces PepHAR, a state-of-the-art generative model for peptide design that innovatively integrates hotspot residues into peptide design, a concept well-supported by biological theories, enhancing the model's relevance and novelty in the field. The three-stage approach, including the use of dihedral angles and the correction stage, is a robust technical framework that ensures the generation of structurally sound peptides. The model demonstrates excellent performance in SSR, Affinity, Novelty, and Diversity, indicating its ability to produce high-quality, functional, and diverse peptides."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "There is no explicit novelty assessment or comparison to prior work in the review; only a recommendation to accept the paper is provided."
      },
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper introduces PepHAR (Hotspot-driven Autoregressive peptide design), a novel three-stage approach for designing peptides that can bind to specific target proteins. The work introduces a clever way to break down peptide design into a three-stage process based on biological understanding, focusing on hot spot residues first, then extending and refining, which makes the problem more tractable while maintaining biological relevance. The method features a novel energy-based density model to identify hot spot residues, using noise contrastive estimation for effective training and Langevin dynamics sampling to efficiently explore the residue distribution space. The hybrid optimization framework creates a novel joint optimization objective combining backbone and dihedral constraints, and introduces an iterative refinement strategy that can handle multiple fragments simultaneously. The authors also introduce a new \"scaffold generation\" task that better reflects real-world peptide drug development scenarios. When compared to state-of-the-art baselines (RFDiffusion, ProteinGenerator, PepFlow), PepHAR demonstrates superior performance across multiple metrics including structural validity, geometric accuracy, binding site accuracy, and novelty/diversity of generated peptides, while maintaining competitive stability and binding affinity."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "The paper introduces PepHAR, a hot-spot-driven autoregressive generative model for designing peptides targeted at specific protein binding sites, proposing a three-stage approach involving hotspot residue sampling, fragmented autoregressive extension, and gradient-based correction. The hotspot-driven approach combined with an autoregressive fragment assembly model is novel within peptide design, providing a unique perspective on addressing the protein-peptide design problem. Through experiments, PepHAR demonstrates competitive performance in scaffold-based peptide design, showing improvements in structure and binding accuracy when compared to state-of-the-art models like RFDiffusion and PepFlow on metrics including binding site overlap, novelty, and stability. However, although PepHAR outperforms some baselines, the reported improvements are incremental and may not justify the added model complexity and limitations."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "PepHAR introduces an autoregressive generative model for peptide binder design that explicitly distinguishes between hot spot and scaffold residues, integrating energy-based hot spot sampling (via Langevin dynamics) with fragment extension and probabilistic dihedral angle modeling. While the explicit hot spot/scaffold distinction and the new scaffold generation benchmark are positioned as novel, similar motif/anchor-based conditioning and scaffolding tasks exist in prior work (e.g., FrameFlow, Motif-Scaffolding, Anchor Extension), making many of the claimed advances incremental or combinatorial rather than fundamentally new. The main technical deltas are the integration of energy-based sampling with autoregressive modeling and the use of von Mises distributions for geometric fidelity, both of which refine but do not radically depart from established methods. The authors tend to overstate the uniqueness of their approach, sometimes downplaying conceptual and technical overlaps with existing literature. Overall, PepHAR’s contributions are best viewed as practical and explicit integrations of existing ideas, with the most substantive novelty in benchmark framing and technical implementation rather than in conceptual breakthroughs."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents PepHAR, a novel approach to peptide design that leverages an energy-based model to identify key residues, extends peptides autoregressively, and refines structures through a correction stage. The paper introduces a novel approach to peptide design that addresses the challenges of residue contribution, geometric constraints, and practical benchmarking scenarios. The use of an energy-based model for hotspot identification and autoregressive fragment extension is innovative."
      }
    ]
  },
  "CKYsXi0dOV": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper presents BLIP-3-Video, which introduces a \"temporal encoder\" alongside a conventional visual tokenizer, allowing it to significantly reduce visual tokens (32 tokens compared to thousands in other models). However, the primary weakness is the insufficient novelty. As detailed in Section 2.2, the only improvements to TTM include (1) time-stamped positional encodings and (2) a 'grouped' TTM temporal encoder. These minor changes do not substantiate a significant contribution."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper presents BLIP-3-Video, a novel multimodal language model for video understanding that introduces an innovative temporal encoder to significantly reduce the number of visual tokens needed to represent videos, offering a new approach to efficiency in video understanding models. While the integration of the temporal encoder is highlighted as a key innovation, I question the core novelty of this paper compared to prior works such as LLaMA-VID, Video-LLaVA, LLaVA-VID, and LLaVA-NEXT, as similar architectures have already been explored in these works. Although the experimental results show that 32 tokens achieved better performance on four short video benchmarks, this standard may change with different video lengths, scenarios, and question complexity, raising concerns about the scalability and generalizability of the method. Perhaps a more effective mechanism for accommodating more frames and selecting key information for video question answering from a large number of visual tokens is worth exploring, rather than focusing on a specific numerical value of visually overfitted tokens on a few benchmarks."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper provides BLIP-3-Video, a Video LLM that builds on the previous BLIP-3 architecture but focuses on incorporating temporality into the architecture by learning spatio-temporal pooling to obtain video representations in only 32 tokens. The paper's key weakness is the limited novelty, it is rather an exploration of architectures. Especially compared to TTM, the novelty, as also stated by the authors, is only the addition of grouping and time-stamp. For this reason, the AC recommends the rejection of the paper as its key contributions don't warrant a full paper at ICLR."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "BLIP-3-Video presents an incremental advance in token-efficient video representation for vision-language models, primarily through the architectural design and explicit integration of a learnable spatio-temporal pooling module. While the submission claims fundamental novelty and to be the first to demonstrate high-quality video QA with only 16–32 tokens, similar results and approaches have already been reported in recent works such as TESTA, SMAUG, and LLaVA-PruMerge. The use of sequential models like Token Turing Machines as temporal encoders is a direct application of existing methods rather than a conceptual innovation. The main technical contribution is the specific pooling architecture, but the overall approach aligns closely with the field’s ongoing trend toward aggressive token reduction and learnable aggregation. Reviewers should recognize BLIP-3-Video as a well-executed, incremental improvement within an established research paradigm, rather than a fundamentally novel or paradigm-shifting contribution."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes BLIP-3-Video, a vision-language model for videos that incorporates a temporal encoder to capture temporal information over multiple frames. The proposed temporal encoder is effective in compressing videos into fewer visual tokens without significantly compromising performance. However, the paper lacks discussion and comparison with relevant works on video token compression, such as [1] and [2], which also aim to compress video tokens to reduce computational costs. A comparison of performance and efficiency with these approaches is needed to better assess the uniqueness and significance of the contribution."
      }
    ],
    "deepreviewer": [
      {
        "id": "deepreviewer",
        "type": "deepreviewer",
        "label": "DeepReviewer",
        "content": "This paper introduces BLIP-3-Video, a novel multimodal language model designed for efficient video understanding, with the core innovation lying in its temporal encoder, which significantly reduces the number of visual tokens required to represent a video and directly addresses the computational challenges of processing long videos in large vision-language models. I find that the core idea of using a temporal encoder to reduce the number of visual tokens is a significant contribution, and the exploration of two distinct temporal encoders—spatio-temporal attentional pooling and Token Turing Machines (TTM)—is another strength that allows for a comparison of different approaches to temporal modeling within the same framework. However, the paper's novelty is somewhat limited by its reliance on existing techniques, as the core idea of using a temporal encoder to reduce the number of visual tokens is not entirely new and similar approaches have been explored in prior works. The authors acknowledge this to some extent, but a more detailed comparison with existing methods, such as token pruning or token merging techniques, would help to better contextualize the contributions of the proposed model and highlight its unique aspects. Despite this, the fact that BLIP-3-Video can abstract a video into as few as 32 tokens while maintaining high accuracy, and achieves competitive performance compared to much larger state-of-the-art models, is a remarkable achievement and constitutes a significant step towards more efficient video understanding. Overall, while the approach offers practical and efficiency improvements, the lack of a thorough analysis of how the proposed temporal encoder differs from existing token reduction methods makes it difficult to fully assess the model's unique contributions."
      }
    ]
  },
  "TxIrMD6lAN": {
    "human": [
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes a novel network design for incremental learning that emphasizes addressing inter-task differences as a key contributor to forgetting, rather than focusing solely on model stability. The approach includes a backbone network for capturing invariant features and adapters—originally introduced for efficient fine-tuning—that serve as feature modifiers for task-specific information, enabling the model to learn new tasks while preserving prior knowledge. However, the overall approach is not entirely novel, as similar concepts have been extensively explored, particularly in meta-learning-based class-incremental learning and dynamic expansion incremental learning approaches, which also train a generalized model and then adapt it to task-specific scenarios. The authors should discuss differences from these approaches in depth, such as those in \"An Incremental Task-Agnostic Meta-learning Approach\" and \"Tkil: Tangent Kernel Optimization for Class-Balanced Incremental Learning.\""
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper presents a method for continual learning that uses task-specific adapters in conjunction with existing methods like LwF or EWC, introducing different adapter formulations and initialization strategies. However, the proposed methodology is lacking in novelty, as the adapter strategy within the context of expansion-based methods for continual learning has been explored many times before (e.g., [a], [b]), and after the recent popularity of LoRA for LLMs, LoRA and similar adaptation techniques have also been ported over to continual learning many times (e.g., [c], with many more in the literature). [a] in particular is very similar to the proposed method, also using a low-rank parameterization inserted between layers of a network to adapt to each task. Claiming to be “among the first to investigate the role of adapters in incremental learning” is not appropriate. Overall, while there may be some useful findings here, I find this paper to be too similar to prior work, which aren’t properly discussed or compared with. Taking a further step back, this paper takes the common “baseline + small addition” approach, for which I’ve seen dozens of continual learning papers; such methods rarely see much adoption, as even if they do lead to improvements, it’s an impossible task trying to combine them with every other such method, for which there often isn’t much synergy."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The manuscript presents a method for class-incremental learning, but there is limited novelty, as noted by the reviewers. Concerns were raised about the lack of comparison with existing methods and the need for a considerable re-write on how its contributions are presented. The reviewers expressed that the manuscript does not clearly establish its originality or significance in relation to prior work, and the overall assessment remained negative due to these unresolved issues."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission proposes an incremental advance in adapter-based continual learning by co-training both the backbone and task-specific adapters with a regularization term, rather than freezing the backbone as in many prior works. While the authors claim novelty in integrating adapters with regularization-based methods (e.g., LwF, EWC), similar approaches—such as AdapterFusion and InfLoRA—already explore related combinations, and regularization for shared feature learning is a standard technique in the field. The main technical delta is the specific co-training regime, but the conceptual contribution is modest, as co-training and regularization are well-established strategies. Some author claims overstate the uniqueness of their approach and underrepresent the flexibility and integration capabilities of prior adapter-based methods. Reviewers should view this as a routine, plausible extension within a rapidly evolving area, and focus on empirical rigor and clear articulation of the technical differences relative to closely related work."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents a network design for incremental learning, consisting of a backbone network for learning invariant features and multiple small adapter networks for modeling task-specific knowledge, which can be integrated with existing incremental learning methods. However, the proposed method, which uses adapters to capture task-specific information, is not novel; many existing works already employ similar approaches. The authors should clarify how their method differs from existing methods and why it is effective."
      }
    ]
  },
  "Hhx3swAQAZ": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This paper introduces a post-tuning method to enhance current video generation models, allowing them to produce longer videos with lower training costs. My major concern is the novelty. The proposed method extends the original SVD by adding an identity 3D convolution layer in the temporal block. The technique novelty is limited."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposes ExVideo, a post-tuning methodology for video synthesis models, designing extension strategies across common temporal model architectures, including 3D convolution, temporal attention, and positional embedding. However, I find a lack of novelty, as the paper proposes extending the pre-trained SVD to generate more frames by adding an additional 3D convolution and a trainable positional embedding, both of which are techniques commonly used in modern video generation model architectures. Section 3.3 includes practical training settings and parameters with no algorithm-level contribution. Since the core contributions are the proposed \"Identity 3D Convolution\" and \"Trainable Positional Embedding,\" I would like to see more detailed illustration and description of these components to better understand any potential originality."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The paper lacks novelty, and there is a lack of comparisons."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents an effective approach to extend the capabilities of video synthesis models to generate longer videos, which is an original contribution in the field of video generation. The paper positions ExVideo as an innovative approach to extend video generation models; however, the methodology described does not appear to introduce any novel techniques but rather builds upon existing model architectures. The paper could benefit from a more detailed discussion of how ExVideo differs from and improves upon current state-of-the-art methods. The work is relatively significant as it addresses a critical challenge in video synthesis, which is the ability to generate longer, high-quality videos, and the proposed solution has the potential to impact various applications, including content creation and entertainment."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "ExVideo presents a practical, parameter-efficient post-tuning method for extending the temporal length of existing video diffusion models, distinguishing itself by enabling longer video generation without full retraining. While the use of adapters for efficient adaptation is well-established in the field (e.g., AnimateDiff, IP-Adapter), ExVideo is the first to systematically apply this approach specifically for temporal extension, making it a notable but incremental advance. The submission’s main contribution is engineering-focused, generalizing adapter-based strategies across multiple temporal modules, though the underlying methodology is not fundamentally new. The authors’ claims of novelty are accurate in the narrow context of temporal extension, but the omission of several relevant prior works (e.g., MagicVideo, LVDM, Latent-Shift) somewhat overstates the contribution. Overall, ExVideo is a solid example of the field’s trend toward modular, efficient adaptation, and reviewers should calibrate expectations accordingly, recognizing the work as a useful, incremental improvement rather than a conceptual breakthrough."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper presents ExVideo, a novel post-tuning methodology aimed at enhancing the capabilities of current video synthesis models to generate longer videos with lower training costs. However, the proposed method is not novel. The authors should clarify the differences between their work and [1][2]."
      }
    ]
  },
  "uGJxl2odR0": {
    "human": [
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "The paper proposes to tackle the challenge of Neural Processes with variable input dimensions by extending the Transformer Neural Process architecture with a Dimension Aggregator Block, leveraging Positional Embeddings to account for different input dimensions before transforming features into a fixed dimensional space. The idea of leveraging positional embeddings on the dimensions axis is, I believe, novel and interesting on itself. However, Positional Embeddings are a well-studied part of the transformer architecture, especially in Language Models, and there has been a substantial amount of work on newer and better positional embeddings; the community has largely moved on from Sinusoidal embeddings to other approaches such as RoPE. While the setting here is very different, I see weaknesses in the paper given that positional embeddings are a crucial part for the DAB module to be dimension-agnostic, and I believe this warrants at least a discussion and acknowledgement of recent work, as well as a stronger argument for using Sinusoidal Embeddings. Despite these concerns, I do not think highlighting the failure modes of using Positional Embeddings diminishes the contributions of the paper; quite the contrary, it is important to highlight the limitations of the proposed approach, and if there is evidence that these failure modes do not exist in the setting exposed here, it makes for an even stronger paper. Overall, while there are some methodological and novelty concerns, I still think the empirical value of this work is enough to be accepted."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "The submission presents a novel approach to neural processes, introducing the Dimension Aggregator Block (DAB) module to handle dimension-agnostic tasks."
      },
      {
        "id": "human_review_4",
        "type": "human",
        "label": "Human Review (review_4)",
        "content": "This paper presents the Dimension Agnostic Neural Process (DANP), which incorporates a Dimension Aggregator Block (DAB) to transform input features into a fixed-dimensional space, aiming to enhance the model's ability to handle diverse datasets. I find that the paper has an evident level of novelty, tackling the diverse input and output dimensions challenge in uncertainty aware meta-learning methods such as neural processes. Two novelties seem to be the case here: the dimension aggregation block and the latent path, in a transformer-like architecture. The authors outperform past existing Neural Process methods, demonstrating advantages and improvements on GP regression, Image and Video Completion, and Bayesian Optimization tasks, although the results in some cases, such as GP Regression (from-scratch case) and image completion, appear to be only marginal improvements over existing methods."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "The submission introduces the Dimension Agnostic Neural Process (DANP), whose main technical contribution is the Dimension Aggregator Block (DAB), enabling Neural Processes to handle regression tasks with arbitrary input and output dimensions in a modular fashion. While this is a substantive advance over prior works—such as MF-HNP, VNP, and ConvCNP—which address related issues like variable input size or multi-fidelity, the claim of being the first to achieve full dimension agnosticism is somewhat overstated, as these earlier models offer partial solutions. The integration of transformer and latent encoding layers in DANP follows standard practice in the field and does not constitute a novel contribution. Empirical results show improved predictive performance, but these gains may be partly attributable to implementation choices or the flexibility of DAB rather than a fundamentally new approach. Overall, DANP’s DAB is a meaningful step forward, but the novelty would be more convincingly established with a more thorough comparison to closely related prior work."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces a novel approach called Dimension Agnostic Neural Processes (DANP), an extension of Neural Processes (NPs), that addresses significant limitations in current NP models by allowing the handling of tasks with varying input and output dimensions—a notable advancement in the field. DANP incorporates a Dimension Agnostic Block (DAB) and a Transformer-based latent path, enabling it to effectively generalize across different tasks with limited data, which is a significant strength and highlights its potential for real-world applications. While DANP is compared with several NP variants, the comparison with other types of meta-learning models or general regression techniques is missing, which could provide a broader context for its performance."
      }
    ]
  },
  "ziB549CQ30": {
    "human": [
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper proposed a self-supervised algorithm for FJSSP (SS-FJSSP) that uses an iterative mechanism to refine pseudo-labels, moving from suboptimal to optimal solutions, and effectively bypasses the common neural combinatorial optimization challenge of obtaining true labels. However, this paper lacks contributions and originality. It just proposed a network based on an encoder and decoder for FJSSP, and the encoder-decoder architecture is common in a deep learning network. The paper uses GAT as the backbone, and the effectiveness of the proposed method is questionable, as the results obtained from SS-FJSSP are not significantly different from those obtained with CP."
      },
      {
        "id": "human_review_0",
        "type": "human",
        "label": "Human Review (review_0)",
        "content": "This paper proposes a self-supervised algorithm (SS-FJSSP) that employs neural combinatorial optimization to handle fuzzy data and solve the Fuzzy Job Shop Scheduling Problem (FJSSP). My primary concern about this paper is regarding its novelty. While using neural combinatorial optimization (NCO) techniques to solve FJSSP problems might be novel, the high-level graph representation of the FJSSP problem adopted in this paper is very similar to many existing studies. It seems that the key difference is the introduction of fuzzy model related features and feature vectors; however, the technical novelty associated with these features has not been properly highlighted and justified. Meanwhile, the training strategy does not seem to be new. Perhaps random solution selection is a novel element of the training algorithm, but the effectiveness of this selection mechanism is only discussed intuitively. The analogy with genetic algorithms does not reveal the true advantages of using random solution selection techniques, since the proposed NCO system is not closely related to genetic algorithms. The baseline algorithms adopted in this paper were published about 8 to 9 years ago, and it is important to include more recent baselines in the experimental comparison to truly understand the technical advancement introduced by the new approach. In particular, to clearly demonstrate the advantages of the newly developed encoder-decoder network architecture, several existing approaches using different GNN architecture designs should be experimentally examined and compared in this paper."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper proposed a learning approach to solve the fuzzy job shop scheduling problem, based on an existing self-supervised learning mechanism with an iterative refinement process to incrementally refine the labels. However, there is limited novelty since the neural architecture and self-supervised training mechanism resembles much to existing works."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents a semi-supervised learning approach for the fuzzy job shop scheduling problem (FJSSP). The main novelty is the application of the semi-supervised learning approach from Corsini et al. (2024) to the FJSSP with a few twists. There are some interesting ideas in here, and the semi-supervised approach is particularly interesting, although the novelty in this paper is only a small advancement over the previous work of Corsini et al. and is not examined in an ablation analysis at all. The second claim to novelty about a \"refinement process\" seems to me to just be the Corsini paper again, so I am not sure what is meant here—the perturbation technique? The method is not interesting in and of itself, as SS-FJSSP has many domain specific components in it, so it ought to be as good as the current techniques available."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "This submission presents the first application of self-supervised neural combinatorial optimization (NCO) to the Fuzzy Job Shop Scheduling Problem (FJSSP), adapting iterative pseudo-label refinement to handle fuzzy objectives. The main contribution is a domain-specific extension of existing self-supervised NCO methods (notably SLIM) to the fuzzy scheduling context, rather than a fundamentally new NCO methodology. While the authors claim methodological novelty, the core techniques are direct adaptations, and the most significant advance is demonstrating that NCO can assimilate fuzzy information for FJSSP. The reported 100x speedup over state-of-the-art methods is empirically impressive, but may be largely attributable to neural inference efficiency rather than algorithmic innovation. The submission omits several recent, highly relevant self-supervised NCO works and sometimes overstates its methodological distinctiveness, so reviewers should weigh the true novelty as primarily in application domain rather than in conceptual advances."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper proposes a self-supervised algorithm (SS-FJSSP) for the FJSSP, which employs an iterative mechanism to refine pseudo-labels, transitioning from suboptimal to optimal solutions. The paper lacks a discussion on the novelty of the proposed method, and I am unclear about the key differences between SS-FJSSP and existing self-supervised learning methods for JSSP, such as [1]. Please clarify the unique contributions of SS-FJSSP. The paper also lacks an introduction to the related work on learning-based methods for JSSP, such as [2-4], and does not discuss how SS-FJSSP compares to these methods or its relevance in the context of existing research."
      }
    ]
  },
  "3b9SKkRAKw": {
    "human": [
      {
        "id": "human_review_3",
        "type": "human",
        "label": "Human Review (review_3)",
        "content": "This manuscript presents a diffusion model that utilizes forward-diffused backgrounds and reverse-diffused foregrounds as inputs, allowing the model to concentrate on reconstructing lesions specifically, and applies a post-processing method to enhance generation quality. The novelty of the proposed approach is limited, as the method does not significantly modify the underlying conditional diffusion process but instead introduces variations solely in the input. I would appreciate revisions to Figures 1 and 2 to more clearly illustrate the novelty of your proposed approach and to focus on presenting a straightforward and cohesive pipeline that highlights the mechanisms unique to your method."
      },
      {
        "id": "human_review_1",
        "type": "human",
        "label": "Human Review (review_1)",
        "content": "This paper introduces a novel 3D lesion inpainting method, LeFusion, which uses diffusion models to address data scarcity in medical imaging by generating synthetic lesions in lung CT and cardiac MRI scans for data augmentation in lesion segmentation tasks. The authors identify that existing lesion inpainting methods struggle to preserve anatomically accurate backgrounds alongside the inpainted lesion, and LeFusion is introduced to address this challenge with a lesion-focused diffusion loss and background preservation at inference time using RePaint. While the Introduction and Background sections seem to imply that the proposed lesion-focused loss is a novel contribution proposed for the first time by the authors, this might not be necessarily true considering that there have been other works that employ similar approaches [2, 3]; mentioning them could further strengthen the contextualisation of the approach. The paper provides multiple key contributions which not only address the research gap but also deal with modality-specific challenges related to lesion texture and shape heterogeneity, and these claims are well supported by the results."
      },
      {
        "id": "human_metareview",
        "type": "human",
        "label": "Human Review (metareview)",
        "content": "This paper proposes LeFusion, a lesion-focused diffusion model that redesigns the diffusion learning objectives to focus on lesion areas, simplifying the learning process while preserving high-fidelity backgrounds by integrating forward diffused background contexts into the reverse diffusion process."
      },
      {
        "id": "human_review_2",
        "type": "human",
        "label": "Human Review (review_2)",
        "content": "This paper presents a latent diffusion model-based method for inserting lesions into healthy medical images, incorporating several additions to address limitations of prior work or naïve approaches, such as combining forward-diffused backgrounds with reverse-diffused foregrounds, introducing intensity histogram-conditioning for lesion texture control, and techniques for further control of lesion shape and size. Overall, I find the paper to have decent technical novelty, as it incorporates many techniques—some pre-existing (like combined noised backgrounds with denoised foregrounds) and some seemingly novel (such as histogram-based textural control)—that come together to result in a strongly-performing method. The approach is relatively watertight, with these additions being lightweight and simple, and not requiring an additional network. Clear improvements over baseline methods are demonstrated, especially over Cond-Diffusion, which would be the naïve approach many would first try for this task, and the limitations of such prior methods are made clear through experiments. However, the main limitation is that the task itself is fairly niche within medical image ML, making the overall impact of the work somewhat limited in the broader context of general machine learning."
      }
    ],
    "ours": [
      {
        "id": "ours_summary",
        "type": "ours",
        "label": "Our System",
        "content": "LeFusion is presented as a lesion-focused diffusion model for medical image synthesis, claiming explicit separation of lesion and background, fine-grained control via histogram-based texture conditioning, and multi-channel decomposition for multi-class lesion generation. While the authors assert several \"firsts,\" independent analysis shows that most contributions—such as mask conditioning, multi-channel decomposition, and mask-based control—are already established in recent diffusion and GAN-based works, making LeFusion primarily an incremental advance. The main technical novelty appears to be the specific implementation of histogram-based texture control and the integration of forward-diffused background into the reverse process, though the conceptual groundwork exists in prior literature. The authors' rhetoric sometimes overstates the novelty and underplays similarities to closely related works, with some relevant prior methods not fully cited or compared. Overall, LeFusion is a solid, well-executed refinement of existing techniques, but its contributions are best viewed as technical improvements and combinations rather than paradigm-shifting innovations; reviewers should calibrate their assessment accordingly given the field's maturity."
      }
    ],
    "openreviewer": [
      {
        "id": "openreviewer",
        "type": "openreviewer",
        "label": "OpenReviewer",
        "content": "This paper introduces a novel approach to generating lesion-containing images from healthy images, addressing data scarcity and long-tail distribution problems in medical imaging. The proposed method, LeFusion, focuses on lesion areas during the diffusion process and includes two new strategies for lesion texture synthesis: histogram-based texture control and multi-channel decomposition, enabling the generation of diverse, high-quality lesions with control over lesion size, location, and boundary. However, the proposed method may lack innovation, as it combines existing techniques such as diffusion-based inpainting and lesion mask generation, which are already well-established in medical image analysis, and may not significantly advance the current state-of-the-art in medical image synthesis. Furthermore, the paper does not compare the proposed method with the latest state-of-the-art approaches in lesion synthesis, such as those using advanced generative models like GANs or other diffusion models, making it difficult to assess the true novelty and effectiveness of the approach in comparison to existing work."
      }
    ]
  }
}